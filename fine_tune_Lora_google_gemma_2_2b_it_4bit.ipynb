{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "LBD1d073Z35o",
    "outputId": "cd51ade7-5575-44b4-ac19-234b4dbfe72b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
      "Requirement already satisfied: trl==0.12.1 in /usr/local/lib/python3.12/dist-packages (0.12.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl==0.12.1) (13.9.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (2.19.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets bitsandbytes trl==0.12.1 transformers peft huggingface-hub accelerate safetensors pandas matplotlib numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ae5da50"
   },
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C0P5HtZ6Z5RR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    #AutoPeftModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftConfig # Added to peft\n",
    "from huggingface_hub import notebook_login\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9F6uR-QnAPE"
   },
   "source": [
    "\n",
    "# Check for bf16 support and set compute dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "apjQgqH_m8o8"
   },
   "outputs": [],
   "source": [
    "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "calculate_dtype = torch.bfloat16 if support else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDTS5RrVm-42",
    "outputId": "909ee1b7-c34a-4270-9fbd-c81ac4971bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(calculate_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBHrxjDJnbbf"
   },
   "source": [
    "#bnb config for loading 4 bit model with nf4 quant type\n",
    "* loading model with quantization config\n",
    "* device map to cuda\n",
    "* 4bit true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "218b2ed8ed61412bb447ecb51d5dba54",
      "b2bc0e1b489349999964c9aa4f3d454b",
      "f78cb76bef564e729061bb773127b806",
      "5261ec2f287a4f3a833a21b02f1acf62",
      "940a8b9c1e6d43d2be9f58cccb68ae53",
      "99a4bd353d434fdcbcdd5cb8eff8d758",
      "dc0d29ddb0c74fc8af9175e3054add4c",
      "79098411af29486ebc8464cda3bf058b",
      "3c2b94b690f34235a762ea348ab1c96f",
      "ebbd10ea5d3c418382835af696831307",
      "66d4a8cc58e344928831d8c96a260b78",
      "bba9420adc97496cb3e1de7cd6b950f7",
      "14b4927c941643cf97f1c5680d38eb0a",
      "acdd9e3ab4824110a62e108c5960ab7f",
      "c1450d12010848eca7c079f3732f3476",
      "afbfe8b59e0e4056b368aacbf4a7873b",
      "a203e64fe4194192a2cc58b9c62f3e65",
      "de469281eaee466da7f825928d8c0300",
      "56ead2e7b1ca4919b68cbed5a2cc8108",
      "e142c0acf97142b287cbd29dab4b86a3",
      "fd2f820ff0f04bcaa425af3d9cd1bc28",
      "4ebafcf7a3ff45a48b58e4bd07b8ea8c",
      "8ca5cc1ed07347038baae9445ad0bcf1",
      "08e02f70f5b843e3b4f60fc10c5c4105",
      "c0e939fa3b114b24bf6487105f4515b6",
      "62627087fdd74e7c8371ad9000c9a5c2",
      "2575bdb04b8e4bdfa2da4ba203bc5293",
      "07bb2c00ee684966a8bc4b74aa13e70a",
      "d007edfeefe74d2dac0985cf33bcc3b1",
      "262ed3002ccb492ca8a17911667b89bc",
      "466daffe225b498a9f6c43c20be631ba",
      "e1cca0fab2394c3296fcad7c76f91c9b",
      "0b61190a8c8a45b683ca96d6afa66bf4",
      "ec13eb20e214488288203e9d7e256193",
      "740acb0ae26f4134b20cb313dcf95b28",
      "669a50d5957a4eaeaca722fb11bb8e4c",
      "6ac89ddc441b46e89b6b3cf5802349af",
      "3bbffe186c8749fca99f664977f878a0",
      "3e98518d2634453b94368c0f4f64a7ba",
      "a9a144bf66444cd4a7496f87ff1ef0f5",
      "80bddc9f18dd47988c44a0b7e338fdbb",
      "515cc8da41e5497ea3d5728f97b89168",
      "140bf47b95884fdd90495fe34b463365",
      "da2e3a9ab77447829e86e36fb206f90c",
      "8feffb169e564e5fb6cda14e67a35bb0",
      "781ffb218a3d4dda8f2af2fa532b91ac",
      "affd169fe79848a3bc7e1e013ed75a62",
      "255636da17d34bfcbb7e32b89a49c3e2",
      "6d0ea2e628f6478f86d6e1d8146e3b54",
      "65ca810807c549d6840a86ff3548aa98",
      "b8b33e287ab14a10b0c075b06cb19868",
      "1b2c4e5e817141eb945e7a0a486737f8",
      "d172c5a376b94d14a14bccae2207e3b5",
      "851a6bd3b454490f8e9b609d37c586f5",
      "4374f3b8c1354ad7954bb17bc32f9618",
      "06ceb279ff9645a689ac1c1a6f875487",
      "07ba0f782d3d4d65bb7843de2537fb66",
      "fed50bf43d51491d9657086f3484c6de",
      "5f23ad5fe7cf4f5ea9aa28bdaa5f246b",
      "f8024f02f1d44ac2b4e62fb530c6c76c",
      "9401f938970748318ae40f265a099eb1",
      "e145f26b63644556bc01d162a0ba2a65",
      "d8bfaca92bb44f92818ac1c6c1a7ba17",
      "cc62463152ec461588f40298f65ae850",
      "0b22bba3ba944e5a8801877a76d348e2",
      "17f77be388c849f1814e1ac07e59cf10",
      "53dfe59b68d64b75bbfe240454c5576c",
      "b46bcbbb1eee470cbc2c680faa792524",
      "cac6b7c94f80477aa0a87dc2e5d05490",
      "39e3e7c66b9445e6ba62d9da056e7777",
      "a98dd38450a74854b2c16df9e2d01175",
      "e90936b37f914ea598b7036a043c060f",
      "d6e652e1bc624ecbbbe7d407b4ce8b35",
      "cfc1d0fc653d4c57a0029f178a927686",
      "323c5cf3c1a449b592a7dea7acec0a73",
      "5f7db94e62ea466db73fe6dceebde6ab",
      "899d7d37b90448f693f8a882b9ba2475"
     ]
    },
    "id": "nkpxpotRZ5Ty",
    "outputId": "4a353cc4-7681-4a7e-d408-53ff443cba34"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218b2ed8ed61412bb447ecb51d5dba54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba9420adc97496cb3e1de7cd6b950f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca5cc1ed07347038baae9445ad0bcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec13eb20e214488288203e9d7e256193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8feffb169e564e5fb6cda14e67a35bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ceb279ff9645a689ac1c1a6f875487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dfe59b68d64b75bbfe240454c5576c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= calculate_dtype, #calculate_dtype can be bf16 or float32- use bf16 if supported\n",
    "    bnb_4bit_use_double_quant= True\n",
    "    )\n",
    "repo = \"google/gemma-2-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, quantization_config= bnb_config, device_map= \"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzT757Suokja"
   },
   "source": [
    "#Check model memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D127_Z6FZ5WN",
    "outputId": "14289396-d8d4-4706-ddc6-bf65c97d90b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2090.7119140625\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQyQHN6zos1k"
   },
   "source": [
    "#model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vm7RO981Z5Yh",
    "outputId": "1b2cb0d4-fbaa-40cc-b172-d1e9e1dbb4b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSh3kYaioxjD"
   },
   "source": [
    "#Prepare model for kbit training\n",
    "##Use Lora Config\n",
    "\n",
    "\n",
    "1.   rank [4,8,16,32] - choose one\n",
    "2.   lora_alpha is a scalling factor which should be 2x the rank of matrix.\n",
    "3.   dropout range from 0.03 to 0.10 which helps prevent overfit\n",
    "4.   module - choose module as per requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzMopk81Z5ao",
    "outputId": "547dfeea-6665-47db-cc85-1e7085c470a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9216, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): GELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 8, #. rank of LoRA - [4-16]\n",
    "    bias = \"none\", # [\"all\", \"lora_only\"] - for train bias term\n",
    "    lora_alpha = 16, # scalling factor\n",
    "    lora_dropout = 0.10, # prevent overfit- used for regularisation\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    "\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzA5DOMp5aA"
   },
   "source": [
    "#once again check memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjtETM3QZ5fK",
    "outputId": "9f720ff5-0c3a-46e6-b849-e5440778550f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3255.78271484375\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z2XsrHMp9kr"
   },
   "source": [
    "#Print base model to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGNI99ZVZ5hr",
    "outputId": "adbb1cda-92e0-41cf-f81d-82d68445a33a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Gemma2ForCausalLM(\n",
      "      (model): Gemma2Model(\n",
      "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-25): 26 x Gemma2DecoderLayer(\n",
      "            (self_attn): Gemma2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Gemma2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=9216, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): GELUTanh()\n",
      "            )\n",
      "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (rotary_emb): Gemma2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.get_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiBoFpSgZ5j2",
    "outputId": "94ff74c0-02d9-477f-9e4c-59ac0974583a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3413.935616\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSmHEA7cqHKh"
   },
   "source": [
    "#Check for trainable Parameters and its percentage for a mathematical view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ff-gwq_dZ5mO",
    "outputId": "ce7610c1-0d12-4a95-a9c2-a7b784f045ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 10,383,360\n",
      "Total Parameters: 2,624,725,248\n",
      "Percentage Trainable: 0.40%\n"
     ]
    }
   ],
   "source": [
    "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Percentage Trainable: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oOA2lywqYKD"
   },
   "source": [
    "#ETL Process for Dataset Prep stage, Tokenizer load and define chat template if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718,
     "referenced_widgets": [
      "3cc155b54ddd470e8c9493f31b37a5cc",
      "a36d776a8b474655890f8b342788a2ac",
      "bd3f26ec66cb493abc2176060d6a7c4b",
      "ac7f17daafd54ea59d55bff242a532b6",
      "2bb2501d3ae84f2f9a80f4349e342977",
      "8abf433a2b4f47f1b417eaaac4fdcfb1",
      "6162cba91d8b4ff1a1e8a9c3da28876a",
      "ce60506937084f528a23537eb9763c68",
      "275c101420a345f6b5845287522f2051",
      "7a04da2d359e47ac82e635e1fafdc8ec",
      "022faaff289247c19c5b5018b2ec1c06",
      "5256f8202ed9403dbcb34e8ec75ad127",
      "d43b357bcd92452691e48dd4764160ed",
      "68b9bd4f6a5e47239b2590ba4bf370cb",
      "8547a1dbe6764e138402b66a45d9ad8b",
      "c4e47311ba1441f1ae4fbe5f7507f09b",
      "4205a157ef9d43118fd0c9fe52472c9a",
      "35f9c09dbb224c94b4270b4a6ada2702",
      "e0e58857396540178a7dc40113ff8229",
      "8dfbd09b252244289b8f5bc5cb62d117",
      "e6c69e10f3f044ea85525efd2169ca68",
      "b01923ebb6274f138bd49d267e1d0679",
      "b63d079be8ae48268a5ad5f49ae4d997",
      "c701c980ce204d90a7fbb054d78d5f15",
      "10a35093dcbe403e98d1655ada6487a8",
      "c1b2caba2a0d4497907f19b4715fd1e5",
      "0f85492387274325af8a17d69d9f3fba",
      "b556448330424096b87d68756633a6b7",
      "c91f0e4344874dd2b47926a9b1b90ba1",
      "34e0654c2ac84039a63d31d56077cc90",
      "106759ea73f442c394ab42e16763efd6",
      "8a83988fad5f423698cebb8a17943bc1",
      "b20c7a2735b249f6b2612a3380cb74d4",
      "51b8c8c2f5434cc585ab4a9af9768cce",
      "2401e7092de54c718f838fef67a640b2",
      "5411e0b1214148fa9e91e705121e652b",
      "b435ec6a343242a9aa486f9eba04b9fb",
      "35c381b759b34ac6ac1ab91284be4f96",
      "3f17027120b844bfbce8ad64aee89585",
      "105d66a477154271b3a22cf7c2a54b14",
      "4d7a6fc986d748d7b370238b3ed5eef0",
      "6906d65ff0d14f2ca317b680ec259969",
      "0e92bbaaf582496a97cbafd69f647340",
      "3fe5c903b9284200b108f92c1dbd55c7",
      "c41fef833b89455aadef6720589de9cd",
      "46f56f7487d0431590e774ce0d60e612",
      "4d81111232cd49f1bdaf2fd4086a7767",
      "c4545e90430c49be9705a5042f987248",
      "05cf2a12680a4194abb0c3375e102c3a",
      "e90c08344fb445fa87649252f8af73b2",
      "e49683a2156c4b5e870b03029023c439",
      "ccb6a99664594232b1193a2d9efa2e1a",
      "e25c9455d08e4f7d9df0a03bd8a9dc1a",
      "112e791d0e7445eb8bfb25fb8f2c8a9e",
      "2dbcdb5f6c1a4465b9deddb7445a7c87",
      "88efa367d3ac473b9ab3bdbb45b3c514",
      "a059f9103d4649d2a1971f69c73b60a5",
      "755efa9de1e647a2a6ab3741ab19980a",
      "5426feeb6f0b4147bdff59aa14944541",
      "0ac4445ea32b429eb191c944a4fec145",
      "d7cf65e9cc63410ba9210f8438d74797",
      "781d2d49771d4f79b0b6ccf72c366e63",
      "1e9c6d1183244b1bb88095ec9a3a9b41",
      "0dd7cd859b554c63a0505ae4bfd8c183",
      "04db90f506a04666b4a334e9a194a246",
      "44f46cc4079f460f8f1646b2d5de5036",
      "3c848ca3f33348c6a5fa537a405feb20",
      "b141865363854d45b81361ecdae19ce3",
      "51aa387080614d2c97a9b04e3be244d1",
      "1c3f3bab6a234ddd84bd1ea9ff70e66b",
      "f9327e4f3fa046139c5e37825c704c6e",
      "b8ba01d387284f4a83d0ed970a56de98",
      "1869af723a8d40138c5f157bf92626f6",
      "abc97395abf04748ad554b8f9cf620de",
      "f1b40377bf964d3fbac25a6287a99359",
      "ee201b3b92f742b69e76283b146ebe5a",
      "a3a111d94da54561ba4d1b86162f7993",
      "da733df67d974852983570fadaf8f563",
      "abcc89c669c046479bf2ecdf859b0728",
      "e7b0c485278d45279a6dd9c347aae369",
      "95624873442b4ced8e3631386ff7f065",
      "3145d9ac443b4f44aeb8933ea7e0e063",
      "fcf85400940b40558f0fef9df2fc161a",
      "0b65c60eb5334eb8ac1ead1b851f972d",
      "8101765eadec413a96e3da61d97ccd08",
      "7649afa1aa5f4a17adf140bb56a2acf1",
      "b5bd889ad86e4bac9f0d72b3f5897f30",
      "f7f5532c63e64f6e8a4d201b8763c65f",
      "16476ae5d2e54b059b070d999a1c8060",
      "3eb9d5e60b9c4d3ca6fec7bc1ddff3cd",
      "7bbf26dda1b14459b645becf51505539",
      "041bc0a3800b43338b35afc50e6df853",
      "588c2626c994483ca76be7908dbd230f",
      "3d714c1a2ca74f15adf4fb4065f5519b",
      "bfd1305c3b24457ebf0c79f1134554fe",
      "40069a83f0ca4a06ac71eee860e6e0cb",
      "f5205fc914e243a1a2b0915b3e3a395c",
      "6d57289b80d84676b086570299de1ac6",
      "00ddd3f4752c42478640bc1655cbe514"
     ]
    },
    "id": "K3Iox1kEre3Q",
    "outputId": "a893c35d-e5ed-4adb-f43b-028af8163342"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc155b54ddd470e8c9493f31b37a5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5256f8202ed9403dbcb34e8ec75ad127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63d079be8ae48268a5ad5f49ae4d997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b8c8c2f5434cc585ab4a9af9768cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChatDoctor-HealthCareMagic dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41fef833b89455aadef6720589de9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/542 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88efa367d3ac473b9ab3bdbb45b3c514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-5e7cb295b9cff0(…):   0%|          | 0.00/70.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c848ca3f33348c6a5fa537a405feb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/112165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10000 samples\n",
      "Sample entry: {'instruction': \"If you are a doctor, please answer the medical questions based on the patient's description.\", 'input': 'I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!', 'output': 'Hi, Thank you for posting your query. The most likely cause for your symptoms is benign paroxysmal positional vertigo (BPPV), a type of peripheral vertigo. In this condition, the most common symptom is dizziness or giddiness, which is made worse with movements. Accompanying nausea and vomiting are common. The condition is due to problem in the ear, and improves in a few days on own. Betahistine tablets would help relieve your symptoms. Doing vestibular rehabilitation or adaptation exercises would prevent the recurrence of these symptoms. An ENT evaluation would also help. I hope it helps. Best wishes, Chat Doctor.'}\n",
      "Dataset columns: ['instruction', 'input', 'output']\n",
      "\n",
      "Formatting medical Q&A prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da733df67d974852983570fadaf8f563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting prompts with Gemma-2 chat template:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16476ae5d2e54b059b070d999a1c8060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing medical conversations:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✅ MEDICAL DATASET PREPARATION COMPLETE!\n",
      "============================================================\n",
      "📏 Max sequence length: 1024\n",
      "🔑 Dataset keys: ['input_ids', 'attention_mask', 'labels']\n",
      "💾 Approximate size: 39.06 MB\n",
      "\n",
      "🚀 Ready to use 'final_dataset' in your SFTTrainer!\n",
      "============================================================\n",
      "\n",
      "📋 Sample formatted prompt (first 20 chars):\n",
      "<start_of_turn>user\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ============================================\n",
    "# SETUP: Gemma-2-2B Model & Tokenizer\n",
    "# ============================================\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ============================================\n",
    "# LOAD MEDICAL DATASET\n",
    "# ============================================\n",
    "print(\"Loading ChatDoctor-HealthCareMagic dataset...\")\n",
    "\n",
    "# Load the medical Q&A dataset (using 10k samples for reasonable training time)\n",
    "raw_dataset = load_dataset(\n",
    "    \"lavita/ChatDoctor-HealthCareMagic-100k\",\n",
    "    split=\"train[:10000]\"  # Using first 10k samples\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(raw_dataset)} samples\")\n",
    "print(f\"Sample entry: {raw_dataset[0]}\")\n",
    "print(f\"Dataset columns: {raw_dataset.column_names}\")\n",
    "\n",
    "# ============================================\n",
    "# FORMAT PROMPTS FOR GEMMA-2\n",
    "# ============================================\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"\n",
    "    Format medical Q&A into Gemma-2's chat format with safety disclaimer\n",
    "\n",
    "    Gemma-2 uses a specific chat template format:\n",
    "    <start_of_turn>user\n",
    "    {user message}<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    {assistant response}<end_of_turn>\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract patient question and doctor response\n",
    "    # The dataset has 'input' (patient question) and 'output' (doctor answer)\n",
    "    patient_question = example[\"input\"]\n",
    "    doctor_response = example[\"output\"]\n",
    "\n",
    "    # Add safety disclaimer to medical responses\n",
    "    safe_response = (\n",
    "        f\"{doctor_response}\\n\\n\"\n",
    "        \"⚕️ Disclaimer: This information is for educational purposes only. \"\n",
    "        \"Please consult a qualified healthcare professional for medical advice.\"\n",
    "    )\n",
    "\n",
    "    # Format in Gemma-2 chat template\n",
    "    prompt = (\n",
    "        f\"<start_of_turn>user\\n\"\n",
    "        f\"{patient_question}<end_of_turn>\\n\"\n",
    "        f\"<start_of_turn>model\\n\"\n",
    "        f\"{safe_response}<end_of_turn>\"\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# ============================================\n",
    "# TOKENIZATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the formatted text with proper padding and labels\"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,  # Increased for medical responses (often longer)\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # Create labels for causal LM training\n",
    "    # Labels = input_ids, but -100 for padded tokens (ignored in loss)\n",
    "    labels = []\n",
    "    for input_ids, attention_mask in zip(tokenized[\"input_ids\"], tokenized[\"attention_mask\"]):\n",
    "        # Convert to list if needed\n",
    "        label = input_ids.copy() if isinstance(input_ids, list) else list(input_ids)\n",
    "\n",
    "        # Set padded positions to -100 (ignored in loss calculation)\n",
    "        for i, mask in enumerate(attention_mask):\n",
    "            if mask == 0:  # This is a padded token\n",
    "                label[i] = -100\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# ============================================\n",
    "# PROCESS DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFormatting medical Q&A prompts...\")\n",
    "formatted_dataset = raw_dataset.map(\n",
    "    format_prompt,\n",
    "    desc=\"Formatting prompts with Gemma-2 chat template\"\n",
    ")\n",
    "\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset.column_names,  # Remove original columns\n",
    "    desc=\"Tokenizing medical conversations\"\n",
    ")\n",
    "\n",
    "# Final dataset ready for training\n",
    "final_dataset = tokenized_dataset\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ MEDICAL DATASET PREPARATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📏 Max sequence length: {len(final_dataset[0]['input_ids'])}\")\n",
    "print(f\"🔑 Dataset keys: {final_dataset.column_names}\")\n",
    "print(f\"💾 Approximate size: {len(final_dataset) * 1024 * 4 / 1024 / 1024:.2f} MB\")\n",
    "print(\"\\n🚀 Ready to use 'final_dataset' in your SFTTrainer!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Optional: Print a sample to verify formatting\n",
    "print(\"\\n📋 Sample formatted prompt (first 20 chars):\")\n",
    "print(formatted_dataset[0][\"text\"][:20] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rWTIyS5oAySa",
    "outputId": "3e1a4099-7e8e-4b9e-a791-8bda9926c704"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏥 Starting Gemma-2-2B Medical Fine-tuning...\n",
      "📊 Training on 1500 medical Q&A samples\n",
      "🔧 LoRA Config: rank=8, alpha=16, dropout=0.1\n",
      "💾 4-bit quantization: nf4\n",
      "⏱️ Estimated time: ~2-3 hours on T4 GPU\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchaubey-amit017\u001b[0m (\u001b[33mhectorlabs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251026_201716-ejssdl22</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hectorlabs/huggingface/runs/ejssdl22' target=\"_blank\">Gemma-2-2B-MedicalQA-LoRA-r8-alpha16</a></strong> to <a href='https://wandb.ai/hectorlabs/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hectorlabs/huggingface' target=\"_blank\">https://wandb.ai/hectorlabs/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hectorlabs/huggingface/runs/ejssdl22' target=\"_blank\">https://wandb.ai/hectorlabs/huggingface/runs/ejssdl22</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 56:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.426900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.376300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.443500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✅ Medical fine-tuning completed!\n",
      "💾 Model saved to: /content/drive/MyDrive/gemma-2-2b-medical/Gemma-2-2B-MedicalQA-finetuned\n",
      "🏥 Your Gemma-2-2B Medical Assistant is ready!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# My optimized parameters for LoRA training on medical data\n",
    "min_effective_batch_size = 4  # Reduced for 2B model with longer sequences (1024 tokens)\n",
    "lr = 2e-5  # Slightly lower LR for Gemma-2 stability with medical data\n",
    "max_seq_length = 1024  # Increased for detailed medical responses\n",
    "collator_fn = None  # I'm not using a custom collator since I pre-pad in tokenization\n",
    "packing = False  # I disabled packing since I'm using fixed-length sequences\n",
    "steps = 20  # My logging and saving frequency (adjusted for larger dataset)\n",
    "num_train_epochs = 3  # Standard for medical fine-tuning with LoRA\n",
    "warmup_ratio = 0.1  # Warmup for stable training start\n",
    "\n",
    "# My SFT configuration for Gemma-2-2B medical assistant\n",
    "sft_config = SFTConfig(\n",
    "    # I'm saving my medical model to a dedicated directory\n",
    "    output_dir = '/content/drive/MyDrive/gemma-2-2b-medical/Gemma-2-2B-MedicalQA-finetuned',\n",
    "\n",
    "    # My data processing settings\n",
    "    packing = packing,\n",
    "    max_seq_length = max_seq_length,\n",
    "\n",
    "    # I disabled gradient checkpointing (not needed with 4-bit quantization)\n",
    "    gradient_checkpointing = False,\n",
    "\n",
    "    # My training batch and precision settings\n",
    "    # Note: Smaller batch size due to longer sequences (1024 vs 512)\n",
    "    per_device_train_batch_size = min_effective_batch_size,\n",
    "    auto_find_batch_size = True,  # Let trainer optimize for available VRAM\n",
    "    bf16 = True,  # Using bf16 for better numerical stability (important for medical accuracy)\n",
    "\n",
    "    # My training schedule optimized for medical domain\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    learning_rate = lr,\n",
    "    lr_scheduler_type = \"cosine\",  # Smooth learning rate decay\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    weight_decay = 0.01,  # Regularization to prevent overfitting on medical terminology\n",
    "    max_grad_norm = 1.0,  # Gradient clipping for stability\n",
    "\n",
    "    # My logging and monitoring setup\n",
    "    report_to = 'wandb',  # Tracking my medical AI experiment\n",
    "    run_name = \"Gemma-2-2B-MedicalQA-LoRA-r8-alpha16\",  # Descriptive run name with LoRA config\n",
    "\n",
    "    # My logging directory\n",
    "    logging_dir = '/content/drive/MyDrive/gemma-2-2b-medical/Gemma-2-2B-MedicalQA-finetuned/logs',\n",
    "\n",
    "    # My checkpoint and logging strategy\n",
    "    logging_strategy = 'steps',\n",
    "    save_strategy = 'steps',\n",
    "    logging_steps = steps,  # I log every 20 steps\n",
    "    save_steps = steps,     # I save checkpoint every 20 steps (more frequent due to larger dataset)\n",
    "    save_total_limit = 2,   # Keep last 2 checkpoints to save Google Drive space\n",
    ")\n",
    "\n",
    "# I create my trainer with the medical dataset and configuration\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                    # My Gemma-2-2B model with LoRA adapters (rank=8, alpha=16, dropout=0.1)\n",
    "    train_dataset = final_dataset.select(range(1000)),    # My 10k medical Q&A dataset\n",
    "    processing_class = tokenizer,     # Gemma-2 tokenizer\n",
    "    data_collator = collator_fn,      # Using default collator\n",
    "    args = sft_config,               # My medical training configuration\n",
    ")\n",
    "\n",
    "# I start the medical fine-tuning process\n",
    "print(\"🏥 Starting Gemma-2-2B Medical Fine-tuning...\")\n",
    "print(f\"📊 Training on {len(final_dataset.select(range(1500)))} medical Q&A samples\")\n",
    "print(f\"🔧 LoRA Config: rank=8, alpha=16, dropout=0.1\")\n",
    "print(f\"💾 4-bit quantization: nf4\")\n",
    "print(f\"⏱️ Estimated time: ~2-3 hours on T4 GPU\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Medical fine-tuning completed!\")\n",
    "print(f\"💾 Model saved to: {sft_config.output_dir}\")\n",
    "print(\"🏥 Your Gemma-2-2B Medical Assistant is ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612,
     "referenced_widgets": [
      "a5f44203a8f142c88f75b4d5a1e014bb",
      "61c7a2c1016d4fdabf69fdcd066b5228",
      "566a5148c9a74489b6e37f44926635b7",
      "6a505ca936164bfa85cfde8a26907206",
      "2a83b76f0acb44069f69f0aeca10d249",
      "5af4dcba48a1488f81e9709276623587",
      "adb089c17a82457ca14cf3c178bb888b",
      "b80a1b33d32b49d999f26875c469d039",
      "72619d33be864a1f8b33fee307ecefb1",
      "e68f064c5d6e41369342bc84a475829e",
      "3750290ac98e4dedb3febe95ec280347",
      "99a888cdf1ca40bfba25e70b6a05bea9",
      "6bd5ea6066fd472583d3e71c7e45f996",
      "24773809533c47ecbb4bc23b83f7e033",
      "9c769f95c6de424ab735bf935f88854d",
      "416791855c7146c08c5ae5b026e47885",
      "7e601de9cf1f4a2680a872c4c1f471cf",
      "accb85cc4b174ad082aac935a65a1586",
      "d9136aaa654940068b78b91a59161f81",
      "875ce026e1ae4f75b35b328e7e038370",
      "e7a68de136854ed0b023ed1b5f2c42fb",
      "21b4e7b855f24322ac34e9f18baec197",
      "e3b730f14bd3463297ef21067f115c94",
      "cc8880949ac041b393a38cf526ab9cbf",
      "cbf50f9e5aa54f8483ec3d0c1c1a25a0",
      "664cd0ef16474b4ab51c1e031f36809d",
      "c08a1508cf574befb86b436fbe5a9731",
      "61dfb6997e154ea88ae44f7bb5f10e1e",
      "ef47d01618604e26beb3340523396bd2",
      "cd6caa2f36f84affa3ab8d6c0d9b8755",
      "533fab4078fe4ac4b9257a5da87eb995",
      "b96f9674e3de4ee19da425fea8e63de8",
      "129040f0accf4c7f816abb76b3f68903",
      "6092abfb24954d0f88bde1b08e3f4f70",
      "d937949698bd4bfda5234e39bd737d08",
      "1c05c73d50e74706ad9477d29fa5c554",
      "6649a596a48b4a1f81619714bc8343cc",
      "c4baac363ec743679591ddb30cc23190",
      "f5a42b5a36d44947b64ea0333d53239b",
      "a40d486ae31e441bac9baa7192035506",
      "f84a5a0aaa02410984e6565e28da95b9",
      "61f54e01475040839dac81d7a70b227b",
      "472fdc961b4442ffb48ae7b34c2438b4",
      "f60d793ab60a488c9be972f5372f572d",
      "5d1ef6f2ea874c93b153107732e572c3",
      "c5116f10a1dd4f85bc586d775beb64c6",
      "1bb597a1940c482ea6eda5d59714e5e6",
      "69e7f40b91184455840e2eeaebde0e04",
      "09dbfea145534edd885298b36038e25d",
      "640ba4f45d32477fa360a61e6cf830d0",
      "866fa7342f7148d5982ba977cafb3af4",
      "f34b87c002da4b2d8a3c7370cefc7cfd",
      "51b086da155f4c9a90ca8616c723747e",
      "0cc29ada026b47a89bdb9c626b40ec30",
      "f1f95fa2a3674795b83a8d3be74446e5",
      "d02f989ca0244125afa9fb96c7d513dc",
      "8a733a5365a54af78af0b0c3259217ed",
      "e74b7d99806440419e99edb9cb6156a8",
      "2ea22e2e10254ea7ad83a1985463b475",
      "f947802899754fcea0dab30d0e0cab94",
      "43dfab3aa7bc48d48842aebad8e65c98",
      "a0f07d99912d4c74a8578fc74e413b7b",
      "5574fa05380745b6aa37c028f3f3abc0",
      "03e7e447b2b04838af5c3ac9a85e51a3",
      "e983cbd50efe49ef9dc4804f649f9acb",
      "fbfbeb2151504aff9673463416b4090d",
      "06f7ca9c21bd47acaf5f1a7b5ac43cb1",
      "5715e5d25d02433cb85089ed211f54e8",
      "3e840dbfe9b345af92ead14edefbb1dd",
      "d5647be5f6ca4584b833904d80a17cc4",
      "7772390e2b034a3d9f8556f2f3966ac2",
      "ef9b9812ebbc4ed1870794db7ed8f09a",
      "bd8a0575564547c78d159422651d02ce",
      "b9510af415724d48ad2263773adac577",
      "770d8fe81101403fba6e3710042f4bec",
      "8cc00438ba31434592736470ff56d46b",
      "994edea1b3744430b669d738bd9b6203",
      "964901c63a384967bc06a619482e7249",
      "5c2b59c4a9aa430381bf7d9012493b15",
      "8fa2c6d9968c408680e4648876c46b8d",
      "fee6fe41ec6740c6b1b789845f914a06",
      "66b94757cbf242568f88df158dd0f314",
      "224e9a03c81049bd95d4251aba386bc9",
      "71f3fb60976f4537a8a33bbcd78bc80e",
      "6ee29628e7f2436f988974c7ade7fc64",
      "228d00fefc8a48868386d65f12a69f85",
      "1ad1a090d3824111b4b76ae04b7277db",
      "ec0508f2a2df473194905994d791ca96"
     ]
    },
    "id": "W7SyBkyK-S-a",
    "outputId": "79a23cdd-478c-441f-ec8d-01136f9a0a17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving my trained Gemma-2-2B medical model...\n",
      "🔧 Loading my PEFT model and merging adapter...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f44203a8f142c88f75b4d5a1e014bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Merging LoRA weights into base model...\n",
      "💾 Saving my merged medical model...\n",
      "☁️ Uploading my medical assistant to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a888cdf1ca40bfba25e70b6a05bea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b730f14bd3463297ef21067f115c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6092abfb24954d0f88bde1b08e3f4f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...al-merged/tokenizer.model: 100%|##########| 4.24MB / 4.24MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1ef6f2ea874c93b153107732e572c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...cal-merged/tokenizer.json: 100%|##########| 34.4MB / 34.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02f989ca0244125afa9fb96c7d513dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0003-of-00003.safetensors:   0%|          |  551kB /  481MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f7ca9c21bd47acaf5f1a7b5ac43cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0002-of-00003.safetensors:   0%|          | 28.1kB / 4.98GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964901c63a384967bc06a619482e7249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0001-of-00003.safetensors:   1%|          | 41.9MB / 4.99GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✅ Model upload completed! 🎉\n",
      "🏥 Your Gemma-2-2B Medical Assistant is now live!\n",
      "============================================================\n",
      "🔗 Model URL: https://huggingface.co/sweatSmile/Gemma-2-2B-MedicalQA-Assistant\n",
      "\n",
      "📋 Model Details:\n",
      "   - Base: google/gemma-2-2b-it\n",
      "   - Dataset: ChatDoctor-HealthCareMagic (1.5k samples)\n",
      "   - LoRA: rank=8, alpha=16, dropout=0.1\n",
      "   - Quantization: 4-bit (nf4)\n",
      "   - Domain: Medical Q&A with safety disclaimers\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: I'm saving my trained medical model locally first\n",
    "print(\"💾 Saving my trained Gemma-2-2B medical model...\")\n",
    "trainer.save_model('/content/gemma-2-medical-saved')\n",
    "\n",
    "# Step 2: I load and merge the LoRA adapter with the base model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print(\"🔧 Loading my PEFT model and merging adapter...\")\n",
    "# I load the saved PEFT model (use the same path as Step 1)\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained('/content/gemma-2-medical-saved')\n",
    "\n",
    "# I merge and unload the adapter to get a single model\n",
    "print(\"⚙️ Merging LoRA weights into base model...\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Step 3: I save the merged model with tokenizer\n",
    "print(\"💾 Saving my merged medical model...\")\n",
    "merged_model.save_pretrained('/content/gemma-2-medical-merged')\n",
    "tokenizer.save_pretrained('/content/gemma-2-medical-merged')\n",
    "\n",
    "# Step 4: I upload my model to Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"☁️ Uploading my medical assistant to Hugging Face Hub...\")\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path='/content/gemma-2-medical-merged',\n",
    "    repo_id=\"sweatSmile/Gemma-2-2B-MedicalQA-Assistant\",  # My medical AI repo\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload Gemma-2-2B fine-tuned on 1.5k medical Q&A (ChatDoctor-HealthCareMagic) with LoRA (r=8, alpha=16)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Model upload completed! 🎉\")\n",
    "print(\"🏥 Your Gemma-2-2B Medical Assistant is now live!\")\n",
    "print(\"=\"*60)\n",
    "print(\"🔗 Model URL: https://huggingface.co/sweatSmile/Gemma-2-2B-MedicalQA-Assistant\")\n",
    "print(\"\\n📋 Model Details:\")\n",
    "print(\"   - Base: google/gemma-2-2b-it\")\n",
    "print(\"   - Dataset: ChatDoctor-HealthCareMagic (1.5k samples)\")\n",
    "print(\"   - LoRA: rank=8, alpha=16, dropout=0.1\")\n",
    "print(\"   - Quantization: 4-bit (nf4)\")\n",
    "print(\"   - Domain: Medical Q&A with safety disclaimers\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
