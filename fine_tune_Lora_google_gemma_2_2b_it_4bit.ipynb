{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "LBD1d073Z35o",
    "outputId": "f893d470-3bf4-4206-c901-4e69f2ee389a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting trl==0.12.1\n",
      "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl==0.12.1) (13.9.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (2.19.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading trl-0.12.1-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, bitsandbytes, trl\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.48.1 numpy-1.26.4 trl-0.12.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "14cc62bbd3b249d6ad21f19e78891369",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install datasets bitsandbytes trl==0.12.1 transformers peft huggingface-hub accelerate safetensors pandas matplotlib numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ae5da50"
   },
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C0P5HtZ6Z5RR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    #AutoPeftModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftConfig # Added to peft\n",
    "from huggingface_hub import notebook_login\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9F6uR-QnAPE"
   },
   "source": [
    "\n",
    "# Check for bf16 support and set compute dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "apjQgqH_m8o8"
   },
   "outputs": [],
   "source": [
    "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "calculate_dtype = torch.bfloat16 if support else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDTS5RrVm-42",
    "outputId": "cb9e78a5-6a60-454d-8d13-1cfdfe873481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(calculate_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBHrxjDJnbbf"
   },
   "source": [
    "#bnb config for loading 4 bit model with nf4 quant type\n",
    "* loading model with quantization config\n",
    "* device map to cuda\n",
    "* 4bit true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "0e2a4d27d8624ca680404de34670e499",
      "bee18f7cf79f41e0a5f095c176de7295",
      "f1d212ac513f475ab9155ce182d9ea4f",
      "5ea4fe207356433cbef790e3ddb60118",
      "50a412bed5754923ad781289ec23aa50",
      "b7b3f333c14648b7a9024ab60f68e68f",
      "7168123e5270450ea737770a28c2b112",
      "cb8ae4bfd1024005808b1c1b80d4ec49",
      "cf0f0c28544644f182582d1603851dd7",
      "ca49950a72334165a51e08e8f5d289bd",
      "bf58ff816165476480919109cc6f61e7",
      "f5a48f066f00486d8a90d19d3bf7e42f",
      "5daf87ceb1cc4264887686200aab3b5c",
      "cef9ed18349b4678a4ac65ac8bc0bc51",
      "969ab014812d4cb0bef0371ba3a9a043",
      "9cb2e1681d574ac7935ac2a97810539b",
      "da32de883ef0473bb76dd9c465b83572",
      "6fdbf95bf1fb417f81878ee831085e9c",
      "e56615875afe4ea595706612a2715e6a",
      "b283eaa8219442cfa1b349971c6aed8b",
      "19b9148d51be4911a37ff39b76a0a77a",
      "6a85eb4c761c435ca72647c09ef82ac9",
      "4fbab5538e5648c690c45fe318e3cc45",
      "13a67724a3a940648d367e7cc4f63d4f",
      "d33909dfb407477fada7040f85b8de22",
      "f4ee841b6181473291a301ee2d3fd2b3",
      "19ee2a7b70344704bebe1854b3aea67e",
      "e7f137573ce148a780cfdfad559e2e40",
      "2cd5afd208d2468fa1c03420b4fa0ea5",
      "9948804bad1a4450b778878fab19edec",
      "83e9bf62e19f48c6bff3f0aaa1e4a71f",
      "91d4c9271c0f40bdacf26cba8739e6af",
      "28986f6ffc72455ca7b262c123d8c82f",
      "c211bad691f349c881010fc3be195fa8",
      "274b5b392f094ffaba64986e7ccfc5bd",
      "03ca5843f2aa4c14adda7f85c61f854a",
      "dee970a53e9e41dc9a182de47151051f",
      "5f1d93f8ffe74dac9cf474e290605d26",
      "b8c08ca7732b48a69a4d05879d18f503",
      "4d5762d963cc4c0b8c05d0e6aad7d701",
      "13a3216172e54fa89a176d8155ec740b",
      "32e2ff4fb856456993d8bf59192ab28e",
      "a726f7deb24e4e8a829c2f84073c41a8",
      "e0896f04a50e45298d0463f48009826f",
      "a1172db947394eef8db53485a5bbb248",
      "e22e03421f5c48a197bc58b14f0efe9e",
      "922fc1882d994550966d16069de07a1a",
      "0d200576352d4b68b78f5163abed7850",
      "d7dc852927864bd98731d472033edf0b",
      "a2d1b6a33d01480b8511e9a274c648f9",
      "5746c37579db4278b8bafa6ae3a76bfd",
      "592991b4b519453cb496b28cf48c15ce",
      "98ce4f2559ee4825bd4b493897671c82",
      "aaf94f41974d4ec683b061163b824c3f",
      "264c8a39d7ec438f8c83f3f61ecb6f08",
      "c1abd247a78d4af984ff6069f92cfb41",
      "83aad573f2404dd891537cecc7e482a7",
      "49e905297d3c4b6cae138406462bc4cb",
      "2581c42bf89d4089a550b456634ba46d",
      "bce3f823cbf348a0b96375bddd13a881",
      "d8a3455f6c0043cba21bd0b2e4adb14a",
      "a32147aef6714885ae80f6cd99a4baad",
      "b726632b7b6146ffaa9f98c1eb27ce3f",
      "87d617510d2c4a2db31768c02ed1c393",
      "eb5a260da0ee47f79823d2cec191474a",
      "1fec60285f5646d7be3fc986906aa71c",
      "fb18fa019f584e80a4df8cb7d930273c",
      "affd4c70acce448ca6c51b2dfd6f018c",
      "31b22fcf771f45d8aa78c7883f851d6d",
      "6a8c71e027db4c2ca00ef43307489bab",
      "86eedc5a95a5478d98e4a001fe6597ce",
      "4cb7c6c715bd401f990c31a9e536f521",
      "ef191bbf619a477c80284c38b28ec94b",
      "d120351b03314f6fbf19aa6a26b137c0",
      "6ec69856e1b648f8916be02e5879c370",
      "b372b5d9648c440599b9ec5d45da404d",
      "1dc07cb340c64672a9456e385c02358a"
     ]
    },
    "id": "nkpxpotRZ5Ty",
    "outputId": "a6f2e507-4933-4475-9a6e-d6bfa91fde2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2a4d27d8624ca680404de34670e499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a48f066f00486d8a90d19d3bf7e42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbab5538e5648c690c45fe318e3cc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c211bad691f349c881010fc3be195fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1172db947394eef8db53485a5bbb248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1abd247a78d4af984ff6069f92cfb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb18fa019f584e80a4df8cb7d930273c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= calculate_dtype, #calculate_dtype can be bf16 or float32- use bf16 if supported\n",
    "    bnb_4bit_use_double_quant= True\n",
    "    )\n",
    "repo = \"google/gemma-2-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, quantization_config= bnb_config, device_map= \"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzT757Suokja"
   },
   "source": [
    "#Check model memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D127_Z6FZ5WN",
    "outputId": "22792f9c-e3cd-48bb-e66e-9df246bc77ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2090.7119140625\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQyQHN6zos1k"
   },
   "source": [
    "#model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vm7RO981Z5Yh",
    "outputId": "2220b9ff-da87-4049-fc4e-28cbddb9d2a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSh3kYaioxjD"
   },
   "source": [
    "#Prepare model for kbit training\n",
    "##Use Lora Config\n",
    "\n",
    "\n",
    "1.   rank [4,8,16,32] - choose one\n",
    "2.   lora_alpha is a scalling factor which should be 2x the rank of matrix.\n",
    "3.   dropout range from 0.03 to 0.10 which helps prevent overfit\n",
    "4.   module - choose module as per requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzMopk81Z5ao",
    "outputId": "5ea9242f-b314-4ca9-94db-3dac66b0ab52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9216, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): GELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 8, #. rank of LoRA - [4-16]\n",
    "    bias = \"none\", # [\"all\", \"lora_only\"] - for train bias term\n",
    "    lora_alpha = 16, # scalling factor\n",
    "    lora_dropout = 0.10, # prevent overfit- used for regularisation\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    "\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzA5DOMp5aA"
   },
   "source": [
    "#once again check memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjtETM3QZ5fK",
    "outputId": "4b7fbdc0-b342-496c-be21-369af3df6d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3255.78271484375\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z2XsrHMp9kr"
   },
   "source": [
    "#Print base model to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGNI99ZVZ5hr",
    "outputId": "bd697b7a-a955-4872-a585-106e688fa03a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Gemma2ForCausalLM(\n",
      "      (model): Gemma2Model(\n",
      "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-25): 26 x Gemma2DecoderLayer(\n",
      "            (self_attn): Gemma2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Gemma2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=9216, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): GELUTanh()\n",
      "            )\n",
      "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (rotary_emb): Gemma2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.get_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiBoFpSgZ5j2",
    "outputId": "1267f109-4387-489f-8a4b-86974dfce7fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3413.935616\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSmHEA7cqHKh"
   },
   "source": [
    "#Check for trainable Parameters and its percentage for a mathematical view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ff-gwq_dZ5mO",
    "outputId": "098134eb-849b-4640-9bde-1aaa1ac88c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 10,383,360\n",
      "Total Parameters: 2,624,725,248\n",
      "Percentage Trainable: 0.40%\n"
     ]
    }
   ],
   "source": [
    "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Percentage Trainable: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oOA2lywqYKD"
   },
   "source": [
    "#ETL Process for Dataset Prep stage, Tokenizer load and define chat template if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736,
     "referenced_widgets": [
      "558407ec79b0431c91dd3723138b0161",
      "82af9324d53444879f2ac961b4ba8b23",
      "552134f19ec34f4aab255562413af841",
      "060c43137c0c43d2b38f5ede2038dfc5",
      "66cb9c7db6044cda8bc8c9ecd7b5c965",
      "2c16f1b3520f4c60926bf5f1d53ca259",
      "3d4788d8b7db4c1681d6d84b2e129bfd",
      "f7213f7869f9435fb8b3d3612b98a45f",
      "42b498489b9f4e2a8e1dbc42c590cd74",
      "472404a6a64a4e6dbfbdc78aeca32527",
      "8d6c4b23ff164f6886aef584570c980b",
      "9c6803335aca4bb8b699acdcaa36e754",
      "c9de63c5b25943e492f1988e26b41234",
      "9dca8b3720104549b12e2589c4dfb890",
      "63cadeed2ebf4a3ea1f7de4d3c5e3673",
      "9099d2c7308345ba812c2c65b58b757c",
      "75893ee41e7d4f92a0c253476d163362",
      "ea0472998bbf4425be6481f448ae3f8b",
      "1fb7b3fc01a3461ab3495d4802990467",
      "1d22e1641ebb460587ad60c8b4a4ca7d",
      "d1e57df991c34786880afbc78428e4c9",
      "ad0e8237fe04482fb4a9320439f26a5e",
      "90ddcd6c1f6049f58f2510b11bcbaf96",
      "0be050caea0944d59561e014e4058521",
      "6db30e2b842a456a99e1a6c47f079794",
      "acb30cf7f4b548f9b3dc0ab0762cb022",
      "da7fad88a97341d698e5b9d8f32f1d83",
      "eaf6eea3953d44aaa16bffba57a9dab1",
      "eb5f5769bd4a48acb6075ba74dff6f91",
      "e266568a6e1844c1a51a970071092ee2",
      "f27bdc4c1b914044a51c03e9fe54d84f",
      "b77aa7259633413c9357938307f4edf7",
      "8dc5b1ad146442a6aed2f5ec22c0787c",
      "f9b9e3635eda40a7b7c0db525e3e2fef",
      "95c1ed9ae4df4154b6824cbf21f02308",
      "fa57783727194202a001cd7ba6fe4b27",
      "9220b9174c2b414aaac22a0ac55192dd",
      "dfa383cd94f64ee5a9414ba37387f17d",
      "9e053373b81d41289d134c0d2082b703",
      "12980408516146f498db028e445a4a83",
      "4060d16298ec412f8460d1eba10b6775",
      "8b7c1d66d92c4062928957f6e8a5f0a1",
      "0d43badf795d4c9d887a3ba5d73b022b",
      "4c3d8558b93c40bcb3423e3f000b4784",
      "7a4f8d8db0e248fcbe4d7dbaf1339d66",
      "11a02d14d2ce46238e3217700a318ace",
      "bd7402bfaddb4275b365c9c48256dd48",
      "36378648c72f491eadf3b837ddb4ecd1",
      "f8e219855bcb41119def7aabe1687528",
      "7007b2758bac49b0ab7cc5e98bb07200",
      "4e28c53104cd4e77905829d0097cdfdc",
      "7a50c0022d114535be4cf2d98c739c5d",
      "4abedfe0f7a5474184ef8c4ded3f77ab",
      "b4e49d02cad04f6b98730ee0b4ca8d1c",
      "a0d201493ff542168bd10a044f612b9a",
      "de32b867e9b04ebc8e23a7f516dd3c59",
      "b2b36034c0924e4d994044af03963c3d",
      "f9faf34ffac9463598fffa895e45b9a5",
      "e1bf17ee8c124d2f877b9873c083260e",
      "03f82da47eb5425fb65053ac8572982d",
      "251494a7a98b4f439269cc17284e6287",
      "5f44a66b992d484e8238c6ddf0c32dc5",
      "d617ec1776f74973bdf4ceb0d8909260",
      "c736e0d5832f4374b685f4f8e8210c6d",
      "5637eb6cc5604966955af9be2cd00fb8",
      "74f6939e59c14cd0ae17f9c5f238dbe6",
      "5fd752ecde5b43d6847538c362c2c4ef",
      "e2640665640943f8839d6dfd3e65182d",
      "6f1cf9534e354e08ae6b3be77e426222",
      "95131ea85be148bfb093a6e30d244e4a",
      "02ca67b9f9854eaa805ead184e4890b0",
      "393730b838c148839e8019686a235d21",
      "78e3d1e36e244a5b846b3f45eb592182",
      "9dcb5613c0d744d58cacd41fd08b1a8f",
      "34487aed2ae949b6bb27ac8eafce31e9",
      "d04f2edc6fae4dad94f82b3aa1ffe657",
      "061d1a3495244e90b6ddf197399ac2cc",
      "10128a3932854db7a03994417cf0baa6",
      "9b85da4fa6674b5484e966e50741bc85",
      "f05efba5e5424c93ad005eaa0567c812",
      "431a1d9556ca4ae2b317b4fdb9731b3a",
      "558975e995c949c180a4eb23d62a4bc5",
      "de60a059deca41c1828c66ea73dc109a",
      "42e0e8bf25b0418bae3ef29984cfb97d",
      "f2e054bd391b4470970ec213dea2f76e",
      "78122a16305147c68d921fab8c7385ed",
      "ce905332734a41cca75ddda5d7667c5b",
      "4dddff31dd0d4eb4850a5fe344b4672f",
      "8fc0598962ee447596d5db151f809389",
      "e94f118a8ee646288a59f737d00cf352",
      "97abea2c155e49ca9aa4925b5eb41a3c",
      "3bbdc91cc66941f79035b45b71802e3b",
      "ba49f0711e0041fbbbdfe90ce9537cd8",
      "386fc97cd45644f3b569e365667c9fe3",
      "271169493a6b4145bff368b5481ef67b",
      "099407d1126d42f1ad553b7d056b252f",
      "fb8c7756901741daa6c6b827b6aadbd2",
      "af700b1f466344a0bce310e2e7c52d73",
      "c2cd2edd9b6243689a560ba19feba215"
     ]
    },
    "id": "K3Iox1kEre3Q",
    "outputId": "3623be0a-ca47-40ad-adf1-fcfdd189beb2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558407ec79b0431c91dd3723138b0161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6803335aca4bb8b699acdcaa36e754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ddcd6c1f6049f58f2510b11bcbaf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b9e3635eda40a7b7c0db525e3e2fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChatDoctor-HealthCareMagic dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4f8d8db0e248fcbe4d7dbaf1339d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/542 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de32b867e9b04ebc8e23a7f516dd3c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-5e7cb295b9cff0(…):   0%|          | 0.00/70.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd752ecde5b43d6847538c362c2c4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/112165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10000 samples\n",
      "Sample entry: {'instruction': \"If you are a doctor, please answer the medical questions based on the patient's description.\", 'input': 'I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!', 'output': 'Hi, Thank you for posting your query. The most likely cause for your symptoms is benign paroxysmal positional vertigo (BPPV), a type of peripheral vertigo. In this condition, the most common symptom is dizziness or giddiness, which is made worse with movements. Accompanying nausea and vomiting are common. The condition is due to problem in the ear, and improves in a few days on own. Betahistine tablets would help relieve your symptoms. Doing vestibular rehabilitation or adaptation exercises would prevent the recurrence of these symptoms. An ENT evaluation would also help. I hope it helps. Best wishes, Chat Doctor.'}\n",
      "Dataset columns: ['instruction', 'input', 'output']\n",
      "\n",
      "Formatting medical Q&A prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10128a3932854db7a03994417cf0baa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting prompts with Gemma-2 chat template:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc0598962ee447596d5db151f809389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing medical conversations:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✅ MEDICAL DATASET PREPARATION COMPLETE!\n",
      "============================================================\n",
      "📊 Total samples: 10000\n",
      "📏 Max sequence length: 1024\n",
      "🔑 Dataset keys: ['input_ids', 'attention_mask', 'labels']\n",
      "💾 Approximate size: 39.06 MB\n",
      "\n",
      "🚀 Ready to use 'final_dataset' in your SFTTrainer!\n",
      "============================================================\n",
      "\n",
      "📋 Sample formatted prompt (first 500 chars):\n",
      "<start_of_turn>user\n",
      "I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relie...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ============================================\n",
    "# SETUP: Gemma-2-2B Model & Tokenizer\n",
    "# ============================================\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ============================================\n",
    "# LOAD MEDICAL DATASET\n",
    "# ============================================\n",
    "print(\"Loading ChatDoctor-HealthCareMagic dataset...\")\n",
    "\n",
    "# Load the medical Q&A dataset (using 10k samples for reasonable training time)\n",
    "raw_dataset = load_dataset(\n",
    "    \"lavita/ChatDoctor-HealthCareMagic-100k\",\n",
    "    split=\"train[:10000]\"  # Using first 10k samples\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(raw_dataset)} samples\")\n",
    "print(f\"Sample entry: {raw_dataset[0]}\")\n",
    "print(f\"Dataset columns: {raw_dataset.column_names}\")\n",
    "\n",
    "# ============================================\n",
    "# FORMAT PROMPTS FOR GEMMA-2\n",
    "# ============================================\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"\n",
    "    Format medical Q&A into Gemma-2's chat format with safety disclaimer\n",
    "\n",
    "    Gemma-2 uses a specific chat template format:\n",
    "    <start_of_turn>user\n",
    "    {user message}<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    {assistant response}<end_of_turn>\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract patient question and doctor response\n",
    "    # The dataset has 'input' (patient question) and 'output' (doctor answer)\n",
    "    patient_question = example[\"input\"]\n",
    "    doctor_response = example[\"output\"]\n",
    "\n",
    "    # Add safety disclaimer to medical responses\n",
    "    safe_response = (\n",
    "        f\"{doctor_response}\\n\\n\"\n",
    "        \"⚕️ Disclaimer: This information is for educational purposes only. \"\n",
    "        \"Please consult a qualified healthcare professional for medical advice.\"\n",
    "    )\n",
    "\n",
    "    # Format in Gemma-2 chat template\n",
    "    prompt = (\n",
    "        f\"<start_of_turn>user\\n\"\n",
    "        f\"{patient_question}<end_of_turn>\\n\"\n",
    "        f\"<start_of_turn>model\\n\"\n",
    "        f\"{safe_response}<end_of_turn>\"\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# ============================================\n",
    "# TOKENIZATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the formatted text with proper padding and labels\"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,  # Increased for medical responses (often longer)\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # Create labels for causal LM training\n",
    "    # Labels = input_ids, but -100 for padded tokens (ignored in loss)\n",
    "    labels = []\n",
    "    for input_ids, attention_mask in zip(tokenized[\"input_ids\"], tokenized[\"attention_mask\"]):\n",
    "        # Convert to list if needed\n",
    "        label = input_ids.copy() if isinstance(input_ids, list) else list(input_ids)\n",
    "\n",
    "        # Set padded positions to -100 (ignored in loss calculation)\n",
    "        for i, mask in enumerate(attention_mask):\n",
    "            if mask == 0:  # This is a padded token\n",
    "                label[i] = -100\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# ============================================\n",
    "# PROCESS DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFormatting medical Q&A prompts...\")\n",
    "formatted_dataset = raw_dataset.map(\n",
    "    format_prompt,\n",
    "    desc=\"Formatting prompts with Gemma-2 chat template\"\n",
    ")\n",
    "\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset.column_names,  # Remove original columns\n",
    "    desc=\"Tokenizing medical conversations\"\n",
    ")\n",
    "\n",
    "# Final dataset ready for training\n",
    "final_dataset = tokenized_dataset\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ MEDICAL DATASET PREPARATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 Total samples: {len(final_dataset)}\")\n",
    "print(f\"📏 Max sequence length: {len(final_dataset[0]['input_ids'])}\")\n",
    "print(f\"🔑 Dataset keys: {final_dataset.column_names}\")\n",
    "print(f\"💾 Approximate size: {len(final_dataset) * 1024 * 4 / 1024 / 1024:.2f} MB\")\n",
    "print(\"\\n🚀 Ready to use 'final_dataset' in your SFTTrainer!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Optional: Print a sample to verify formatting\n",
    "print(\"\\n📋 Sample formatted prompt (first 500 chars):\")\n",
    "print(formatted_dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "rWTIyS5oAySa",
    "outputId": "a2647010-0d98-4def-ae78-5dd2a0f08e9c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SFTConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1911986495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# My SFT configuration for Gemma-2-2B medical assistant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m sft_config = SFTConfig(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# I'm saving my medical model to a dedicated directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/gemma-2-2b-medical/Gemma-2-2B-MedicalQA-finetuned'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SFTConfig' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# My optimized parameters for LoRA training on medical data\n",
    "min_effective_batch_size = 4  # Reduced for 2B model with longer sequences (1024 tokens)\n",
    "lr = 2e-5  # Slightly lower LR for Gemma-2 stability with medical data\n",
    "max_seq_length = 1024  # Increased for detailed medical responses\n",
    "collator_fn = None  # I'm not using a custom collator since I pre-pad in tokenization\n",
    "packing = False  # I disabled packing since I'm using fixed-length sequences\n",
    "steps = 20  # My logging and saving frequency (adjusted for larger dataset)\n",
    "num_train_epochs = 3  # Standard for medical fine-tuning with LoRA\n",
    "warmup_ratio = 0.1  # Warmup for stable training start\n",
    "\n",
    "# My SFT configuration for Gemma-2-2B medical assistant\n",
    "sft_config = SFTConfig(\n",
    "    # I'm saving my medical model to a dedicated directory\n",
    "    output_dir = '/content/drive/MyDrive/gemma-2-2b-medical/Gemma-2-2B-MedicalQA-finetuned',\n",
    "\n",
    "    # My data processing settings\n",
    "    packing = packing,\n",
    "    max_seq_length = max_seq_length,\n",
    "\n",
    "    # I disabled gradient checkpointing (not needed with 4-bit quantization)\n",
    "    gradient_checkpointing = False,\n",
    "\n",
    "    # My training batch and precision settings\n",
    "    # Note: Smaller batch size due to longer sequences (1024 vs 512)\n",
    "    per_device_train_batch_size = min_effective_batch_size,\n",
    "    auto_find_batch_size = True,  # Let trainer optimize for available VRAM\n",
    "    bf16 = True,  # Using bf16 for better numerical stability (important for medical accuracy)\n",
    "\n",
    "    # My training schedule optimized for medical domain\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    learning_rate = lr,\n",
    "    lr_scheduler_type = \"cosine\",  # Smooth learning rate decay\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    weight_decay = 0.01,  # Regularization to prevent overfitting on medical terminology\n",
    "    max_grad_norm = 1.0,  # Gradient clipping for stability\n",
    "\n",
    "    # My logging and monitoring setup\n",
    "    report_to = 'wandb',  # Tracking my medical AI experiment\n",
    "    run_name = \"Gemma-2-2B-MedicalQA-LoRA-r8-alpha16\",  # Descriptive run name with LoRA config\n",
    "\n",
    "    # My logging directory\n",
    "    logging_dir = '/content/drive/MyDrive/gemma-2-2b-medical/Gemma-2-2B-MedicalQA-finetuned/logs',\n",
    "\n",
    "    # My checkpoint and logging strategy\n",
    "    logging_strategy = 'steps',\n",
    "    save_strategy = 'steps',\n",
    "    logging_steps = steps,  # I log every 20 steps\n",
    "    save_steps = steps,     # I save checkpoint every 20 steps (more frequent due to larger dataset)\n",
    "    save_total_limit = 2,   # Keep last 2 checkpoints to save Google Drive space\n",
    ")\n",
    "\n",
    "# I create my trainer with the medical dataset and configuration\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                    # My Gemma-2-2B model with LoRA adapters (rank=8, alpha=16, dropout=0.1)\n",
    "    train_dataset = final_dataset.select(range(1500)),    # My 10k medical Q&A dataset\n",
    "    processing_class = tokenizer,     # Gemma-2 tokenizer\n",
    "    data_collator = collator_fn,      # Using default collator\n",
    "    args = sft_config,               # My medical training configuration\n",
    ")\n",
    "\n",
    "# I start the medical fine-tuning process\n",
    "print(\"🏥 Starting Gemma-2-2B Medical Fine-tuning...\")\n",
    "print(f\"📊 Training on {len(final_dataset.select(range(1500)))} medical Q&A samples\")\n",
    "print(f\"🔧 LoRA Config: rank=8, alpha=16, dropout=0.1\")\n",
    "print(f\"💾 4-bit quantization: nf4\")\n",
    "print(f\"⏱️ Estimated time: ~2-3 hours on T4 GPU\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Medical fine-tuning completed!\")\n",
    "print(f\"💾 Model saved to: {sft_config.output_dir}\")\n",
    "print(\"🏥 Your Gemma-2-2B Medical Assistant is ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7SyBkyK-S-a"
   },
   "outputs": [],
   "source": [
    "# Step 1: I'm saving my trained medical model locally first\n",
    "print(\"💾 Saving my trained Gemma-2-2B medical model...\")\n",
    "trainer.save_model('/content/gemma-2-medical-saved')\n",
    "\n",
    "# Step 2: I load and merge the LoRA adapter with the base model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print(\"🔧 Loading my PEFT model and merging adapter...\")\n",
    "# I load the saved PEFT model (use the same path as Step 1)\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained('/content/gemma-2-medical-saved')\n",
    "\n",
    "# I merge and unload the adapter to get a single model\n",
    "print(\"⚙️ Merging LoRA weights into base model...\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Step 3: I save the merged model with tokenizer\n",
    "print(\"💾 Saving my merged medical model...\")\n",
    "merged_model.save_pretrained('/content/gemma-2-medical-merged')\n",
    "tokenizer.save_pretrained('/content/gemma-2-medical-merged')\n",
    "\n",
    "# Step 4: I upload my model to Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"☁️ Uploading my medical assistant to Hugging Face Hub...\")\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path='/content/gemma-2-medical-merged',\n",
    "    repo_id=\"sweatSmile/Gemma-2-2B-MedicalQA-Assistant\",  # My medical AI repo\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload Gemma-2-2B fine-tuned on 1.5k medical Q&A (ChatDoctor-HealthCareMagic) with LoRA (r=8, alpha=16)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Model upload completed! 🎉\")\n",
    "print(\"🏥 Your Gemma-2-2B Medical Assistant is now live!\")\n",
    "print(\"=\"*60)\n",
    "print(\"🔗 Model URL: https://huggingface.co/sweatSmile/Gemma-2-2B-MedicalQA-Assistant\")\n",
    "print(\"\\n📋 Model Details:\")\n",
    "print(\"   - Base: google/gemma-2-2b-it\")\n",
    "print(\"   - Dataset: ChatDoctor-HealthCareMagic (1.5k samples)\")\n",
    "print(\"   - LoRA: rank=8, alpha=16, dropout=0.1\")\n",
    "print(\"   - Quantization: 4-bit (nf4)\")\n",
    "print(\"   - Domain: Medical Q&A with safety disclaimers\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
