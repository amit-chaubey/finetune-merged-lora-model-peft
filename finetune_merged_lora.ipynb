{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNRwpO4-0g6J"
   },
   "outputs": [],
   "source": [
    "!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "QsJWTbybBC3J"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "XRun9pTxBL_G",
    "outputId": "0f11f430-e8d7-47fc-e0ae-99f3d1f5ec66"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchaubey-amit017\u001b[0m (\u001b[33mhectorlabs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "LtRBoKavllJM",
    "outputId": "ebb46b7d-cb70-4d44-b360-4ea060a96c21"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "cannot import name 'DataCollatorForCompletionOnlyLM' from 'trl' (/usr/local/lib/python3.11/dist-packages/trl/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2411934390.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoPeftModelForCausalLM\u001b[0m \u001b[0;31m# Added to peft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook_login\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSFTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup_chat_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForCompletionOnlyLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataCollatorForCompletionOnlyLM' from 'trl' (/usr/local/lib/python3.11/dist-packages/trl/__init__.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    #AutoPeftModelForCausalLM, # Removed from transformers\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM # Added to peft\n",
    "from huggingface_hub import notebook_login\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jIgr9imllLq",
    "outputId": "70958fff-6e56-4ab0-ecb4-5bdf941381de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation = False)\n",
    "print(supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jvf95PBmllOG",
    "outputId": "a99ba647-7965-45a0-c02c-2c79b656b15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "dtype_compute = torch.bfloat16 if supported else torch.float16\n",
    "print(dtype_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zlkMRk6DllQG"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = dtype_compute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "651c1da5758246389b0a03ae4ddf3ef3",
      "ae7cd4242c63431fa6c40eefbf25d950",
      "eb158839bc8e4d758be941c89b690fa7",
      "b8d3c42bc9d24d9cb7c7b2f2fcc4bbcd",
      "2f235a85aca34b98b70e6dca41e928e8",
      "a960c19470a54d18a3d0be57a2de69d7",
      "ed71ea891020400caa7c6a4077c14b46",
      "76449e4ab6514d76a15a560962e1d454",
      "b6fd2ec60fa842388412e362f4575660",
      "97727e648a134853a3ff3621fc4de8f6",
      "5752d8180b36463cb8a8e88d7b4eb68f",
      "6e9cd03a471b452b9394de6cd8610c27",
      "818453e7b0eb423c80a3c9d85dfa6c37",
      "2ac5e32b20eb49859b5f15aacad4c860",
      "a4c5b01a78be49d69bb9afa5d3ba7a10",
      "ddab7b9613754a58a4b663cf3a1536b5",
      "4ccf3adfb56549b28821ddf00b216aec",
      "bdb7df9dbcfb42188028ae34e36ee2cd",
      "171ef5a0bc984de19863e9e334e0c269",
      "a1016f329ab3496d8b2f5a5b6ef45ed1",
      "57df89be99634dd081eb2b454e31bcf9",
      "61861a3bf6e14c8e865575a09aa5011d",
      "7c2f76ec05654222af58e36cde189401",
      "979ede33163b4b8fbce8e0639e20bd02",
      "aed1f2b08e9c485b8d511b7fa1d69f18",
      "f25f5d78b26a48ab9b4fb831c7d782a3",
      "ef3c9911a112472aaba9607cd8c1e336",
      "6935f097ea6c444fafd8e27e56342ac6",
      "ddb021adee83494b8181cee2961830f5",
      "c96d83d03e8b48a892f5da8abdf6b3ac",
      "f1b35f50345e4d0f992f250a4663da79",
      "682b8c316d404dcd9dacbb64818192a2",
      "87f7607e47004dfe989002087ce38ee3",
      "7aaff3d5a2ee49c5abe7100d9bf26f14",
      "7285f5b43155436a9d7b58d70b768965",
      "72e8822cafb44e9e99baf601115587e2",
      "f6599eb00c7a4cbb84b291baf140d080",
      "59e085f4a7e444d49e5d618c8273a6be",
      "9030a2222fc449ffae590987107014b7",
      "98f2085b3b9a4e948f6b3c59fa777fbc",
      "14d22b4803d54a71a276a6e653c89680",
      "3ea7f06e8a1a4a9ebf4b8ac2f6d3b662",
      "218492f8d38549e299f56a558146f647",
      "05f86f0f7b334a728fa39f90ce21a78f",
      "0207b96b661f49ab81bfe887363d3047",
      "455f58fb68a144c2b40bd168a9883c72",
      "d86b0b75272549d5b1261db83157fe94",
      "0546bfcd02c94709b7916174a0e5638b",
      "5afd22f9199246bb9fecc9493fee3299",
      "49f4edd85cb94ed0ae3cbbac30b7a348",
      "f2acd886a00940cba4d91786ebbb1c6f",
      "05205b1a2bbf4668b876eb2a18a7b87c",
      "bad7968f8b9349b5adde09e4e2b97af2",
      "b493775ae1944647a45ce39bd969fbb6",
      "f5652052f3444b6e88ed29dd9d04c814",
      "ebdf9081c7d24d12a7635e27435d7962",
      "18e8a902b58641468153793f5f59af14",
      "a46b3fb2756e417ea3acc79e20de4646",
      "8068cfafe7a144eb8137ea7cef4911f9",
      "72797d483d714e86a53a0ff93a58fc8d",
      "5a1c2149e475451fbda9ebea7079a4eb",
      "7e822e1190e84af8b585525748dd81fd",
      "99027be2f04c4b7f934c958d2686f221",
      "de62c7a8d0d14ee19330afd187b3bb47",
      "0429d7c60fe04f41bc65c7193ebc0c58",
      "55f58c57dcd14a87888d8f3928d9e77b",
      "d21d2be3cdda4535b8ce4c2ff97c5e59",
      "aa03dffe904c435ba7ab6e676aa94398",
      "4f393bea989a4e45bb357385c690ea6a",
      "553609bd8e5b4838847f009d09b86fb7",
      "a6c44187bc1e4712a06b339a09fda19f",
      "9348bfb334f7457b8bee8ac5445f3514",
      "7b4f66bf1d7f4ddaaf35dc4cf608cfd8",
      "722aff75b64f48a7ad336244b96bd882",
      "e24398569f31415ab1cd98704036a79b",
      "499c92fbed49420fa6f9235a943dfd41",
      "f88e42620956418280f5880654caa25c"
     ]
    },
    "id": "0G_UznYellSU",
    "outputId": "bd0be850-2290-4371-8bf0-dd67b7908281"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651c1da5758246389b0a03ae4ddf3ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9cd03a471b452b9394de6cd8610c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2f76ec05654222af58e36cde189401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaff3d5a2ee49c5abe7100d9bf26f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0207b96b661f49ab81bfe887363d3047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdf9081c7d24d12a7635e27435d7962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21d2be3cdda4535b8ce4c2ff97c5e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/182 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM3-3B\", device_map = \"cuda:0\", quantization_config = bnb_config\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUcIywwg2PvB",
    "outputId": "da35ac30-ff21-40c7-bebc-4fa1b5362948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolLM3ForCausalLM(\n",
      "  (model): SmolLM3Model(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x SmolLM3DecoderLayer(\n",
      "        (self_attn): SmolLM3Attention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): SmolLM3MLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): SmolLM3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): SmolLM3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): SmolLM3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): SmolLM3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ba4b9Lfl4XHg",
    "outputId": "09722508-b0aa-4846-e449-0e7c58637444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o_proj', 'gate_proj', 'v_proj', 'down_proj', 'up_proj', 'k_proj', 'q_proj']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 3B models\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "print(find_all_linear_names(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "53K0F2KZpBnV"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules = [\"query_key_value\", \"o_proj\", \"qkv_proj\", \"gate_up_proj\", \"down_proj\"],\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184,
     "referenced_widgets": [
      "f04797b09ce840b99702f67071416931",
      "6bbd3afaf77046ef93fcd84c292a05bc",
      "fe8eb332e26049788c1afddf8aac0f0d",
      "03cd9ca5035747ca8313c09aa081fece",
      "772e2f8d07c140fa84d42d78f704fe91",
      "cca6ee02acc646738b4c2e9fb71431c5",
      "d25778eb2a434799a7c0db2278b60702",
      "1852cb026b564c8faa893ddcfd140445",
      "08ec0b84fc514573b78e3491d9170054",
      "14b2de323edc4fa9a0c7001fefe3e3c2",
      "d1eee0160d8a47888b49eb161895a353",
      "574968b6298b4b9cabaf624e3d8c2e3c",
      "726ddea1aa5f4ecb9f17dbe42b389670",
      "537a70e1a7334c21a552b4a0ab0e6210",
      "a220b65c2e174190b6a3aaeb6cea3b51",
      "3e19312a3964496ea3809d1ad137ce9a",
      "b50a74fd941a4219a833f8bb972da409",
      "e5477c77fc9e40cb92caba75e9bde69b",
      "3b93a183b6cf44e7914f62e9fbfc3fbc",
      "6c3c8468c82341ef940dcd6504379017",
      "75c8923663e14d52a9e332727bbc3c4b",
      "a8f631305a3e4a18ace029be33026b9c",
      "7bcac43b895a4f27a22af80af61500bd",
      "db1060d548224a35a1cad6d821f61ad9",
      "2e1277f300e64d579415a88c0ebd7b15",
      "6ff7f18a11514d2699e580046191fc1b",
      "0bb3c2ece599423b94ef9e0b0abab566",
      "943bafbadc0640229056db1dc4cb23bf",
      "83197b5327524333b4516d8a739a66ea",
      "cc395c64f43543ba81e3644448e5e0c4",
      "ce175e1345a7411f8e719d0bf916c339",
      "caec49e8b98145369f077f156f7aeb5c",
      "07bc4b6a063d40828c03011cc0e3a907"
     ]
    },
    "id": "gQjULn735xAC",
    "outputId": "02b10208-b5d0-4b59-c5b2-1cc07cb3e9f4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04797b09ce840b99702f67071416931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574968b6298b4b9cabaf624e3d8c2e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sarcastic_dataset_cleaned.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcac43b895a4f27a22af80af61500bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sweatSmile/sarcastic-dataset\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPL1Pouu50uw",
    "outputId": "205412ff-ea9b-40de-8016-bbdeac05e6e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.remove_columns([\"translation\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfS0SfOb6X_G",
    "outputId": "1b4b07ad-9226-48a7-f201-d0fdc9ee1d60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'The birch canoe slid on the smooth planks.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Oh, wow, a birch canoe slid on smooth planks? Who would have thought that would happen? Next, you'll tell me water's wet and the sky is blue!\"}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0]['prompt']},\n",
    "    {\"role\": \"assistant\", \"content\": dataset[0]['completion']}\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "collapsed": true,
    "id": "mZ2Bw6pZ55EQ",
    "outputId": "31031779-9977-406c-808e-d1e6a497936d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'{# ───── defaults ───── #}\\n{%- if enable_thinking is not defined -%}\\n{%- set enable_thinking = true -%}\\n{%- endif -%}\\n\\n{# ───── reasoning mode ───── #}\\n{%- if enable_thinking -%}\\n  {%- set reasoning_mode = \"/think\" -%}\\n{%- else -%}\\n  {%- set reasoning_mode = \"/no_think\" -%}\\n{%- endif -%}\\n\\n{# ───── header (system message) ───── #}\\n{{- \"<|im_start|>system\\\\n\" -}}\\n\\n{%- if messages[0].role == \"system\" -%}\\n  {%- set system_message = messages[0].content -%}\\n  {%- if \"/no_think\" in system_message -%}\\n    {%- set reasoning_mode = \"/no_think\" -%}\\n  {%- elif \"/think\" in system_message -%}\\n    {%- set reasoning_mode = \"/think\" -%}\\n  {%- endif -%}\\n  {%- set custom_instructions = system_message.replace(\"/no_think\", \"\").replace(\"/think\", \"\").rstrip() -%}\\n{%- endif -%}\\n\\n{%- if \"/system_override\" in system_message -%}\\n  {{- custom_instructions.replace(\"/system_override\", \"\").rstrip() -}}\\n  {{- \"<|im_end|>\\\\n\" -}}\\n{%- else -%}\\n  {{- \"## Metadata\\\\n\\\\n\" -}}\\n  {{- \"Knowledge Cutoff Date: June 2025\\\\n\" -}}\\n  {%- set today = strftime_now(\"%d %B %Y\") -%}\\n  {{- \"Today Date: \" ~ today ~ \"\\\\n\" -}}\\n  {{- \"Reasoning Mode: \" + reasoning_mode + \"\\\\n\\\\n\" -}}\\n  \\n  {{- \"## Custom Instructions\\\\n\\\\n\" -}}\\n  {%- if custom_instructions -%}\\n    {{- custom_instructions + \"\\\\n\\\\n\" -}}\\n  {%- elif reasoning_mode == \"/think\" -%}\\n    {{- \"You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\\\\n\\\\n\" -}}\\n  {%- else -%}\\n    {{- \"You are a helpful AI assistant named SmolLM, trained by Hugging Face.\\\\n\\\\n\" -}}\\n  {%- endif -%}\\n\\n  {%- if xml_tools or python_tools or tools -%}\\n    {{- \"### Tools\\\\n\\\\n\" -}}\\n    {%- if xml_tools or tools -%}\\n      {%- if tools -%}\\n        {%- set xml_tools = tools -%}\\n      {%- endif -%}\\n      {%- set ns = namespace(xml_tool_string=\"You may call one or more functions to assist with the user query.\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n\\\\n<tools>\\\\n\") -%}\\n      {%- for tool in xml_tools[:] -%} {# The slicing makes sure that xml_tools is a list #}\\n        {%- set ns.xml_tool_string = ns.xml_tool_string ~ (tool | string) ~ \"\\\\n\" -%}\\n      {%- endfor -%}\\n      {%- set xml_tool_string = ns.xml_tool_string + \"</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call>\" -%}\\n      {{- xml_tool_string -}}\\n    {%- endif -%}\\n    {%- if python_tools -%}\\n      {%- set ns = namespace(python_tool_string=\"When you send a message containing Python code between \\'<code>\\' and \\'</code>\\' tags, it will be executed in a stateful Jupyter notebook environment, and you will then be given the output to continued reasoning in an agentic loop.\\\\n\\\\nYou can use the following tools in your python code like regular functions:\\\\n<tools>\\\\n\") -%}\\n      {%- for tool in python_tools[:] -%} {# The slicing makes sure that python_tools is a list #}\\n        {%- set ns.python_tool_string = ns.python_tool_string ~ (tool | string) ~ \"\\\\n\" -%}\\n      {%- endfor -%}\\n      {%- set python_tool_string = ns.python_tool_string + \"</tools>\\\\n\\\\nThe state persists between code executions: so variables that you define in one step are still available thereafter.\" -%}\\n      {{- python_tool_string -}}\\n    {%- endif -%}\\n    {{- \"\\\\n\\\\n\" -}}\\n    {{- \"<|im_end|>\\\\n\" -}}\\n  {%- endif -%}\\n{%- endif -%}\\n{# ───── main loop ───── #}\\n{%- for message in messages -%}\\n    {%- set content = message.content if message.content is string else \"\" -%}\\n    {%- if message.role == \"user\" -%}\\n        {{ \"<|im_start|>\" + message.role + \"\\\\n\"  + content + \"<|im_end|>\\\\n\" }}\\n    {%- elif message.role == \"assistant\" -%}\\n        {% generation %}\\n        {%- if reasoning_mode == \"/think\" -%}\\n            {{ \"<|im_start|>assistant\\\\n\" + content.lstrip(\"\\\\n\") + \"<|im_end|>\\\\n\" }}\\n        {%- else -%}\\n            {{ \"<|im_start|>assistant\\\\n\" + \"<think>\\\\n\\\\n</think>\\\\n\" + content.lstrip(\"\\\\n\") + \"<|im_end|>\\\\n\" }}\\n        {%- endif -%}\\n        {% endgeneration %}\\n    {%- elif message.role == \"tool\" -%}\\n    {{ \"<|im_start|>\" + \"user\\\\n\"  + content + \"<|im_end|>\\\\n\" }}\\n    {%- endif -%}\\n{%- endfor -%}\\n{# ───── generation prompt ───── #}\\n{%- if add_generation_prompt -%}\\n    {%- if reasoning_mode == \"/think\" -%}\\n        {{ \"<|im_start|>assistant\\\\n\" }}\\n    {%- else -%}\\n        {{ \"<|im_start|>assistant\\\\n\" + \"<think>\\\\n\\\\n</think>\\\\n\"  }}\\n    {%- endif -%}\\n{%- endif -%}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psZCAeGs55Hu",
    "outputId": "8b2c3f99-11f1-4c52-8ebb-4cba13e51cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 05 August 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "<|im_start|>user\n",
      "The birch canoe slid on the smooth planks.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Oh, wow, a birch canoe slid on smooth planks? Who would have thought that would happen? Next, you'll tell me water's wet and the sky is blue!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "u6va_hEV4Egc"
   },
   "outputs": [],
   "source": [
    "# def modify_tokenizer(tokenizer,\n",
    "#                      alternative_bos_token='<|im_start|>',\n",
    "#                      alternative_unk_token='<unk>',\n",
    "#                      special_tokens=None,\n",
    "#                      tokens=None):\n",
    "#     eos_token, bos_token = tokenizer.eos_token, tokenizer.bos_token\n",
    "#     pad_token, unk_token = tokenizer.pad_token, tokenizer.unk_token\n",
    "\n",
    "#     # BOS token must be different than EOS token\n",
    "#     if bos_token == eos_token:\n",
    "#         bos_token = alternative_bos_token\n",
    "\n",
    "#     # UNK token must be different than EOS token\n",
    "#     if unk_token == eos_token:\n",
    "#         unk_token = alternative_unk_token\n",
    "\n",
    "#     # PAD token must be different than EOS token\n",
    "#     # but can be the same as UNK token\n",
    "#     if pad_token == eos_token:\n",
    "#         pad_token = unk_token\n",
    "\n",
    "#     assert bos_token != eos_token, \"Please choose a different BOS token.\"\n",
    "#     assert unk_token != eos_token, \"Please choose a different UNK token.\"\n",
    "\n",
    "#     # Creates dict for BOS, PAD, and UNK tokens\n",
    "#     # Keeps the EOS token as it was originally defined\n",
    "#     special_tokens_dict = {'bos_token': bos_token,\n",
    "#                            'pad_token': pad_token,\n",
    "#                            'unk_token': unk_token}\n",
    "\n",
    "#     # If there are additional special tokens, add them\n",
    "#     if special_tokens is not None:\n",
    "#         if isinstance(special_tokens, list):\n",
    "#             special_tokens_dict.update({'additional_special_tokens': special_tokens})\n",
    "\n",
    "#     tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "#     # If there are new regular (not special) tokens to add\n",
    "#     if tokens is not None:\n",
    "#         if isinstance(tokens, list):\n",
    "#             tokenizer.add_tokens(tokens)\n",
    "\n",
    "#     return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jedWrSsc5CkD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def jinja_template(tokenizer):\n",
    "#     return (\"{% for message in messages %}\"\n",
    "#             f\"{{{{'{tokenizer.bos_token}' + message['role'] + '\\n' + message['content'] + '{tokenizer.eos_token}' + '\\n'}}}}\"\n",
    "#             \"{% endfor %}\"\n",
    "#             \"{% if add_generation_prompt %}\"\n",
    "#             f\"{{{{ '{tokenizer.bos_token}assistant\\n' }}}}\"\n",
    "#             \"{% endif %}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "diSfxVKl4P2c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def add_template(tokenizer, chat_template=None):\n",
    "#     # If not chat template was given, creates a ChatML template\n",
    "#     # using the BOS and EOS tokens\n",
    "#     if chat_template is None:\n",
    "#         chat_template = jinja_template(tokenizer)\n",
    "\n",
    "#     # Assigns chat template to tokenizer\n",
    "#     tokenizer.chat_template = chat_template\n",
    "\n",
    "#     return tokenizer\n",
    "\n",
    "# def get_multiple_of(vocab_size):\n",
    "#     return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "# def modify_model(model, tokenizer):\n",
    "#     # If new tokenizer length exceeds vocabulary size\n",
    "#     # resizes it while keeping it a multiple of the same value\n",
    "#     if len(tokenizer) > model.config.vocab_size:\n",
    "#         pad_to_multiple_of = get_multiple_of(model.vocab_size)\n",
    "#         model.resize_token_embeddings(len(tokenizer),\n",
    "#                                       pad_to_multiple_of=pad_to_multiple_of)\n",
    "\n",
    "#     # Updates token ids on model configurations\n",
    "#     if getattr(model, \"config\", None) is not None:\n",
    "#         model.config.pad_token_id = tokenizer.pad_token_id\n",
    "#         model.config.bos_token_id = tokenizer.bos_token_id\n",
    "#         model.config.eos_token_id = tokenizer.eos_token_id\n",
    "#     if getattr(model, \"generation_config\", None) is not None:\n",
    "#         model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "#         model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "#         model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "#     return model\n",
    "\n",
    "# def generate(model, tokenizer, sentence, max_new_tokens=64, skip_special_tokens=False):\n",
    "#     converted_sample = [\n",
    "#         {\"role\": \"user\", \"content\": sentence},\n",
    "#     ]\n",
    "#     prompt = tokenizer.apply_chat_template(converted_sample,\n",
    "#                                            tokenize=False,\n",
    "#                                            add_generation_prompt=True)\n",
    "\n",
    "#     tokenized_input = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "#     input_ids = tokenized_input[\"input_ids\"].to(model.device)\n",
    "\n",
    "#     model.eval()\n",
    "#     generation_output = model.generate(input_ids=input_ids,\n",
    "#                                        eos_token_id=tokenizer.eos_token_id,\n",
    "#                                        max_new_tokens=max_new_tokens)\n",
    "\n",
    "#     output = tokenizer.batch_decode(generation_output,\n",
    "#                                     skip_special_tokens=skip_special_tokens)[0]\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LM6fnoEDpBsQ"
   },
   "outputs": [],
   "source": [
    "# peft_model = modify_model(peft_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "BlQKeuHqDj6K",
    "outputId": "aa615cf6-d41b-43e2-9ed4-f172f1ce781a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataCollatorForCompletionOnlyLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4226850110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresponse_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<|im_start|>assistant\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcollator_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataCollatorForCompletionOnlyLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DataCollatorForCompletionOnlyLM' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side='left'\n",
    "\n",
    "response_template = '<|im_start|>assistant\\n'\n",
    "collator_fn=DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAi2cG33B1HW"
   },
   "source": [
    "#some experiments to see how the training goes, it looks like it oscillates and no sign of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958
    },
    "id": "ZSNc_jJtqaAA",
    "outputId": "2133b6e6-43c2-4084-9236-078c154b66d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='359' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [359/450 08:58 < 02:17, 0.66 it/s, Epoch 7.96/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>3.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>3.163700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2337440012.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2238\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/memory.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No executable batch size found, reached zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_reduce_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2576\u001b[0m                     )\n\u001b[1;32m   2577\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2578\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3838\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3840\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2577\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2578\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_effective_batch_size = 8\n",
    "lr = 3e-4\n",
    "# max_seq_length = 64\n",
    "# collator_fn = None\n",
    "# packing = (collator_fn is None)\n",
    "steps = 20\n",
    "num_train_epochs = 10\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir = '/content/drive/MyDrive/hf-smol3b/hf-SmolLM3-3B-ada',\n",
    "    # packing = packing,\n",
    "    # max_seq_length=64, # Removed from SFTConfig\n",
    "    gradient_checkpointing = True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    gradient_accumulation_steps = 2,\n",
    "    per_device_train_batch_size = min_effective_batch_size,\n",
    "    auto_find_batch_size = True,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    learning_rate = lr,\n",
    "    report_to = 'wandb',\n",
    "    logging_dir = '/content/drive/MyDrive/hf-smol3b/hf-SmolLM3-3B/logs',\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps = steps,\n",
    "    save_strategy = 'steps',\n",
    "    save_steps = steps\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    train_dataset = dataset,\n",
    "    processing_class = tokenizer,\n",
    "    # data_collator = collator_fn,\n",
    "    args = sft_config,\n",
    "    # max_seq_length = max_seq_length # Added here\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLSdm9pJqaKs"
   },
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "  pass\n",
    "\n",
    "sentence = \"What a day\"\n",
    "prompt = gen_prompt(tokenizer, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I2hN35lqaNL"
   },
   "outputs": [],
   "source": [
    "def generate(peft_model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=True):\n",
    "  tokenized_input = tokenizer(prompt,add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "  model.eval()\n",
    "  generation_output = model.generate(**tokenized_input, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "  output = tokenizer.batch_decode(generation_output, skip_special_tokens=skip_special_tokens)[0]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAL3bPwFtWZ3"
   },
   "outputs": [],
   "source": [
    "print(generate(peft_model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPBCdRk0thhC"
   },
   "source": [
    "#Saving the Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSbwCjqttm-G"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('your_model_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQLoIFr2uUQK"
   },
   "source": [
    "#Merged the model with the unquantised base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "SK_j7ypnubHd",
    "outputId": "1a27979f-f306-42b6-c8bb-64b8352d0916"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoPeftModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1980696386.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoPeftModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_model_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreloaded_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoPeftModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "reloaded_model = AutoPeftModelForCausalLM.from_pretrained('your_model_name')\n",
    "reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27QxyB_lubOn"
   },
   "outputs": [],
   "source": [
    "merged_model = reloaded_model.merge_and_unload()\n",
    "merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oO3DwCNYuzlN"
   },
   "outputs": [],
   "source": [
    "merged_model.save_pretrained('your_model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0ieMfR7u656"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LenXaAQOvEj_"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "549ef6dd",
    "outputId": "5ae9a396-bb79-472d-d429-84a39db734c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AlignPropConfig', 'AlignPropTrainer', 'AllTrueJudge', 'AutoModelForCausalLMWithValueHead', 'AutoModelForSeq2SeqLMWithValueHead', 'BCOConfig', 'BCOTrainer', 'BaseBinaryJudge', 'BaseJudge', 'BasePairwiseJudge', 'BaseRankJudge', 'BestOfNSampler', 'CPOConfig', 'CPOTrainer', 'DDPOConfig', 'DDPOPipelineOutput', 'DDPOSchedulerOutput', 'DDPOStableDiffusionPipeline', 'DDPOTrainer', 'DPOConfig', 'DPOTrainer', 'DefaultDDPOStableDiffusionPipeline', 'FDivergenceConstants', 'FDivergenceType', 'GKDConfig', 'GKDTrainer', 'GRPOConfig', 'GRPOTrainer', 'HfPairwiseJudge', 'IterativeSFTConfig', 'IterativeSFTTrainer', 'KTOConfig', 'KTOTrainer', 'LogCompletionsCallback', 'MergeModelCallback', 'ModelConfig', 'NashMDConfig', 'NashMDTrainer', 'ORPOConfig', 'ORPOTrainer', 'OnlineDPOConfig', 'OnlineDPOTrainer', 'OpenAIPairwiseJudge', 'PPOConfig', 'PPOTrainer', 'PRMConfig', 'PRMTrainer', 'PairRMJudge', 'PreTrainedModelWrapper', 'RLOOConfig', 'RLOOTrainer', 'RewardConfig', 'RewardTrainer', 'RichProgressCallback', 'SFTConfig', 'SFTTrainer', 'SUPPORTED_ARCHITECTURES', 'ScriptArguments', 'SyncRefModelCallback', 'TextEnvironment', 'TextHistory', 'TrlParser', 'WinRateCallback', 'XPOConfig', 'XPOTrainer', '__all__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_class_to_module', '_import_structure', '_modules', '_name', '_objects', 'apply_chat_template', 'clone_chat_template', 'create_reference_model', 'data_utils', 'environment', 'extract_prompt', 'extras', 'get_kbit_device_map', 'get_peft_config', 'get_quantization_config', 'init_zero_verbose', 'is_conversational', 'maybe_apply_chat_template', 'maybe_convert_to_chatml', 'maybe_extract_prompt', 'maybe_unpair_preference_dataset', 'models', 'pack_dataset', 'scripts', 'setup_chat_format', 'trainer', 'trainer.callbacks', 'trainer.utils', 'truncate_dataset', 'unpair_preference_dataset']\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "print(dir(trl))\n",
    "# # # You can also try to explore submodules\n",
    "# # # print(dir(trl.trainer))\n",
    "# print(dir(trl.trainer.utils))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HjGsnygD6yX"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
