{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LBD1d073Z35o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278b272e-7b51-4e1d-a34e-cedc893bb2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: trl==0.12.1 in /usr/local/lib/python3.12/dist-packages (0.12.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl==0.12.1) (13.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets bitsandbytes trl==0.12.1 transformers peft huggingface-hub accelerate safetensors pandas matplotlib numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    #AutoPeftModelForCausalLM, # Removed from transformers\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "# from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftConfig # Added to peft\n",
        "from huggingface_hub import notebook_login\n",
        "from trl import SFTTrainer, SFTConfig, setup_chat_format, DataCollatorForCompletionOnlyLM"
      ],
      "metadata": {
        "id": "C0P5HtZ6Z5RR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "calculate_dtype = torch.bfloat16 if support else torch.float32"
      ],
      "metadata": {
        "id": "apjQgqH_m8o8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDTS5RrVm-42",
        "outputId": "a65a0042-936a-4392-bfd7-e1699465e85e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "calculate_dtype = torch.bfloat16 if support else torch.float32\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= calculate_dtype, #calculate_dtype can be bf16 or float32- use bf16 if supported\n",
        "    bnb_4bit_use_double_quant= True\n",
        "    )\n",
        "repo = \"microsoft/phi-1\"\n",
        "model = AutoModelForCausalLM.from_pretrained(repo, quantization_config= bnb_config, device_map= \"cuda:0\")"
      ],
      "metadata": {
        "id": "nkpxpotRZ5Ty"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_memory_footprint()/1024/1024)"
      ],
      "metadata": {
        "id": "D127_Z6FZ5WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f60292-f077-4572-b4ff-cf712745f726"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "977.1367797851562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "vm7RO981Z5Yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbd8d3d-f5d8-4be2-88d3-93edda95bb74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): Embedding(51200, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x PhiDecoderLayer(\n",
              "        (self_attn): PhiAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (rotary_emb): PhiRotaryEmbedding()\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r = 4, #. rank of LoRA - [4-16]\n",
        "    bias = \"none\", # [\"all\", \"lora_only\"] - for train bias term\n",
        "    lora_alpha = 8, # scalling factor\n",
        "    lora_dropout = 0.10, # prevent overfit- used for regularisation\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    task_type = \"CAUSAL_LM\"\n",
        "\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model"
      ],
      "metadata": {
        "id": "uzMopk81Z5ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fffef08-ed9c-4d4c-ed9e-62a39d586293"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): PhiForCausalLM(\n",
              "      (model): PhiModel(\n",
              "        (embed_tokens): Embedding(51200, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x PhiDecoderLayer(\n",
              "            (self_attn): PhiAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "            )\n",
              "            (mlp): PhiMLP(\n",
              "              (activation_fn): NewGELUActivation()\n",
              "              (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "              (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "            )\n",
              "            (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (rotary_emb): PhiRotaryEmbedding()\n",
              "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_memory_footprint()/1024/1024)"
      ],
      "metadata": {
        "id": "mjtETM3QZ5fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec11f790-8282-40da-c3a6-210f4bc72800"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1382.7734985351562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_base_model)"
      ],
      "metadata": {
        "id": "sGNI99ZVZ5hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55681ba4-f8aa-4e48-e3e1-d2fb82abef4c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): PhiForCausalLM(\n",
            "      (model): PhiModel(\n",
            "        (embed_tokens): Embedding(51200, 2048)\n",
            "        (layers): ModuleList(\n",
            "          (0-23): 24 x PhiDecoderLayer(\n",
            "            (self_attn): PhiAttention(\n",
            "              (q_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
            "            )\n",
            "            (mlp): PhiMLP(\n",
            "              (activation_fn): NewGELUActivation()\n",
            "              (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "              (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            )\n",
            "            (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (rotary_emb): PhiRotaryEmbedding()\n",
            "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
            "    )\n",
            "  )\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_memory_footprint()/1e6)"
      ],
      "metadata": {
        "id": "JiBoFpSgZ5j2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb93e778-d956-4e42-9046-e7b1912c999f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1449.943104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
        "percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Percentage Trainable: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "Ff-gwq_dZ5mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f405bac8-da82-4cf0-c091-5f3528c3f50c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters: 1,179,648\n",
            "Total Parameters: 1,419,450,368\n",
            "Percentage Trainable: 0.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
        "\n",
        "# Map into input-output format\n",
        "def format_for_phi(example):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies tweets into sentiment: negative, neutral, or positive.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the sentiment of this tweet:\\n\\n{example['text']}\"},\n",
        "            {\"role\": \"assistant\", \"content\": example[\"label_text\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "train_dataset = dataset[\"train\"].select(range(500)).map(format_for_phi) # Select 2000 samples\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Print some samples to inspect the data\n",
        "print(\"=== Dataset Info ===\")\n",
        "print(f\"Original dataset: {dataset}\")\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Dataset features: {train_dataset.features}\")\n",
        "\n",
        "print(\"\\n=== Sample Data Inspection ===\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Original text: {dataset['train'][i]['text']}\")\n",
        "    print(f\"Original label: {dataset['train'][i]['label_text']}\")\n",
        "    print(f\"Formatted messages:\")\n",
        "    for msg in train_dataset[i]['messages']:\n",
        "        print(f\"  {msg['role']}: {msg['content']}\")\n",
        "\n",
        "# Custom function to format messages (since Phi-1 doesn't have chat template)\n",
        "def format_conversation(messages):\n",
        "    \"\"\"Convert messages to a formatted string for Phi-1\"\"\"\n",
        "    conversation = \"\"\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            conversation += f\"System: {message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            conversation += f\"User: {message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            conversation += f\"Assistant: {message['content']}\"\n",
        "    return conversation\n",
        "\n",
        "# Test custom formatting\n",
        "print(\"\\n=== Custom Formatting Test ===\")\n",
        "sample_messages = train_dataset[0]['messages']\n",
        "formatted_text = format_conversation(sample_messages)\n",
        "print(f\"Formatted conversation:\\n{formatted_text}\")\n",
        "\n",
        "# Tokenize function for the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Format conversations using our custom function\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        conversation = format_conversation(messages)\n",
        "        texts.append(conversation)\n",
        "\n",
        "    # Tokenize the formatted conversations\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # For causal language modeling, labels are the same as input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "print(\"\\n=== Tokenizing Dataset ===\")\n",
        "tokenized_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
        "print(f\"Dataset features: {tokenized_dataset.features}\")\n",
        "\n",
        "# Print tokenized sample details\n",
        "print(\"\\n=== Tokenized Sample Analysis ===\")\n",
        "sample_idx = 0\n",
        "input_ids = tokenized_dataset[sample_idx]['input_ids']\n",
        "attention_mask = tokenized_dataset[sample_idx]['attention_mask']\n",
        "labels = tokenized_dataset[sample_idx]['labels']\n",
        "\n",
        "print(f\"Input IDs length: {len(input_ids)}\")\n",
        "print(f\"Attention mask length: {len(attention_mask)}\")\n",
        "print(f\"Labels length: {len(labels)}\")\n",
        "print(f\"Sequence length: {len(input_ids)}\")\n",
        "print(f\"Number of padding tokens: {input_ids.count(tokenizer.pad_token_id)}\")\n",
        "\n",
        "print(f\"\\nFirst 10 tokens: {input_ids[:10]}\")\n",
        "print(f\"Decoded first 50 tokens: {tokenizer.decode(input_ids[:50])}\")\n",
        "\n",
        "# Check dataset statistics\n",
        "print(f\"\\n=== Dataset Statistics ===\")\n",
        "sequence_lengths = [len(item['input_ids']) for item in tokenized_dataset]\n",
        "print(f\"Average sequence length: {sum(sequence_lengths) / len(sequence_lengths):.2f}\")\n",
        "print(f\"Max sequence length: {max(sequence_lengths)}\")\n",
        "print(f\"Min sequence length: {min(sequence_lengths)}\")\n",
        "\n",
        "print(f\"\\n=== Ready for Training Setup ===\")\n",
        "print(\"Dataset is tokenized and ready for Trainer!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ltawWMoUFtR",
        "outputId": "ecc6a354-fcac-4384-ce7d-ab35d3331705"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dataset Info ===\n",
            "Original dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'text', 'label', 'label_text'],\n",
            "        num_rows: 26732\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'text', 'label', 'label_text'],\n",
            "        num_rows: 3432\n",
            "    })\n",
            "})\n",
            "Train dataset size: 500\n",
            "Dataset features: {'id': Value('string'), 'text': Value('string'), 'label': Value('int64'), 'label_text': Value('string'), 'messages': List({'content': Value('string'), 'role': Value('string')})}\n",
            "\n",
            "=== Sample Data Inspection ===\n",
            "\n",
            "Sample 1:\n",
            "Original text:  I`d have responded, if I were going\n",
            "Original label: neutral\n",
            "Formatted messages:\n",
            "  system: You are a helpful assistant that classifies tweets into sentiment: negative, neutral, or positive.\n",
            "  user: Classify the sentiment of this tweet:\n",
            "\n",
            " I`d have responded, if I were going\n",
            "  assistant: neutral\n",
            "\n",
            "Sample 2:\n",
            "Original text:  Sooo SAD I will miss you here in San Diego!!!\n",
            "Original label: negative\n",
            "Formatted messages:\n",
            "  system: You are a helpful assistant that classifies tweets into sentiment: negative, neutral, or positive.\n",
            "  user: Classify the sentiment of this tweet:\n",
            "\n",
            " Sooo SAD I will miss you here in San Diego!!!\n",
            "  assistant: negative\n",
            "\n",
            "Sample 3:\n",
            "Original text: my boss is bullying me...\n",
            "Original label: negative\n",
            "Formatted messages:\n",
            "  system: You are a helpful assistant that classifies tweets into sentiment: negative, neutral, or positive.\n",
            "  user: Classify the sentiment of this tweet:\n",
            "\n",
            "my boss is bullying me...\n",
            "  assistant: negative\n",
            "\n",
            "=== Custom Formatting Test ===\n",
            "Formatted conversation:\n",
            "System: You are a helpful assistant that classifies tweets into sentiment: negative, neutral, or positive.\n",
            "User: Classify the sentiment of this tweet:\n",
            "\n",
            " I`d have responded, if I were going\n",
            "Assistant: neutral\n",
            "\n",
            "=== Tokenizing Dataset ===\n",
            "Tokenized dataset: Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 500\n",
            "})\n",
            "Dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
            "\n",
            "=== Tokenized Sample Analysis ===\n",
            "Input IDs length: 89\n",
            "Attention mask length: 89\n",
            "Labels length: 89\n",
            "Sequence length: 89\n",
            "Number of padding tokens: 42\n",
            "\n",
            "First 10 tokens: [11964, 25, 921, 389, 257, 7613, 8796, 326, 1398, 6945]\n",
            "Decoded first 50 tokens: System: You are a helpful assistant that classifies tweets into sentiment: negative, neutral, or positive.\n",
            "User: Classify the sentiment of this tweet:\n",
            "\n",
            " I`d have responded, if I were going\n",
            "Assistant: neutral<|endoftext|><|endoftext|><|endoftext|>\n",
            "\n",
            "=== Dataset Statistics ===\n",
            "Average sequence length: 89.00\n",
            "Max sequence length: 89\n",
            "Min sequence length: 89\n",
            "\n",
            "=== Ready for Training Setup ===\n",
            "Dataset is tokenized and ready for Trainer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Ultra aggressive memory management\n",
        "# Set memory management environment variables - already set in a previous cell, but keeping for clarity\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Force garbage collection and clear cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Optimized parameters for better training stability and performance - Further reduced for memory\n",
        "min_effective_batch_size = 1  # Absolute minimum batch size per device\n",
        "lr = 1e-4  # Further reduced learning rate\n",
        "max_seq_length = 32  # Significantly reduced max sequence length\n",
        "collator_fn = None\n",
        "packing = False # Disable packing for memory saving\n",
        "steps = 5  # Save/log more frequently to see progress, but might increase memory slightly during saves\n",
        "num_train_epochs = 1  # Only 1 epoch for initial test\n",
        "warmup_ratio = 0.0 # No warmup\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir = '/content/drive/MyDrive/phi1-sentiment-lora',\n",
        "    packing = packing,\n",
        "    max_seq_length = max_seq_length,\n",
        "    gradient_checkpointing = True,\n",
        "    gradient_checkpointing_kwargs = {'use_reentrant': False},\n",
        "    gradient_accumulation_steps = 8, # Increased accumulation steps\n",
        "    per_device_train_batch_size = min_effective_batch_size,\n",
        "    auto_find_batch_size = False, # Disable auto batch size finding\n",
        "    bf16 = True,\n",
        "    fp16 = False,\n",
        "    num_train_epochs = num_train_epochs,\n",
        "    learning_rate = lr,\n",
        "    lr_scheduler_type = \"constant\", # Simpler scheduler\n",
        "    warmup_ratio = warmup_ratio,\n",
        "    weight_decay = 0.01,\n",
        "    max_grad_norm = 1.0,\n",
        "    report_to = 'none', # Disable reporting to save memory\n",
        "    run_name = \"Phi1-Sentiment-LoRA-Finetune-Minimal\",\n",
        "    logging_dir = '/content/drive/MyDrive/phi1-sentiment-lora/logs_minimal',\n",
        "    logging_strategy = 'steps',\n",
        "    save_strategy = 'steps',\n",
        "    logging_steps = steps,\n",
        "    save_steps = steps,\n",
        "    save_total_limit = 1, # Keep only the last checkpoint\n",
        "    dataloader_pin_memory=False, # Disable pin memory\n",
        "    remove_unused_columns=True, # Remove unused columns from dataset\n",
        "    optim=\"adamw_torch\", # Explicitly set optimizer\n",
        "    group_by_length=False, # Disable group by length\n",
        "    dataloader_num_workers=0, # Set workers to 0\n",
        ")\n",
        "\n",
        "# Clear memory before trainer creation\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Creating trainer with minimal settings...\")\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = tokenized_dataset, # Using the tokenized dataset from cell before\n",
        "    processing_class = tokenizer,\n",
        "    data_collator = collator_fn, # Use default collator\n",
        "    args = sft_config,\n",
        ")\n",
        "\n",
        "print(\"Starting minimal training...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Training failed with error: {e}\")\n",
        "    print(\"Model might still be too large for available GPU memory\")\n",
        "    print(\"Consider using a smaller model or upgrading GPU\")"
      ],
      "metadata": {
        "id": "bV9xcomRZ50z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b847bf4c-2439-4582-decb-51df13501189"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating trainer with minimal settings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50256, 'bos_token_id': 50256, 'pad_token_id': 50256}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting minimal training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training failed with error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 59.38 MiB is free. Process 145802 has 22.10 GiB memory in use. Of the allocated memory 21.48 GiB is allocated by PyTorch, and 391.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Model might still be too large for available GPU memory\n",
            "Consider using a smaller model or upgrading GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 1: Save and merge the model locally first\n",
        "# trainer.save_model('/content/phi1-sentiment-lora-saved')\n",
        "\n",
        "# # Step 2: Load and merge the adapter\n",
        "# from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "# # Load the saved PEFT model (use the same path as Step 1)\n",
        "# peft_model = AutoPeftModelForCausalLM.from_pretrained('/content/phi1-sentiment-lora-saved')\n",
        "\n",
        "# # Merge and unload adapter\n",
        "# merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "# # Step 3: Save the merged model\n",
        "# merged_model.save_pretrained('/content/phi1-sentiment-merged')\n",
        "# tokenizer.save_pretrained('/content/phi1-sentiment-merged')\n",
        "\n",
        "# # Step 4: Initialize Hugging Face API and upload the merged model\n",
        "# from huggingface_hub import HfApi\n",
        "\n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path='/content/phi1-sentiment-merged',\n",
        "#     repo_id=\"your-username/phi1-sentiment-classifier\",  # Update with your HF username\n",
        "#     repo_type=\"model\",\n",
        "#     commit_message=\"Upload Phi-1 fine-tuned on tweet sentiment classification dataset with LoRA\"\n",
        "# )"
      ],
      "metadata": {
        "id": "W7SyBkyK-S-a"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}