{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "LBD1d073Z35o",
    "outputId": "b0d9ab14-505c-4668-be80-7a9d1256fc1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
      "Requirement already satisfied: trl==0.12.1 in /usr/local/lib/python3.12/dist-packages (0.12.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl==0.12.1) (13.9.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.1.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (2.19.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets bitsandbytes trl==0.12.1 transformers peft huggingface-hub accelerate safetensors pandas matplotlib numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ae5da50"
   },
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "C0P5HtZ6Z5RR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    #AutoPeftModelForCausalLM, # Removed from transformers\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftConfig # Added to peft\n",
    "from huggingface_hub import notebook_login\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9F6uR-QnAPE"
   },
   "source": [
    "\n",
    "# Check for bf16 support and set compute dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "apjQgqH_m8o8"
   },
   "outputs": [],
   "source": [
    "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "calculate_dtype = torch.bfloat16 if support else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDTS5RrVm-42",
    "outputId": "fe860121-5451-42e9-cb69-c5803505e44e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(calculate_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBHrxjDJnbbf"
   },
   "source": [
    "#bnb config for loading 4 bit model with nf4 quant type\n",
    "* loading model with quantization config\n",
    "* device map to cuda\n",
    "* 4bit true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "fae4aa16f3dd460f990a47214dfd5572",
      "a094def5cc4844eea2cb3cfc7e932463",
      "0799128c28b64b1c803182072ac83fa6",
      "77ed24a3589f4ae5bd563c90af4da5ea",
      "6f47a6b27b34494381acd9a87d2984f5",
      "e49b3fde9e61499db17254ef05f7a514",
      "8879185858144e30a0bb877a61f93aa8",
      "b5bb9ae81ef2429d942de2c57b883fed",
      "9cb7fcc5a4424c31a0dc5e00fa1bc4b6",
      "af7a88941ac44113b48e2fcb37d4b0a7",
      "9abc68fb420045a2abd023591a723a90",
      "c51dc6595101470db2126a6cfa82d302",
      "07710937b6b74d84ad80ab582e172c22",
      "6130584f779f4525b57ce4cfb66cbe72",
      "92d30ee46f5640258114fd8754245794",
      "c27921efda964de7b4d43e3549b3a791",
      "7ab613e4312f4c908924d3a42503c0b3",
      "db000b8b30774fa080cfbdc0c24e248f",
      "403fac019b4f4b739b4e3c7d1c645ef1",
      "043914aae99f4ebda15e33618b295ce7",
      "04152a92c84947398045bc278b927324",
      "193365eece4e46ad84d6abfc64fa73f8",
      "c5c876a39d5148f6b1745cf458279f38",
      "6add82902fc4489fae440d9d2b339793",
      "4d0a1fb6cced4292a2f98c8d513bccef",
      "e166681e4f9742449e8a03fe8d7e4858",
      "254745b967dc4754a244926f441e530c",
      "b1821d8386af42b9b5405f221207b060",
      "852ab013028c49c183674d731725691b",
      "65c58691089d4a9680e375883201a20e",
      "7c70d5906ff441d29b0945c08df80f4a",
      "4b1d69e8ce214ed1b24c26f3aa01489c",
      "65b1c22260e44950adc9720d19f51de5"
     ]
    },
    "id": "nkpxpotRZ5Ty",
    "outputId": "13501fb0-9b31-416e-f819-655a5c94cdf8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae4aa16f3dd460f990a47214dfd5572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51dc6595101470db2126a6cfa82d302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c876a39d5148f6b1745cf458279f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= calculate_dtype, #calculate_dtype can be bf16 or float32- use bf16 if supported\n",
    "    bnb_4bit_use_double_quant= True\n",
    "    )\n",
    "repo = \"microsoft/DialoGPT-small\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, quantization_config= bnb_config, device_map= \"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzT757Suokja"
   },
   "source": [
    "#Check model memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D127_Z6FZ5WN",
    "outputId": "6789ecb2-aafc-47ef-f7e2-c8def79bc49c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.8501205444336\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQyQHN6zos1k"
   },
   "source": [
    "#model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vm7RO981Z5Yh",
    "outputId": "7b210fd6-ddcd-4b07-ccfb-37171d07df09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSh3kYaioxjD"
   },
   "source": [
    "#Prepare model for kbit training\n",
    "##Use Lora Config\n",
    "\n",
    "\n",
    "1.   rank [4,8,16,32] - choose one\n",
    "2.   lora_alpha is a scalling factor which should be 2x the rank of matrix.\n",
    "3.   dropout range from 0.03 to 0.10 which helps prevent overfit\n",
    "4.   module - choose module as per requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzMopk81Z5ao",
    "outputId": "eef5e886-85ae-437e-d685-4f1568fcc3d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT model successfully configured with LoRA!\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for quantized training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# DialoGPT/GPT-2 uses different layer names than newer models\n",
    "config = LoraConfig(\n",
    "    r = 8,  # rank of LoRA - [4-16]\n",
    "    bias = \"none\",  # [\"all\", \"lora_only\"] - for train bias term\n",
    "    lora_alpha = 16,  # scaling factor\n",
    "    lora_dropout = 0.10,  # prevent overfit - used for regularisation\n",
    "\n",
    "    # CORRECTED target modules for DialoGPT/GPT-2 architecture\n",
    "    target_modules = [\n",
    "        \"c_attn\",    # Combined Q, K, V projection (replaces q_proj, k_proj, v_proj)\n",
    "        \"c_proj\",    # Output projection (replaces o_proj)\n",
    "        \"c_fc\",      # Feed-forward layer 1 (replaces gate_proj/up_proj)\n",
    "        \"c_proj\"     # Feed-forward layer 2 (replaces down_proj) - Note: same name used twice in GPT-2\n",
    "    ],\n",
    "\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, config)\n",
    "print(\"DialoGPT model successfully configured with LoRA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzA5DOMp5aA"
   },
   "source": [
    "#once again check memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjtETM3QZ5fK",
    "outputId": "72e41611-2f9d-4bbf-dab5-047edcd027fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207.7002182006836\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z2XsrHMp9kr"
   },
   "source": [
    "#Print base model to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGNI99ZVZ5hr",
    "outputId": "c077bef1-7a1f-40da-867e-11f3037d2418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.get_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiBoFpSgZ5j2",
    "outputId": "91f10aad-80c2-4262-dc19-63548ec0ea12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217.789464\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSmHEA7cqHKh"
   },
   "source": [
    "#Check for trainable Parameters and its percentage for a mathematical view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ff-gwq_dZ5mO",
    "outputId": "40b38b83-8fb7-42e7-a876-7b8f8a8401c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 1,179,648\n",
      "Total Parameters: 125,619,456\n",
      "Percentage Trainable: 0.94%\n"
     ]
    }
   ],
   "source": [
    "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Percentage Trainable: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oOA2lywqYKD"
   },
   "source": [
    "#ETL Process for Dataset Prep stage, Tokenizer load and define chat template if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571,
     "referenced_widgets": [
      "3f4535aa46ba485cb5befa35751f131b",
      "9e85dd15be58440391111bb74aa487d1",
      "d514acee2a474794ba70ab4505e444f5",
      "d45425cf34494b2fbba8cf329d937e25",
      "4d0e8326a5a14bd395dec46df0cc4c8e",
      "008d77b0050b49d8a48a6067e5ec7068",
      "2280442dd7594d32be7ca2da44e8e911",
      "7e1ee82e37fa432fbee4e122df768ba8",
      "e420b56334c14bb798eb285f077d3455",
      "fa0eadcd3b074f4592329503dfcb6f03",
      "370e3637d0e74d4796a0ca42998417c6",
      "d14c50ba2c38427d83ade27547677db8",
      "58faef8c7d6c498da802ec986dab5d7c",
      "f90695460c904dfeb633f83d13230f8f",
      "0c5338789366405da8588fe2df6a0ed0",
      "9998da33dbd44a1ca218cf132fbf3608",
      "89790267f9594d449f030378ceb3c7dc",
      "42b6e0f25f234a14a841083a549e9921",
      "dbefce69b4e44847a5a2c7b70d200cdd",
      "d62d9c3e0d934fc8a17743e113c499a8",
      "b42f5a70f83041bb9f1a598d46b16d96",
      "e1a39324bb374ff7908fd3746559f5be",
      "45c2110b539b4792b549c15ad6e97d05",
      "30365596c9204f20b95d3a4f0e7fedbe",
      "71dad4c2c753432fbbc0bfe91826a524",
      "6f72ad23beab4fc884daad116f0205a4",
      "870d646cedbc4d1ab82e01e3a81a1caa",
      "fe1814e473c549249181ac0911973fd4",
      "185925f4e2a74cf5815c97a713db861b",
      "d3030e2614424598a6c795afa08ec972",
      "9dc6b2937db24a95938ac0eefcbba807",
      "81212cd961f641b5b11304189893d439",
      "49aeceb314d04a0db193d53fd408ccf3",
      "730c22632c5c4b01a54cf1fe7c3702c9",
      "84b79c25cbaf49b19f933cbbad55d8c5",
      "87caeb57a3dd401cb6564ae405fcdd43",
      "d630b45e13da4ecf95bb1e507d76f085",
      "0efd20a7d86c4515846e44b3d10a0c86",
      "f6dcbfd9391e4d2ab090c5f2c7821c7f",
      "336486887c094a519478a352ffa0f12b",
      "490734dfb83f4fb08a428322fb87a521",
      "8848b2f687bd48a2976d773ad0004ed0",
      "0a39582e6fdb4579bca1f857bfb8127c",
      "38da3ae0036c445aaaf0e65a495e4876",
      "4f4dc3e50dc84282a426e49575162d75",
      "3b9288e6a8104abdbdb277349c4a201d",
      "cf46327d18384956a0a7701d89bf4ad1",
      "3315c6fd8cc74e99a73744b776b1cbe2",
      "f008292e6ada46a5ab512e55f963d183",
      "6c65ee333ff4481eac50273b11b6e4bd",
      "58ebd91a20d9467f8d6a436937fe688f",
      "e71e138e105d419dab7d5efbfd4c6a80",
      "23335e6860164e6e8655a9460c5dcb2d",
      "3b28ba79d014481d9ca403800063f8f3",
      "8f8b4cccdd944b6e88da0f12e7ff17d3",
      "ca1ee91afaef440987cb9903080dca6c",
      "090fe9807670428ca91278d4c6b8d9da",
      "f862dca6334c4043ab7a16c54ea4b2d7",
      "c41d764dc20c45faa76eaf94193c4850",
      "0f1f01ce88ab49cc9d7a1bf9e1f6af57",
      "c4804ed4eb7a492b8db150429348e975",
      "478de19e9c734d4e8c3d044bd0024196",
      "a27984eca84246b3bfa057d1cdb96011",
      "afeaa93b09814a57a5ade1f4e683773e",
      "821a0fa8371c44b5b15cb47580e7feaf",
      "31a19a182cfe48b8a972a100945053a9",
      "dcef1bb40a924a3f8ef8fef110fa67d0",
      "7a1181ab7f454091af666463a124f7c5",
      "ca293284356f4320ada770f6da5112d4",
      "1bebe5685e214b5893671cb8bf86aab4",
      "88d67c1c1e8544309f8ca960f70063b2",
      "7ef957b7ea1146b69edaaaac26e33d62",
      "e5367e3b7c2d4da4b90842dfc6d03356",
      "a0c908758b504857983b49c628451725",
      "2a140659c24e415f8b91754da2af6f4a",
      "6003f17a955a47b292855d0ba5b50a2f",
      "1564e1cf47a14617b368608668926f8c",
      "d9ce152a64264f2ab182fc18a39256b9",
      "70db25cc4d2d481eac02a298c40fff86",
      "070e7413fd0b4f749d53d66c0b879cfd",
      "1dbd985a5b614e5bb0aec20f9d219b2d",
      "908b91e9da1545bbbe1ee923c290766f",
      "a168ef918312446d835a5a1ebd6e4e21",
      "ad7b2e6ffc824651ba685df2f3467f69",
      "55e12b0289c742728220ff322dcc9c01",
      "78ab7c67cb1c4982a7edeb6757abf3cc",
      "cb952fd4ef3e474b8eb53cc4bf3c3589",
      "deee979d24714116a0f1b9817498e9cf",
      "2f0d81c5c5664939ab6d0ea819a87329",
      "b1e5cfc94d824a93a47a08ee4a5bf47a",
      "343b50ceede64c5ba7fed95f8fb0cd24",
      "5f703f8c62a845babd973c159eebc269",
      "7d25b2ca100844dc838b9329a85c087a",
      "9bd15ad4d0ac4847bada21463947f5a0",
      "9d7b516577394cc48e5f6239c34896cc",
      "3e0ffcafffce41cb9ec3c522f69cb1c8",
      "5042d022e94d441587b2ea9db56ef57b",
      "620ff5cf746a405f9606a934a822c5f4",
      "a5c6694041f64f7aae107fc82bc9755a",
      "abee80cc4fb5488b9c997930c74dae9b",
      "c2b77684e0d14199ba2df59064d4b1c1",
      "5be8e2b1590642b48e0f6e31598a74ef",
      "9bc23ba2cad0437daf8e762656bc3118",
      "283d1a2fe7f64e0fa40ac24f441d4b4a",
      "0dcedec81db243c5887379d51d6758be",
      "45a0308dcbbf4768812163c89f0d8fc1",
      "b694586204f44a269ac31b4664b40fc8",
      "247f5fd6d6894654ab34901d4d2de2ae",
      "883ee50e2d23431a83568df399d9c3bc",
      "c873efe1f30f4b37ab43c82e5bfa651d"
     ]
    },
    "id": "K3Iox1kEre3Q",
    "outputId": "69bd602f-3618-4ada-8821-3371c5d5a0fe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4535aa46ba485cb5befa35751f131b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14c50ba2c38427d83ade27547677db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c2110b539b4792b549c15ad6e97d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730c22632c5c4b01a54cf1fe7c3702c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4dc3e50dc84282a426e49575162d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sent_train.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1ee91afaef440987cb9903080dca6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sent_valid.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcef1bb40a924a3f8ef8fef110fa67d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ce152a64264f2ab182fc18a39256b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial sentiment dataset loaded: 1500 samples\n",
      "Sample entry: {'text': '$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT', 'label': 0}\n",
      "Formatting financial sentiment conversations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0d81c5c5664939ab6d0ea819a87329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted example:\n",
      "<|user|> What's the market sentiment for this news: $BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT <|bot|> Based on the financial news analysis, the market sentiment app...\n",
      "Tokenizing financial sentiment dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abee80cc4fb5488b9c997930c74dae9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing financial sentiment conversations:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Financial sentiment dataset ready!\n",
      "ðŸ“Š Total samples: 1500\n",
      "ðŸ”¤ Sample token length: 256\n",
      "ðŸ’¹ Dataset contains: Market sentiment analysis, trading insights, financial news interpretation\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for DialoGPT-small\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load financial sentiment dataset - perfect for trading/market analysis roles!\n",
    "raw_dataset = load_dataset(\"zeroshot/twitter-financial-news-sentiment\", split=\"train[:1500]\")  # 1500 samples for optimal training\n",
    "\n",
    "print(f\"Financial sentiment dataset loaded: {len(raw_dataset)} samples\")\n",
    "print(f\"Sample entry: {raw_dataset[0]}\")\n",
    "\n",
    "def format_financial_sentiment_prompt(example):\n",
    "    \"\"\"Format financial sentiment data for trading assistant training\"\"\"\n",
    "    text = example[\"text\"]\n",
    "    sentiment = example[\"label\"]  # 0=negative, 1=neutral, 2=positive\n",
    "\n",
    "    # Map sentiment labels to readable format\n",
    "    sentiment_map = {0: \"bearish\", 1: \"neutral\", 2: \"bullish\"}\n",
    "    sentiment_label = sentiment_map[sentiment]\n",
    "\n",
    "    # Create a market analysis conversation format\n",
    "    prompt = f\"<|user|> What's the market sentiment for this news: {text} <|bot|> Based on the financial news analysis, the market sentiment appears {sentiment_label}. This suggests {sentiment_label} market conditions for the mentioned assets.<|endoftext|>\"\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "def tokenize_financial_sentiment_function(examples):\n",
    "    \"\"\"Tokenize the formatted financial sentiment conversations\"\"\"\n",
    "    # Tokenize with proper padding for DialoGPT\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Consistent tensor sizes\n",
    "        max_length=256,        # Shorter for sentiment analysis\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # For DialoGPT, labels are same as input_ids but ignore padded tokens\n",
    "    labels = []\n",
    "    for input_ids, attention_mask in zip(tokenized[\"input_ids\"], tokenized[\"attention_mask\"]):\n",
    "        # Copy input_ids for labels\n",
    "        label = input_ids.copy() if isinstance(input_ids, list) else input_ids[:]\n",
    "        # Set padded positions to -100 (ignored in loss calculation)\n",
    "        for i, mask in enumerate(attention_mask):\n",
    "            if mask == 0:  # Padded token\n",
    "                label[i] = -100\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Format the financial sentiment dataset\n",
    "print(\"Formatting financial sentiment conversations...\")\n",
    "formatted_dataset = raw_dataset.map(format_financial_sentiment_prompt)\n",
    "\n",
    "# Show a formatted example\n",
    "print(f\"\\nFormatted example:\\n{formatted_dataset[0]['text'][:200]}...\")\n",
    "\n",
    "# Tokenize the financial sentiment dataset\n",
    "print(\"Tokenizing financial sentiment dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_financial_sentiment_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset.column_names,  # Clean up\n",
    "    desc=\"Tokenizing financial sentiment conversations\"\n",
    ")\n",
    "\n",
    "# This is your final dataset for training\n",
    "final_dataset = tokenized_dataset\n",
    "\n",
    "print(f\"\\nâœ… Financial sentiment dataset ready!\")\n",
    "print(f\"ðŸ“Š Total samples: {len(final_dataset)}\")\n",
    "print(f\"ðŸ”¤ Sample token length: {len(final_dataset[0]['input_ids'])}\")\n",
    "print(f\"ðŸ’¹ Dataset contains: Market sentiment analysis, trading insights, financial news interpretation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rWTIyS5oAySa",
    "outputId": "8ee6082c-7ced-4609-9e0d-e90d33b764fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting my DialoGPT financial sentiment fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 02:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.883900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.821200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.861800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.829000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! My model is saved to: /content/drive/MyDrive/finance-models/DialoGPT-Financial-Market-Sentiment-finetuned\n"
     ]
    }
   ],
   "source": [
    "# My SFT Trainer Configuration for DialoGPT-small Financial Sentiment Fine-tuning\n",
    "# No evaluation split needed - using full dataset for training only\n",
    "\n",
    "# My optimized parameters for LoRA training\n",
    "min_effective_batch_size = 8  # I increased slightly for DialoGPT-small efficiency\n",
    "lr = 3e-4  # I increased learning rate for smaller model and dataset\n",
    "max_seq_length = 256  # I reduced to match sentiment analysis tokenization\n",
    "collator_fn = None  # I'm not using a custom collator since I pre-pad in tokenization\n",
    "packing = False  # I disabled packing since I'm using fixed-length sequences\n",
    "steps = 25  # My logging and saving frequency\n",
    "num_train_epochs = 3  # I kept epochs low to prevent overfitting on 1500 samples\n",
    "warmup_ratio = 0.03  # I reduced warmup for smaller dataset\n",
    "\n",
    "# My SFT configuration with updated paths and names\n",
    "sft_config = SFTConfig(\n",
    "    # I'm saving my model to a new directory for this sentiment experiment\n",
    "    output_dir = '/content/drive/MyDrive/finance-models/DialoGPT-Financial-Market-Sentiment-finetuned',\n",
    "\n",
    "    # My data processing settings\n",
    "    packing = packing,\n",
    "    max_seq_length = max_seq_length,\n",
    "\n",
    "    # I disabled gradient checkpointing to fix potential errors\n",
    "    gradient_checkpointing = False,\n",
    "\n",
    "    # My training batch and precision settings\n",
    "    per_device_train_batch_size = min_effective_batch_size,\n",
    "    auto_find_batch_size = True,  # I let the trainer find optimal batch size\n",
    "    fp16 = True,  # I use fp16 for DialoGPT better compatibility\n",
    "\n",
    "    # My training schedule\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    learning_rate = lr,\n",
    "    lr_scheduler_type = \"linear\",  # I use linear scheduler for conversational models\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    weight_decay = 0.005,  # I reduced weight decay for smaller dataset\n",
    "    max_grad_norm = 0.5,  # I reduced gradient clipping for stability\n",
    "\n",
    "    # My logging and monitoring setup\n",
    "    report_to = 'wandb',  # I'm tracking my experiments with Weights & Biases\n",
    "    run_name = \"DialoGPT-Financial-Market-Sentiment-Trading-LoRA\",  # My updated run name\n",
    "\n",
    "    # My logging directory (updated path)\n",
    "    logging_dir = '/content/drive/MyDrive/finance-models/DialoGPT-Financial-Market-Sentiment-finetuned/logs',\n",
    "\n",
    "    # My checkpoint and logging strategy\n",
    "    logging_strategy = 'steps',\n",
    "    save_strategy = 'steps',\n",
    "    logging_steps = steps,  # I log every 25 steps\n",
    "    save_steps = steps,     # I save checkpoint every 25 steps\n",
    "    save_total_limit = 2,   # I keep only the last 2 checkpoints to save space\n",
    ")\n",
    "\n",
    "# I create my trainer with the prepared dataset and configuration\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                    # My loaded model (should be already loaded)\n",
    "    train_dataset = final_dataset,    # My prepared dataset from the previous script\n",
    "    processing_class = tokenizer,     # My tokenizer for text processing\n",
    "    data_collator = collator_fn,      # My data collator (None for default)\n",
    "    args = sft_config,               # My training configuration\n",
    ")\n",
    "\n",
    "# I start the training process\n",
    "print(\"Starting my DialoGPT financial sentiment fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Training completed! My model is saved to:\", sft_config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "0cde4467d2d14616ad2160865141ecbb",
      "5c8f79c2a32d4feea7c017f1b9c89a40",
      "66bec2a308a94629a1bdefc59dd72bb2",
      "1c26f4eacb2741e89bda461d3094f46b",
      "4b85fb71fb154611a5fcd52395ad9863",
      "ff68ced70ef442bea20e7f5e40f15dbb",
      "5ba2fbdbb2bd4a2e88609aea99ed1fb4",
      "0f783d5f10044730a1b9c7adb8846bfc",
      "2f2e9a6c5f8940cfb8ab38f567759def",
      "498fbec1cc4a4344b7df06baf6cb5cab",
      "07c30d624cb84ab3b05637d1f00a10fb",
      "1a0f01f1e0b044ae960da8948a345842",
      "05f4d328ea6b410689811bda0fa22b06",
      "d5bc6989f2934d44b5cace25cf232242",
      "babd1bf1bb764e7bbad6877e51e302c8",
      "f7d4ce6498114d7bbff5c1ffce3d52fd",
      "2b143deec2d747828895419216e1c3d7",
      "76b99e34659b4ab6afa1cec35348ea96",
      "95bc96de34a342ca955ad5d385b97f29",
      "e7591eb8ea2140b982225e215f3650bd",
      "6b0d35f641f347d8bbc4b1f6e4fb00ce",
      "1fff0dbe2b4d44b793968d4d14fa6523",
      "dc998f2847584b448020eb6de1f1f691",
      "c18479911d344e56b25fc3c45d2145f7",
      "5325ab067d5f46609fbebd3a84204e08",
      "a499da8763ad41fabb405887c9d33092",
      "682a88e7e5804aad94a1c8889acdb82e",
      "28ba0b8cf84d4de7811c81892569270c",
      "1b648c27c1814e91bebf278974f2b120",
      "abae6f22b80040089c9ef9f17af59d39",
      "c2b59dd980d845ab924acc44c07fbbe9",
      "6c36a043b10543cc98f8152aaa41c8b7",
      "f5c0840ca09641c2985d9e792662a513"
     ]
    },
    "id": "W7SyBkyK-S-a",
    "outputId": "ba76f27d-f9a7-4bf8-8c18-3795a647486e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving my trained DialoGPT financial sentiment model...\n",
      "Loading my PEFT model and merging adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving my merged model...\n",
      "Uploading my model to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cde4467d2d14616ad2160865141ecbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0f01f1e0b044ae960da8948a345842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc998f2847584b448020eb6de1f1f691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-sentiment-merged/model.safetensors:   0%|          |  550kB /  498MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upload completed! ðŸŽ‰\n",
      "Model is now available at: https://huggingface.co/sweatSmile/DialoGPT-Financial-Market-Sentiment-Trading-Assistant\n"
     ]
    }
   ],
   "source": [
    "# Step 1: I'm saving my trained model locally first\n",
    "print(\"Saving my trained DialoGPT financial sentiment model...\")\n",
    "trainer.save_model('/content/dialogpt-sentiment-saved')\n",
    "\n",
    "# Step 2: I load and merge the LoRA adapter with the base model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print(\"Loading my PEFT model and merging adapter...\")\n",
    "# I load the saved PEFT model (use the same path as Step 1)\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained('/content/dialogpt-sentiment-saved')\n",
    "\n",
    "# I merge and unload the adapter to get a single model\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Step 3: I save the merged model with tokenizer\n",
    "print(\"Saving my merged model...\")\n",
    "merged_model.save_pretrained('/content/dialogpt-sentiment-merged')\n",
    "tokenizer.save_pretrained('/content/dialogpt-sentiment-merged')\n",
    "\n",
    "# Step 4: I upload my model to Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"Uploading my model to Hugging Face Hub...\")\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path='/content/dialogpt-sentiment-merged',\n",
    "    repo_id=\"sweatSmile/DialoGPT-Financial-Market-Sentiment-Trading-Assistant\",  # My new repo name\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload DialoGPT-small fine-tuned on financial sentiment dataset for market analysis and trading insights with LoRA\"\n",
    ")\n",
    "\n",
    "print(\"Model upload completed! ðŸŽ‰\")\n",
    "print(\"Model is now available at: https://huggingface.co/sweatSmile/DialoGPT-Financial-Market-Sentiment-Trading-Assistant\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
