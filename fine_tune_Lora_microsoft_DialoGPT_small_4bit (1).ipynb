{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "LBD1d073Z35o",
    "outputId": "b9e12a94-fea0-42af-f9fd-37cbe2577fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Collecting trl==0.12.1\n",
      "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl==0.12.1) (13.9.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.1.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (2.19.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading trl-0.12.1-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, bitsandbytes, trl\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.47.0 numpy-1.26.4 trl-0.12.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "4199a265add74acd86e3a06d17054958",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install datasets bitsandbytes trl==0.12.1 transformers peft huggingface-hub accelerate safetensors pandas matplotlib numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ae5da50"
   },
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C0P5HtZ6Z5RR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    #AutoPeftModelForCausalLM, # Removed from transformers\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftConfig # Added to peft\n",
    "from huggingface_hub import notebook_login\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9F6uR-QnAPE"
   },
   "source": [
    "\n",
    "# Check for bf16 support and set compute dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "apjQgqH_m8o8"
   },
   "outputs": [],
   "source": [
    "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "calculate_dtype = torch.bfloat16 if support else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDTS5RrVm-42",
    "outputId": "99baf85b-3237-46c1-9ab2-9abed27d591f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(calculate_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBHrxjDJnbbf"
   },
   "source": [
    "#bnb config for loading 4 bit model with nf4 quant type\n",
    "* loading model with quantization config\n",
    "* device map to cuda\n",
    "* 4bit true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "60dab3cfbfc644dfa6de272158a2e949",
      "6ee210a166cf447b9c2f11cf7f615d1b",
      "e113b25de0734ebea68e78636f2d42a4",
      "cd3263026e1844e2acb54f77b96eee29",
      "c36510fec9b942269fde939115142107",
      "d1186ed9681347cb8951a15fe6165a62",
      "ea2ff1a889b54a6aaf1560883b838dfc",
      "3b377fcf60a6465e834cf3da608cef11",
      "562082abc3a945c1940afb44e26c9b1d",
      "537954e588344bbaafe374200fb4af1a",
      "9e172f67e1564c2b9c8da79158e3f7bd",
      "7b1d86a7195b4bcb9d980208ac724461",
      "f01f909cf0e542aaaef99dc787b30b22",
      "753d959680fd40719657b76e5645679a",
      "ffa24fe8b0a34009ad7434403cebb569",
      "8f8b0dedf50a4e64a4d161a3d7ef1c59",
      "dc2d592737d349d7a5fb92d7ebee26fb",
      "b91ed9d6499441eaa65a7a3aa2605548",
      "74d7519ee20f4d48a7e139475cdcc8d6",
      "5406caea68b74e4d8dc190ee1aec2730",
      "407529631d6a48a3aad31d57ff7619a2",
      "f5f9f0f9b43b45e49768502a04be7e80",
      "6d778895081544b1812c7565b1ce5790",
      "69e0669e2bf84649a8cc029ab5b0fcb8",
      "f47b4d58f35c4c28a6b0e8eab6cc8fe4",
      "b597679bb0634d119affd2147dad2b29",
      "b7a5cc2b9b8c441882a044d55d771543",
      "14e07ba42a6c40b3a8a062e592a762e2",
      "ab4c99eed4c84e42b277313e5a0b8e86",
      "91d3a8cc7f9a43788326913a05e53fc3",
      "7dfde844f2a74c74880b1990bdc295e2",
      "6d1681c0ecaa4cafa1a28ac229e72378",
      "a6defcb62ab9426da1dcf8eea7cdd5c6"
     ]
    },
    "id": "nkpxpotRZ5Ty",
    "outputId": "da483bee-055e-4d0a-e206-aa16dde40590"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60dab3cfbfc644dfa6de272158a2e949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1d86a7195b4bcb9d980208ac724461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d778895081544b1812c7565b1ce5790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= calculate_dtype, #calculate_dtype can be bf16 or float32- use bf16 if supported\n",
    "    bnb_4bit_use_double_quant= True\n",
    "    )\n",
    "repo = \"microsoft/DialoGPT-small\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, quantization_config= bnb_config, device_map= \"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzT757Suokja"
   },
   "source": [
    "#Check model memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D127_Z6FZ5WN",
    "outputId": "3a2e9262-e723-4612-920f-161c9cc04a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.8501205444336\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQyQHN6zos1k"
   },
   "source": [
    "#model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vm7RO981Z5Yh",
    "outputId": "07a8d273-0d1f-4bf6-c68a-183b411fe379"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSh3kYaioxjD"
   },
   "source": [
    "#Prepare model for kbit training\n",
    "##Use Lora Config\n",
    "\n",
    "\n",
    "1.   rank [4,8,16,32] - choose one\n",
    "2.   lora_alpha is a scalling factor which should be 2x the rank of matrix.\n",
    "3.   dropout range from 0.03 to 0.10 which helps prevent overfit\n",
    "4.   module - choose module as per requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzMopk81Z5ao",
    "outputId": "b584baea-304a-41c6-cf23-34c7e5db22c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT model successfully configured with LoRA!\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for quantized training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# DialoGPT/GPT-2 uses different layer names than newer models\n",
    "config = LoraConfig(\n",
    "    r = 8,  # rank of LoRA - [4-16]\n",
    "    bias = \"none\",  # [\"all\", \"lora_only\"] - for train bias term\n",
    "    lora_alpha = 16,  # scaling factor\n",
    "    lora_dropout = 0.6,  # prevent overfit - used for regularisation\n",
    "\n",
    "    # CORRECTED target modules for DialoGPT/GPT-2 architecture\n",
    "    target_modules = [\n",
    "        \"c_attn\",    # Combined Q, K, V projection (replaces q_proj, k_proj, v_proj)\n",
    "        \"c_proj\",    # Output projection (replaces o_proj)\n",
    "        \"c_fc\",      # Feed-forward layer 1 (replaces gate_proj/up_proj)\n",
    "        \"c_proj\"     # Feed-forward layer 2 (replaces down_proj) - Note: same name used twice in GPT-2\n",
    "    ],\n",
    "\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, config)\n",
    "print(\"DialoGPT model successfully configured with LoRA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzA5DOMp5aA"
   },
   "source": [
    "#once again check memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjtETM3QZ5fK",
    "outputId": "b694ecfa-3c69-4ce9-e067-0c3e91f3c5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207.7002182006836\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z2XsrHMp9kr"
   },
   "source": [
    "#Print base model to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGNI99ZVZ5hr",
    "outputId": "627e1efc-9f2a-4892-8286-6a545859ee0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.6, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.6, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.6, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.6, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.get_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiBoFpSgZ5j2",
    "outputId": "a779490e-0ad6-40b6-e2c5-c9031372b0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217.789464\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSmHEA7cqHKh"
   },
   "source": [
    "#Check for trainable Parameters and its percentage for a mathematical view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ff-gwq_dZ5mO",
    "outputId": "e6702c35-204d-48b5-e991-986edd664162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 1,179,648\n",
      "Total Parameters: 125,619,456\n",
      "Percentage Trainable: 0.94%\n"
     ]
    }
   ],
   "source": [
    "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Percentage Trainable: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oOA2lywqYKD"
   },
   "source": [
    "#ETL Process for Dataset Prep stage, Tokenizer load and define chat template if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351,
     "referenced_widgets": [
      "fde7192312294290ba6497cc77f57c45",
      "cb85070123904e3c99ebfb97e3446d5c",
      "3be1c317c60e4ce4bc3e8cd36043cd6f",
      "b22c654c71aa48d89053b79c7ed39d58",
      "ee88635f220048228bf7cd69ec4266ff",
      "cc32070e923e4b9eb7a790f650640cc9",
      "0f29b5dfeb074bdcad30b62cdb8ed240",
      "935237ed8f5d4acf91448532d6b3270a",
      "d2603dac232c4eb79d27454af58f1549",
      "bbb10c2634974b858a5e699c5922c3a1",
      "e7fdc3b8335d402a87bc29ed5b624d11",
      "532c8439f66c458a8c0fa784cdccdf58",
      "268bdb27accb4616a6b010ab47b5c715",
      "0a6869750e77479c89b66fbc36093314",
      "4deb11a2057d43aa90a24e478ede61be",
      "7fcae24fdcbe48068f69b638e3e68fa6",
      "800b8d132746497b8e2d8099002bd27a",
      "bdc984891a734a6bba5cec39d9c18047",
      "78555ce0e0b646eeaae44f54101111e4",
      "b8ac57596636452b8028cb878f6f5870",
      "5850690a45ea4993be37a558625f4648",
      "08b3c3d6105b40b3acf57eede4265a0d"
     ]
    },
    "id": "K3Iox1kEre3Q",
    "outputId": "dac3e916-fbfb-499b-8030-ae169ba1e534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Q&A dataset loaded: 1000 samples\n",
      "Sample entry: {'id': 'finqa0', 'query': 'Please answer the given financial question based on the context.\\nContext: interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) . if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million . foreign currency exposure as more fully described in note 2i . in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s . dollar-based exposures by entering into forward foreign currency exchange contracts . the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months . currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses . relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates . the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged . the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings . we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties . while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk . the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties . the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s . dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .\\n||october 31 2009|november 1 2008|\\n|fair value of forward exchange contracts asset ( liability )|$ 6427|$ -23158 ( 23158 )|\\n|fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability )|$ 20132|$ -9457 ( 9457 )|\\n|fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability|$ -6781 ( 6781 )|$ -38294 ( 38294 )|\\nfair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) . . . . . . . . . $ 20132 $ ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability . . . . . . . . . . . . . . . . . . . . . . $ ( 6781 ) $ ( 38294 ) the calculation assumes that each exchange rate would change in the same direction relative to the u.s . dollar . in addition to the direct effects of changes in exchange rates , such changes typically affect the volume of sales or the foreign currency sales price as competitors 2019 products become more or less attractive . our sensitivity analysis of the effects of changes in foreign currency exchange rates does not factor in a potential change in sales levels or local currency selling prices. .\\nQuestion: what is the the interest expense in 2009?\\nAnswer:', 'answer': '3.8', 'text': 'what is the the interest expense in 2009?'}\n",
      "Formatting financial advisor conversations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde7192312294290ba6497cc77f57c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted example:\n",
      "<|user|> As my financial advisor, please help me understand: Please answer the given financial question based on the context.\n",
      "Context: interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) a...\n",
      "Tokenizing financial advisor dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532c8439f66c458a8c0fa784cdccdf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing financial advisor conversations:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Financial advisor dataset ready!\n",
      "üìä Total samples: 1000\n",
      "üî§ Sample token length: 320\n",
      "üíº Dataset contains: Financial advisory, investment guidance, wealth management Q&A\n",
      "üéØ Perfect for: Wealth management, investment advisory, private banking, financial planning roles\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer for DialoGPT-small\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load financial Q&A dataset - PERFECT for wealth management & investment advisory!\n",
    "raw_dataset = load_dataset(\"ChanceFocus/flare-finqa\", split=\"train[:1000]\")  # 1000 samples for optimal training\n",
    "\n",
    "print(f\"Financial Q&A dataset loaded: {len(raw_dataset)} samples\")\n",
    "print(f\"Sample entry: {raw_dataset[0]}\")\n",
    "\n",
    "def format_financial_advisor_prompt(example):\n",
    "    \"\"\"Format financial Q&A data for wealth management advisor training\"\"\"\n",
    "    question = example[\"query\"]  # Fixed: dataset uses 'query' not 'question'\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    # Create a wealth management advisor conversation format\n",
    "    prompt = f\"<|user|> As my financial advisor, please help me understand: {question} <|bot|> As your financial advisor, let me explain: {answer} This information will help you make informed investment decisions.<|endoftext|>\"\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "def tokenize_financial_advisor_function(examples):\n",
    "    \"\"\"Tokenize the formatted financial advisor conversations\"\"\"\n",
    "    # Tokenize with proper padding for DialoGPT\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Consistent tensor sizes\n",
    "        max_length=320,        # Medium length for detailed financial advice\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # For DialoGPT, labels are same as input_ids but ignore padded tokens\n",
    "    labels = []\n",
    "    for input_ids, attention_mask in zip(tokenized[\"input_ids\"], tokenized[\"attention_mask\"]):\n",
    "        # Copy input_ids for labels\n",
    "        label = input_ids.copy() if isinstance(input_ids, list) else input_ids[:]\n",
    "        # Set padded positions to -100 (ignored in loss calculation)\n",
    "        for i, mask in enumerate(attention_mask):\n",
    "            if mask == 0:  # Padded token\n",
    "                label[i] = -100\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Format the financial advisor dataset\n",
    "print(\"Formatting financial advisor conversations...\")\n",
    "formatted_dataset = raw_dataset.map(format_financial_advisor_prompt)\n",
    "\n",
    "# Show a formatted example\n",
    "print(f\"\\nFormatted example:\\n{formatted_dataset[0]['text'][:250]}...\")\n",
    "\n",
    "# Tokenize the financial advisor dataset\n",
    "print(\"Tokenizing financial advisor dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_financial_advisor_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset.column_names,  # Clean up\n",
    "    desc=\"Tokenizing financial advisor conversations\"\n",
    ")\n",
    "\n",
    "# This is your final dataset for training\n",
    "final_dataset = tokenized_dataset\n",
    "\n",
    "print(f\"\\n‚úÖ Financial advisor dataset ready!\")\n",
    "print(f\"üìä Total samples: {len(final_dataset)}\")\n",
    "print(f\"üî§ Sample token length: {len(final_dataset[0]['input_ids'])}\")\n",
    "print(f\"üíº Dataset contains: Financial advisory, investment guidance, wealth management Q&A\")\n",
    "print(f\"üéØ Perfect for: Wealth management, investment advisory, private banking, financial planning roles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rWTIyS5oAySa",
    "outputId": "b78de9a9-ed33-443c-96d2-2efaa8a45be1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting my DialoGPT financial advisor fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchaubey-amit017\u001b[0m (\u001b[33mhectorlabs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250921_161345-sghqnmuh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hectorlabs/huggingface/runs/sghqnmuh' target=\"_blank\">DialoGPT-Financial-Wealth-Management-Advisor-LoRA</a></strong> to <a href='https://wandb.ai/hectorlabs/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hectorlabs/huggingface' target=\"_blank\">https://wandb.ai/hectorlabs/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hectorlabs/huggingface/runs/sghqnmuh' target=\"_blank\">https://wandb.ai/hectorlabs/huggingface/runs/sghqnmuh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>14.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>10.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.575100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.901400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.500700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>4.376300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>4.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>4.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>4.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>4.186200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! My model is saved to: /content/drive/MyDrive/finance-models/DialoGPT-Financial-Wealth-Advisor-finetuned\n"
     ]
    }
   ],
   "source": [
    "# My SFT Trainer Configuration for DialoGPT-small Financial Q&A Fine-tuning\n",
    "# No evaluation split needed - using full dataset for training only\n",
    "\n",
    "# My optimized parameters for LoRA training\n",
    "min_effective_batch_size = 8  # I kept your preferred batch size\n",
    "lr = 2e-4  # I kept optimal learning rate for DialoGPT-small\n",
    "max_seq_length = 320  # I updated to match Q&A tokenization\n",
    "collator_fn = None  # I'm not using a custom collator since I pre-pad in tokenization\n",
    "packing = False  # I disabled packing since I'm using fixed-length sequences\n",
    "steps = 20  # My logging and saving frequency\n",
    "num_train_epochs = 3  # I kept your preferred 3 epochs\n",
    "warmup_ratio = 0.05  # I kept optimal warmup for smaller dataset\n",
    "\n",
    "# My SFT configuration with updated paths and names\n",
    "sft_config = SFTConfig(\n",
    "    # I'm saving my model to a new directory for this financial advisor experiment\n",
    "    output_dir = '/content/drive/MyDrive/finance-models/DialoGPT-Financial-Wealth-Advisor-finetuned',\n",
    "\n",
    "    # My data processing settings\n",
    "    packing = packing,\n",
    "    max_seq_length = max_seq_length,\n",
    "\n",
    "    # I disabled gradient checkpointing to fix potential errors\n",
    "    gradient_checkpointing = False,\n",
    "\n",
    "    # My training batch and precision settings\n",
    "    per_device_train_batch_size = min_effective_batch_size,\n",
    "    auto_find_batch_size = True,  # I let the trainer find optimal batch size\n",
    "    fp16 = True,  # I use fp16 for DialoGPT better compatibility\n",
    "\n",
    "    # My training schedule\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    learning_rate = lr,\n",
    "    lr_scheduler_type = \"linear\",  # I use linear scheduler for conversational models\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    weight_decay = 0.01,  # I use standard weight decay\n",
    "    max_grad_norm = 1.0,  # I use standard gradient clipping\n",
    "\n",
    "    # My logging and monitoring setup\n",
    "    report_to = 'wandb',  # I'm tracking my experiments with Weights & Biases\n",
    "    run_name = \"DialoGPT-Financial-Wealth-Management-Advisor-LoRA\",  # My updated run name\n",
    "\n",
    "    # My logging directory (updated path)\n",
    "    logging_dir = '/content/drive/MyDrive/finance-models/DialoGPT-Financial-Wealth-Advisor-finetuned/logs',\n",
    "\n",
    "    # My checkpoint and logging strategy\n",
    "    logging_strategy = 'steps',\n",
    "    save_strategy = 'steps',\n",
    "    logging_steps = steps,  # I log every 20 steps\n",
    "    save_steps = steps,     # I save checkpoint every 20 steps\n",
    "    save_total_limit = 2,   # I keep only the last 2 checkpoints to save space\n",
    ")\n",
    "\n",
    "# I create my trainer with the prepared dataset and configuration\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                    # My loaded model (should be already loaded)\n",
    "    train_dataset = final_dataset,    # My prepared dataset from the previous script\n",
    "    processing_class = tokenizer,     # My tokenizer for text processing\n",
    "    data_collator = collator_fn,      # My data collator (None for default)\n",
    "    args = sft_config,               # My training configuration\n",
    ")\n",
    "\n",
    "# I start the training process\n",
    "print(\"Starting my DialoGPT financial advisor fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Training completed! My model is saved to:\", sft_config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "bf5d3ab26e994228a3f19dce5eeec559",
      "cf261bef5d0246b5b9adb8a6555cfd7e",
      "f84034466197485da1ea95c1678acbed",
      "ad12f27b4a584f548c0f4feabd66acf4",
      "7549cbeb567e489d93afdfa562e0c60a",
      "f7f1b1eea1524d84b60e440bbdb42a15",
      "e32e2236d90b4ac9bfc5856c008ce2de",
      "c6786765ec5e466991897c28f37ca4e5",
      "9dc0120936144eb5bd1161ed119fe610",
      "3f778918d3844384b4f33535e5c7adb9",
      "a35d1e14fd774b7c9f1c52f44ff54993",
      "8f5a5c0e11fb47b1b853304383cadf43",
      "556d5d9ba0024729924632c02665cc09",
      "98e6fe9f6ec44b53ac0ea79ab4f45187",
      "5fa36dc0e142467b93842dfa6750e89b",
      "a368e7b8618e4834aede9a33aa2129fd",
      "a416406a720e41b8a180b53e86ced508",
      "f2943a8c48524bbdadce518acb0ca856",
      "f8205fe6889e4d0290f41dfe72d3867d",
      "6547b5682ab64cf585aa7732a1c38ef3",
      "d82c7b387d114ccb94cd48131068d948",
      "623ab7e0058c41d3b38bed63f95a2195",
      "4e4f9d7cc2c34f5b955fa911a18aebb8",
      "6bd602bda02e45719c44260e73b03f10",
      "d026dcafa111474b9d8b53b98c7e20d1",
      "07576d3ad67140b7896b150571586c58",
      "b8d379bd213c40a4939900ba48831b15",
      "ccf5a1392a6b46f78ebf04a90140de3b",
      "63a0985fce064e79befdd31a9bca90a5",
      "1593db42ffb34d4a96206935b55be2f1",
      "c5faa1555c8a418db28fa24f8f8a8b8e",
      "ec6c325efd334470b247fdb7c2bea23c",
      "b95516232c4943edb43d0e5068dfae82"
     ]
    },
    "id": "W7SyBkyK-S-a",
    "outputId": "0540b20d-b4fb-43e1-f940-dc7a2933504f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving my trained DialoGPT financial advisor model...\n",
      "Loading my PEFT model and merging adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving my merged model...\n",
      "Uploading my model to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5d3ab26e994228a3f19dce5eeec559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5a5c0e11fb47b1b853304383cadf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4f9d7cc2c34f5b955fa911a18aebb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...pt-advisor-merged/model.safetensors:   0%|          |  550kB /  498MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upload completed! üéâ\n",
      "Model is now available at: https://huggingface.co/sweatSmile/DialoGPT-Financial-Wealth-Management-Advisor\n"
     ]
    }
   ],
   "source": [
    "# Step 1: I'm saving my trained model locally first\n",
    "print(\"Saving my trained DialoGPT financial advisor model...\")\n",
    "trainer.save_model('/content/dialogpt-advisor-saved')\n",
    "\n",
    "# Step 2: I load and merge the LoRA adapter with the base model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print(\"Loading my PEFT model and merging adapter...\")\n",
    "# I load the saved PEFT model (use the same path as Step 1)\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained('/content/dialogpt-advisor-saved')\n",
    "\n",
    "# I merge and unload the adapter to get a single model\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Step 3: I save the merged model with tokenizer\n",
    "print(\"Saving my merged model...\")\n",
    "merged_model.save_pretrained('/content/dialogpt-advisor-merged')\n",
    "tokenizer.save_pretrained('/content/dialogpt-advisor-merged')\n",
    "\n",
    "# Step 4: I upload my model to Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"Uploading my model to Hugging Face Hub...\")\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path='/content/dialogpt-advisor-merged',\n",
    "    repo_id=\"sweatSmile/DialoGPT-Financial-Wealth-Management-Advisor\",  # My new repo name\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload DialoGPT-small fine-tuned on financial Q&A dataset for wealth management and investment advisory with LoRA\"\n",
    ")\n",
    "\n",
    "print(\"Model upload completed! üéâ\")\n",
    "print(\"Model is now available at: https://huggingface.co/sweatSmile/DialoGPT-Financial-Wealth-Management-Advisor\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
