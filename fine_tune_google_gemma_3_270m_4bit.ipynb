{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "yZ5u-2Nrk-LS",
    "outputId": "c41f6b5d-13d6-46ab-ea49-fe191d34dd5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.47.0)\n",
      "Requirement already satisfied: trl==0.12.1 in /usr/local/lib/python3.11/dist-packages (0.12.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.34.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl==0.12.1) (13.9.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.12.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.12.1) (2.19.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets bitsandbytes trl==0.12.1 transformers peft huggingface-hub accelerate safetensors pandas matplotlib numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "u44dmmdnlK3o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    #AutoPeftModelForCausalLM, # Removed from transformers\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM # Added to peft\n",
    "from huggingface_hub import notebook_login\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ppAGs-jPlK6Y"
   },
   "outputs": [],
   "source": [
    "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "calculate_dtype = torch.bfloat16 if support else torch.float32\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= calculate_dtype, #calculate_dtype can be bf16 or float32- use bf16 if supported\n",
    "    bnb_4bit_use_double_quant= True\n",
    "    )\n",
    "repo = \"google/gemma-3-270m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, quantization_config= bnb_config, device_map= \"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UY_vkUNNlK82",
    "outputId": "ca092fd6-4be1-4804-e70c-f813bb9f911f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367.92016792297363\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGAkPVPDlK_b",
    "outputId": "9ad5d618-3f4d-44d8-d05a-afe4d0387475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear4bit(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVw1S1yZlLBr",
    "outputId": "61e5d099-db21-4fe4-cd34-785be2381cd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForCausalLM(\n",
       "      (model): Gemma3TextModel(\n",
       "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x Gemma3DecoderLayer(\n",
       "            (self_attn): Gemma3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=640, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=640, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=640, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=640, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Gemma3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=640, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=640, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=640, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (rotary_emb): Gemma3RotaryEmbedding()\n",
       "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 8, #. rank of LoRA - [4-16]\n",
    "    bias = \"none\", # [\"all\", \"lora_only\"] - for train bias term\n",
    "    lora_alpha = 16, # scalling factor\n",
    "    lora_dropout = 0.10, # prevent overfit- used for regularisation\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    "\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yrx7anWrlLEQ",
    "outputId": "01f0c8c2-2a9d-466b-8e46-f071afaa8713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695.2690448760986\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1Bfvcf0lLGl",
    "outputId": "76863bf8-cfe0-4001-f4ea-69cda17f244d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Gemma3ForCausalLM(\n",
      "      (model): Gemma3TextModel(\n",
      "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-17): 18 x Gemma3DecoderLayer(\n",
      "            (self_attn): Gemma3Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=640, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=640, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=640, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1024, out_features=640, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            )\n",
      "            (mlp): Gemma3MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=640, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=640, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=640, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "            (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "            (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "            (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "        (rotary_emb): Gemma3RotaryEmbedding()\n",
      "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.get_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwHjKk_RlZBl",
    "outputId": "d849e0ab-4c6e-4685-c257-4e3d89092875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729.042434\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZxafvRclZED",
    "outputId": "8bda17a0-ac17-4775-9884-f9c36d553af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 1,898,496\n",
      "Total Parameters: 269,996,672\n",
      "Percentage Trainable: 0.70%\n"
     ]
    }
   ],
   "source": [
    "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Percentage Trainable: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76dcFu4ilZGw",
    "outputId": "75dca7ef-2ffc-49ff-9d23-6b184ade811a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 699\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sweatSmile/buddha-taught-qa\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irbo0eDAle-3",
    "outputId": "686c21f1-b863-4be0-d02e-bd44a2e98583"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is referred to as the Fully-Enlightened One in the text?',\n",
       " 'answer': 'The Buddha is referred to as the Fully-Enlightened One.'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXjcgzQ6lfBw",
    "outputId": "197c7b58-ead8-40e7-89e2-d1533ecda275"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 699\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"question\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"answer\", \"completion\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmEfTkkKlfEO",
    "outputId": "bf38d194-345f-4c3e-eada-175c31adc559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Who is referred to as the Fully-Enlightened One in the text?',\n",
       " 'completion': 'The Buddha is referred to as the Fully-Enlightened One.'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mReEM39bllBY",
    "outputId": "5beeb12b-69b8-4b28-a6be-95721a042db0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Who is referred to as the Fully-Enlightened One in the text?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The Buddha is referred to as the Fully-Enlightened One.'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0]['prompt']},\n",
    "    {\"role\": \"assistant\", \"content\": dataset[0]['completion']}\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "ACa-QbHr1H23",
    "outputId": "10116062-89b0-4e79-c9e4-9334855e5545"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<start_of_turn>user\\n' + message['content'] + '<end_of_turn>\\n' }}{% elif message['role'] == 'system' %}{{ '<start_of_turn>system\\n' + message['content'] + '<end_of_turn>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<start_of_turn>model\\n' + message['content'] + '<end_of_turn>\\n' }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<start_of_turn>model\\n' }}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set pad token to be the same as eos token\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<start_of_turn>user\\n' + message['content'] + '<end_of_turn>\\n' }}{% elif message['role'] == 'system' %}{{ '<start_of_turn>system\\n' + message['content'] + '<end_of_turn>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<start_of_turn>model\\n' + message['content'] + '<end_of_turn>\\n' }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<start_of_turn>model\\n' }}{% endif %}{% endfor %}\"\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Icn9sXck1Zpj",
    "outputId": "641db035-d2b1-4b7f-dad6-802454d26a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "Who is referred to as the Fully-Enlightened One in the text?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The Buddha is referred to as the Fully-Enlightened One.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732,
     "referenced_widgets": [
      "d82b3a30960c4e2f9b51a7f57b08df68",
      "67168a81b27e47128372393ec7ee9c99",
      "4dfb0290d6c94b3fadfa61d47596302d",
      "8e759f82062e41bcb179ee9fc913a821",
      "580b8dfb41f447d1a82554bcd0fcb244",
      "e2aa0433a6a246b7bb1107cc3d414014",
      "f114704f3720473d8103d44018fd6257",
      "0e95e9bf6cf04286af1dc0540f1c538c",
      "416a6b4435604d3fb0c61591c2a951e2",
      "de13a7c314d0422486e2145e4a0f3e9a",
      "e9a311962f53498fbb0c94247f266052"
     ]
    },
    "id": "EX6sFxpgF7Ed",
    "outputId": "3e9144ea-65b8-42c5-d487-b7061be283c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82b3a30960c4e2f9b51a7f57b08df68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/256 06:11, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.714600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.505200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=256, training_loss=1.8118571266531944, metrics={'train_runtime': 373.4104, 'train_samples_per_second': 12.019, 'train_steps_per_second': 0.686, 'total_flos': 176172906184704.0, 'train_loss': 1.8118571266531944, 'epoch': 8.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min_effective_batch_size = 8\n",
    "# lr = 3e-4\n",
    "# max_seq_length = 64\n",
    "# collator_fn = None\n",
    "# packing = (collator_fn is None)\n",
    "# steps = 20\n",
    "# num_train_epochs = 10\n",
    "\n",
    "# Optimized parameters for better training stability and performance\n",
    "min_effective_batch_size = 6  # Slightly reduced from 8 for better gradient updates\n",
    "lr = 2e-4  # Reduced from 3e-4 for more stable training\n",
    "max_seq_length = 64  # Keep at 64\n",
    "collator_fn = None\n",
    "packing = (collator_fn is None)\n",
    "steps = 15  # Slightly more frequent than original 20\n",
    "num_train_epochs = 8  # Reduced from 10 to prevent overfitting\n",
    "warmup_ratio = 0.05  # Small warmup for learning rate stability\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir = '/content/drive/MyDrive/google/gemma-3-270m-ada',\n",
    "    packing = packing,\n",
    "    max_seq_length = max_seq_length,\n",
    "    gradient_checkpointing = True,\n",
    "    gradient_checkpointing_kwargs = {'use_reentrant': False},\n",
    "    gradient_accumulation_steps = 3,  # Slightly increased\n",
    "    per_device_train_batch_size = min_effective_batch_size,\n",
    "    auto_find_batch_size = True,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    learning_rate = lr,\n",
    "    lr_scheduler_type = \"cosine\",  # Better than linear decay\n",
    "    warmup_ratio = warmup_ratio,  # Gradual learning rate warmup\n",
    "    weight_decay = 0.01,  # Light regularization\n",
    "    max_grad_norm = 1.0,  # Gradient clipping for stability\n",
    "    report_to = 'wandb',\n",
    "    logging_dir = '/content/drive/MyDrive/google/gemma-3-270m/logs',\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps = steps,\n",
    "    save_strategy = 'steps',\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 2,  # Keep only 2 best checkpoints\n",
    "    fp16 = True,  # Mixed precision for memory efficiency\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    processing_class = tokenizer,\n",
    "    data_collator = collator_fn,\n",
    "    args = sft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "jMseFYnulwUz"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('Gemma-3-270m-4bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLSsbFo8lzAG",
    "outputId": "58e2de55-9913-4dd7-98e7-dab827cb7a4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForCausalLM(\n",
       "      (model): Gemma3TextModel(\n",
       "        (embed_tokens): Gemma3TextScaledWordEmbedding(262145, 640, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x Gemma3DecoderLayer(\n",
       "            (self_attn): Gemma3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=640, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=640, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=640, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Gemma3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=640, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=640, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=640, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (rotary_emb): Gemma3RotaryEmbedding()\n",
       "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=640, out_features=262145, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model = AutoPeftModelForCausalLM.from_pretrained('Gemma-3-270m-4bit')\n",
    "reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jV2xWEolzCY",
    "outputId": "a0ee4185-bb49-4090-ecd9-a11a789050bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262145, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262145, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model = reloaded_model.merge_and_unload()\n",
    "merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "f191d314185144d185b83c6e538049ed",
      "392731a070444775ad5c4bf46e83057d",
      "67ca94e2f87e4fa0a3cf484b117ad234",
      "02ae49e42c98401f97cb6763852a355a",
      "8122a7d20fd94ae88827678ccc1e8a9b",
      "5155713a5dc5440caeb4ec2994da72e6",
      "f2929ee47da64afbafe3f4bacf1167e6",
      "fbb40cd87f6147c88c8572379c9dd45b",
      "02a3b3805cc64b11962bea58c225bcd9",
      "1deee06b1af1480a95d5eeb1a8a361a0",
      "06115c5510b741efa551b96e0e433649",
      "a9e1f0c17a144a468a320f0bc45c867e",
      "7390e32e743043c7a3e7f50b02671c83",
      "67affe91ccdf4555a9de9f565cfd3586",
      "8fc1483c410f4e19b6485155dd15ed01",
      "ccf488b5b96a4281b493e9a11d9b9bd7",
      "0fbc9b369e924bcb90adf9399bfa238c",
      "6dba78abf2c74fbab9f5c879e9fc2136",
      "39a511ca4ce14d3abc1d006302896192",
      "ce643a379c414e33ad3587ff2c9d7918"
     ]
    },
    "id": "2dazBCIslzEX",
    "outputId": "82b9ae94-b06c-450e-808f-0d4fdce4d652"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f191d314185144d185b83c6e538049ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280,
     "referenced_widgets": [
      "c63ed32875ad4b82bd6cd08dead3a6a1",
      "43d4e5492ee54280850dea1fedfa376c",
      "315c1c2d8fff4486ac2adf88ca59effd",
      "12e445a0e7764639a0814b7115075b20",
      "9c9946ad34814568a46294954f2bca46",
      "243fffc8fe3a48cab99ced7defd8c65b",
      "01e42935a25c4c14b0bdd558993c935a",
      "09e34226e8874c139a721f97c0da01b3",
      "9c53c8411c3549eaa640270d3a8c7fbb",
      "ea1e47678df945aebd5ccf0527e19dde",
      "f147b8ab4de8477cb983517157be660f",
      "b3cd787bbe494a558b9fff055cdecd55",
      "4262381ba0604dedbfd3980099c95cbd",
      "24085d7d66ae4684b908342a59efd37e",
      "5d92506a3df0463497a12af7ff825e03",
      "98bb8a01c3324c86a89c9efad83859e4",
      "742a22064b764e678db1e28a1e2e81f9",
      "47b97391e43d4a57b9e6da965c563bf2",
      "7458faab55754af8acbc5d68932f69d0",
      "3ce4dd72fc534f86a25e69d35774fdea",
      "3c8ee766651b4acdb6955e867ff73d74",
      "47f6e3de92954577b451324e0aee4d0d",
      "bf4a76d8ba03423da65c545d5f12eb9b",
      "29119bcb9fcb4b3d999e81bf09abc105",
      "55e379ae923f4762ae8257ac08f1855c",
      "a0ddf5e70b0941778e96685347f4ef55",
      "aa7b45d4494a44bb90f9892120b0ec14",
      "b59dcd16a0574702ad64f0a58920a6fa",
      "604141e0a3314a1ab160123f7d98d5d0",
      "24aa29c888d145b48495c45bcc918d30",
      "f5d37427685d4eec813e7e47c1a5f97d",
      "07d2e1f0001e47a09b4fee1b4bc34f1b",
      "fa751bf108154fdd9afc5f3fe2c6ed35",
      "4debf9eb3f334ed18df8744c032d8c49",
      "46aaff590d484e429474fa902ff5b058",
      "b74e3970a9004033ba7521b128059b2a",
      "a737ce4899c548a494ccdaa9e15c4cd8",
      "5846509a5456483fb562a817da20103c",
      "1f3e7ca2a12b40d78957bbdb4d9a46bf",
      "e46df9a92332484b877f333fc41e4712",
      "ec8a1b1536914d92a180f92a1471926a",
      "ce960cf721a642fd9a36a08a357bebae",
      "ef22cae584f047d4a619f75dfe725efb",
      "a021e2699f5c4bed9daeb5c166302a78",
      "4c2d8683eb774fb188feff9ee757e270",
      "53866904fb37404099af9eb57a204452",
      "472abf5485414734abba6391106fc089",
      "73c8e7d19eeb40f4ba80450493989c7c",
      "79c81084778049d186f485c41c058b72",
      "b7ab628017c84b1ea037aaeca297dafc",
      "aba356d6067d492a9fff622bc78aa395",
      "d7838fa2da504e438cf0e9ba268041a4",
      "ed27a91a3d8549b1bb0986bb8f945ee8",
      "fa312976a0144429bd8822bb392ec0dc",
      "4f873fbd7c70459594394c436b8453c7",
      "867fa0df256a47729d0b51b96b13b75e",
      "ef09dd58585f484083faa4ae3d5e98cc",
      "135bd58269e94160a826f6107ab14971",
      "051647189d19441094e836b887479b0f",
      "61f9a193dd6f4804bed6d8cc8864b2a0",
      "4d8e585854de4708a1b0a84337867be3",
      "3ff0c1eedc4f46c5b9452115ca97a475",
      "0a7ab320675d4166add799bd4195a3c3",
      "cc613c4568c349fcab61140ec5992de6",
      "fa2f593b11df4d11ae7c20269647b64c",
      "63c321f3d0734215aac954322794f6dc"
     ]
    },
    "id": "23f611e8",
    "outputId": "fb46e18b-d00d-425a-b5aa-d6613baeed76"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63ed32875ad4b82bd6cd08dead3a6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cd787bbe494a558b9fff055cdecd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4a76d8ba03423da65c545d5f12eb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...Gemma-3-270m-4bit/training_args.bin: 100%|##########| 5.69kB / 5.69kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4debf9eb3f334ed18df8744c032d8c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...t/Gemma-3-270m-4bit/tokenizer.model: 100%|##########| 4.69MB / 4.69MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2d8683eb774fb188feff9ee757e270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...270m-4bit/adapter_model.safetensors: 100%|##########| 7.63MB / 7.63MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867fa0df256a47729d0b51b96b13b75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...nt/Gemma-3-270m-4bit/tokenizer.json: 100%|##########| 33.4MB / 33.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sweatSmile/Gemma-3-270m-Buddha-QA/commit/9f16a67f2aad6f49d51614a5918b24499f7aaddc', commit_message='Upload Gemma-3-270m fine-tuned on Buddha QA dataset', commit_description='', oid='9f16a67f2aad6f49d51614a5918b24499f7aaddc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sweatSmile/Gemma-3-270m-Buddha-QA', endpoint='https://huggingface.co', repo_type='model', repo_id='sweatSmile/Gemma-3-270m-Buddha-QA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# Set your token directly\n",
    "HF_TOKEN = \"Token_here\"\n",
    "\n",
    "# Define your folder path (update if different)\n",
    "folder_path = \"/content/Gemma-3-270m-4bit\"  # Change this if needed\n",
    "\n",
    "# Define your model repo name\n",
    "repo_id = \"sweatSmile/Gemma-3-270m-Buddha-QA\"\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"/content/Gemma-3-270m-4bit\",\n",
    "    repo_id=\"sweatSmile/Gemma-3-270m-Buddha-QA\",\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload Gemma-3-270m fine-tuned on Buddha QA dataset\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
