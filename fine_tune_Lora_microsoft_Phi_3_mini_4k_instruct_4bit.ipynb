{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "LBD1d073Z35o",
    "outputId": "825ead10-3f81-4cae-b035-5b03a66d5846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting trl==0.12.1\n",
      "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl==0.12.1) (13.9.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.12.1) (2.19.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading trl-0.12.1-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m126.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, bitsandbytes, trl\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.48.2 numpy-1.26.4 trl-0.12.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "bd719b1550254076b1c41fa844b1f765",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install datasets bitsandbytes trl==0.12.1 transformers peft huggingface-hub accelerate safetensors pandas matplotlib numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ae5da50"
   },
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C0P5HtZ6Z5RR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    #AutoPeftModelForCausalLM, # Removed from transformers\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftConfig # Added to peft\n",
    "from huggingface_hub import notebook_login\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9F6uR-QnAPE"
   },
   "source": [
    "\n",
    "# Check for bf16 support and set compute dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "apjQgqH_m8o8"
   },
   "outputs": [],
   "source": [
    "support = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "calculate_dtype = torch.bfloat16 if support else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDTS5RrVm-42",
    "outputId": "224a34b0-a4e4-46b3-8f35-5e6fb7b8849e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(calculate_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBHrxjDJnbbf"
   },
   "source": [
    "#bnb config for loading 4 bit model with nf4 quant type\n",
    "* loading model with quantization config\n",
    "* device map to cuda\n",
    "* 4bit true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "86dbc5739e7a4eaca76c10b7f732edca",
      "a76ade38073b4da3a4b253bebf641fc4",
      "f5d0099e8c224722876ad35ce87d5d11",
      "e575bea34d02447c8a8cba0ae3bff305",
      "4b68683b4247439787e1b5f988dd6302",
      "8696f7b0c11b4921be4a2a63201675be",
      "efcd916beb7a46698e1a6d419b911246",
      "b9076e8b175c442da21fb7e31fddb3b1",
      "f8fad38a3e244bc8a0c8f30d2549dc4b",
      "d0bba41f5b5e4787bfd77ea2aa75ad85",
      "6c9488d2cb7c40208fe3cee1d6d59bbf",
      "87d0f1457ef74fc38ada50a25833f893",
      "d709419374a64ee3a1822a85b449809e",
      "9ea65914b5a948e295b71da46d5333c0",
      "fc5203a2e85f4ffbbff86f60c7699a22",
      "574851f321764f1f9beb59873ae0ad5c",
      "858f12a7230644aea5f84aec55ecd31f",
      "891b06bb7dcd409b851b02d106c43905",
      "c49226f3174b47d1b3d0cf3e79d1a373",
      "52214b9807e14e6286a621b1c9ca754a",
      "5a6fa0c2a2cb492da054196a1775daa6",
      "16ccc4574a8f43cbba497b193dc80048",
      "9cb9487f8909494d8f92cc91bf524ee8",
      "8fa723f5d374458091519076345482d7",
      "c5de3f39541c4999b2d18760559ed8f2",
      "0d68112a1eff4084bcbb09f4937344bd",
      "0296c4bb310441a2a52de28c0de1c27a",
      "f65d69c485e448339d398e5aab08ba20",
      "a9012dd14a944e019450e8259f719d3c",
      "53a28921f24d48d2a32d7fe4e9d70144",
      "18da48c37c55454dbbc4b36e92cbb52e",
      "2c67ae52eb184e5e87e71d5f4d2eb3ff",
      "670986c06bda45ffb91f243b1c051962",
      "23df2db484764fc8b2d494331c27ff62",
      "92a7fdd00de847ab8d84ee85ca0e592f",
      "dee1d79743fe4b82a011dbb2e2f0426b",
      "e170d3083e504085bd65d07a0661160c",
      "10292c4ba96c419ab2528a4a6e74a5d9",
      "275b025a3da143c5afa2640c34f1f211",
      "f2f6fdd21b4447d888be0d64ccaadd24",
      "12e22f2eb48243bc90dd54062de880ba",
      "296ce095dd5c49cea5fbcd08e60ea749",
      "ff9aa9d5bda7440d8e15d2de582e98f7",
      "c35fc4b60d54439fb222ac1ec6bd6250",
      "ce5a3297971940fbbb8be96274318c61",
      "b1325bcd15594525b809181d5f3c5516",
      "5780bb843c654328a8add17d512de5e3",
      "5045c350c88b486199db73da8eb1efb7",
      "63f5c0284755458fa6950ea317ef6340",
      "b818031e8fba4523956313989e130415",
      "1cde367ec9d544a8aaef23578f9d5040",
      "dc8c3428cffd43f5ab36482017e42be7",
      "6638434bb23446279b8ea28bb599b3e8",
      "579b96b02c7c426aa8f28d6af3021998",
      "119d32e4c853473f90eeff0bffa3dbc5",
      "c2597c50dbf442b7aae86122fc72c847",
      "fb6e20a3475a498c8ffbd0e4bbc0fe7c",
      "f51c896cdd904a69b4a1682928e6bb87",
      "36f13bc688f646a0aafbdb59a5e750d4",
      "51c88f86836540628ec2582215dc2d02",
      "16c20fdd1c3141ea9f4e95510edc8048",
      "ac4e7a35eda24f6babad7b5f074296f4",
      "354777c2a10e4d57b896aa33acde5463",
      "cf26ceff73224135b51a14761fe5816c",
      "ebcde46dd6ed4a0fa62aa3ae0c15f72c",
      "c67281da08784392adb76f3939021cac",
      "926e39b6183a4f859486b126b1993a2f",
      "8709ac6fea40466585d8ab0fffa9107d",
      "8c180dedbb9742a88a70eedcae3f0d88",
      "cbc47b8a5724429dbd6b2185f1bb5a61",
      "9dfbb50a97c046e7b5b4033379015e4a",
      "022bf98b5926429da987ca57a43e17cc",
      "80d67b9697084ec19c70f9bb4f1de8ee",
      "8d13eb8cf9dd45c6b873b2419966b060",
      "1e6761f08a8740ddb1138d2b53991dfd",
      "8b1e5385576b4b19a0656d8510ce7c86",
      "ce0fc526fe8e4a898dfd6002da04eecb"
     ]
    },
    "id": "nkpxpotRZ5Ty",
    "outputId": "8882e535-750b-4aff-e926-45eabcefa4cf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86dbc5739e7a4eaca76c10b7f732edca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d0f1457ef74fc38ada50a25833f893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb9487f8909494d8f92cc91bf524ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23df2db484764fc8b2d494331c27ff62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5a3297971940fbbb8be96274318c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2597c50dbf442b7aae86122fc72c847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926e39b6183a4f859486b126b1993a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= calculate_dtype, #calculate_dtype can be bf16 or float32- use bf16 if supported\n",
    "    bnb_4bit_use_double_quant= True\n",
    "    )\n",
    "repo = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, quantization_config= bnb_config, device_map= \"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzT757Suokja"
   },
   "source": [
    "#Check model memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D127_Z6FZ5WN",
    "outputId": "338edb11-f9d1-478b-960a-8b0904270016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104.1310424804688\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQyQHN6zos1k"
   },
   "source": [
    "#model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vm7RO981Z5Yh",
    "outputId": "829754ab-674b-4578-c195-c0a9def15a83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSh3kYaioxjD"
   },
   "source": [
    "#Prepare model for kbit training\n",
    "##Use Lora Config\n",
    "\n",
    "\n",
    "1.   rank [4,8,16,32] - choose one\n",
    "2.   lora_alpha is a scalling factor which should be 2x the rank of matrix.\n",
    "3.   dropout range from 0.03 to 0.10 which helps prevent overfit\n",
    "4.   module - choose module as per requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzMopk81Z5ao",
    "outputId": "685a04b5-8d81-45fc-85ef-0d0646a6ed04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 8, #. rank of LoRA - [4-16]\n",
    "    bias = \"none\", # [\"all\", \"lora_only\"] - for train bias term\n",
    "    lora_alpha = 16, # scalling factor\n",
    "    lora_dropout = 0.10, # prevent overfit- used for regularisation\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    "\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzA5DOMp5aA"
   },
   "source": [
    "#once again check memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjtETM3QZ5fK",
    "outputId": "5292bb59-697d-4244-bf4a-7fe3af4da49a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497.2619018554688\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1024/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z2XsrHMp9kr"
   },
   "source": [
    "#Print base model to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGNI99ZVZ5hr",
    "outputId": "e4fe0d33-27c8-4098-e95f-e2ddc2ceb363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Phi3ForCausalLM(\n",
      "      (model): Phi3Model(\n",
      "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x Phi3DecoderLayer(\n",
      "            (self_attn): Phi3Attention(\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
      "            )\n",
      "            (mlp): Phi3MLP(\n",
      "              (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (activation_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (rotary_emb): Phi3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.get_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiBoFpSgZ5j2",
    "outputId": "8c0ff035-04c2-4a64-a76e-81a49b751791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2618.568896\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSmHEA7cqHKh"
   },
   "source": [
    "#Check for trainable Parameters and its percentage for a mathematical view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ff-gwq_dZ5mO",
    "outputId": "7c7608d4-48c6-47bf-d719-416593d9ada9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 4,456,448\n",
      "Total Parameters: 3,825,536,000\n",
      "Percentage Trainable: 0.12%\n"
     ]
    }
   ],
   "source": [
    "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
    "percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Percentage Trainable: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oOA2lywqYKD"
   },
   "source": [
    "#ETL Process for Dataset Prep stage, Tokenizer load and define chat template if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3698f070b9d34b65be4da893cbd4f30c",
      "0768273575f9479d9a3347a4ee26dc90",
      "9b226f189a174285a8908c873d77aac6",
      "7113cda6bfe1470f9afbe4d1ec0dda10",
      "e413310892f146e7bd561fc15ac0e8e5",
      "9e57e79158ff4a25b7bc98d7efbf26be",
      "70ea9ab12e2742c5a4e6294633481513",
      "0060c2569162428dba752eee8a70f13b",
      "b31d7d23b6004c6782b01436faf27289",
      "2c477abdd680455e9f0785348969278a",
      "0fba6156d03b4cc8b59c5180529807de",
      "9a6211ce062c457694470771b1da6169",
      "b01afc7acbc24a4ba4080e5df55981f5",
      "cd90b02ec5124f72adc66057c9d921a8",
      "efbea06b91bc4dab9501588ae7b65800",
      "38e82580d8e949acb57a48fdbe8916f2",
      "7d80a833688c459688573b416465a0eb",
      "06c2a1413cdb45b7a2d61099596f366e",
      "e4fcc620610244bf801d047ae20ae76e",
      "29eb2d2547ce4130b97adf4d7fe64ebb",
      "823ce44dec1b4b40b5d96361dee59272",
      "afe100f43b304930b97a6d831f40fe4a",
      "e2210a3724bd4c45ae899288e0ebabeb",
      "8b0fcc39e7e24191ac035a60b5c030ab",
      "aa7ddbb0517b441d90fffa1c877f212c",
      "180f0d4e1d6448ff8a423625b0c6b56d",
      "e56d60c7c3bd42f8888b6005758ffcc7",
      "0ca6be346e1940e7a14c494c89c8a3d6",
      "5a3fa1064b44427b92673b86a0d53c89",
      "6a4fa95738994ef49569fc879427688f",
      "16f11ce7b60443268eb28788962d1225",
      "6133d0854291476883407c383e6b8957",
      "d883b9fa0d9647ae837a539c2dd671eb",
      "93b9d2cc9bea4699b22aad07aeadc9d9",
      "4f2219d14e4c47d1914b0f2a83064e0f",
      "fcc48a1a5b654620934c0fb51f1f2866",
      "cf69debacd304575ab4e56363d208b78",
      "9b4b656ba0b0455392ecddccc688249a",
      "0f5b500aa5c2411c92c3c29b883dddb7",
      "f8a54e46f1ba48ecb1e0312b99bb3aef",
      "5ba1bcffb415443e8d7cde1a2e0c2af4",
      "d640a088142040159c86da3bba63046a",
      "6110a7ab94fb4adcad84922f8e3cf89c",
      "09e2566161a24139b5ad5dc393bbd681",
      "6e65d14477d54284b6aa9da249378705",
      "28727e65656d46fe8096a90b84ff26ec",
      "d8c9862f269243e29b154abcf859c44d",
      "e6a462c83f914020a99b3e9a9a2d5cad",
      "635cbd8af8884b4fbfc25ecc506f36f3",
      "979d3f88be0544038f06082dd8b0dd13",
      "5914d38bfd9f46b397ae16d7cf53cc7c",
      "d6cda0ba0a924f168685bfa7c0424e2b",
      "6fe0c487125346a091d6fc660682a133",
      "1a0460d67d8840b9b8dc7914b4df2e18",
      "19d20f6f3fb24066a0a23b9c76f73352",
      "bdadcecb6186412db3b78e0b6bd9fdda",
      "20a6db2f004a44f19dbb094be563694a",
      "595a065994004befa5d5da737e35d3b9",
      "ef403981b70f4645ae56297a4dfcc5a5",
      "f4def08f8ae34f9d8344dc2b266fca9c",
      "486464d4326a45a2adffbffa24307078",
      "de54aab9e72845ec9bb03bd893c992a9",
      "d608cc9bd09446d785601972c1b01f39",
      "61de08a7df91469daaf934abb9fb4ed7",
      "9e7f39c8ca2a4fd4b13dda737f6c38a8",
      "4fa01405dd7e4366b591b8ad23d25808",
      "cea5b0d04b0644b7ae294e53f371a33e",
      "64842025c19f4bc19ee7c9f4f9a06f14",
      "2caa531406a948fbb0ef2794fcdee240",
      "24590c18209747a7ac51611bb5f03ca9",
      "c8a3350aa14e4714b73bd83cad11dde0",
      "79eb0b94c30c4646adf78e373a7c6793",
      "47e5f5eabf5f40ef972163bf161c9d37",
      "0866b36052b6471684f6a519c18af5f2",
      "81d7e188007f46b58e5b4b634c65682f",
      "b914e488400744c2a0847a0da3b0b18a",
      "0942033993f046ef8c31e351ea30741b",
      "984bad60c9d94d7bb21be259cc1720ee",
      "fc1c71c9b4f444ebb089b02293361cef",
      "74a4bc97650e425895648e341ca0d060",
      "f51da19a057248cd987adc15ef90b4e1",
      "b8d4295bcfc74d11baff46d8fdda6472",
      "c5b57d26ff3945a49e5a7989e91fe33c",
      "fd20716d76964d7a9920dd7c9321259a",
      "1dd2e3c2bc754baea7ee34d83f27d336",
      "25d049df0f624ceeabe50b110d14800b",
      "a193f074b7524dee9d8e5ec719869c67",
      "6c0006942f2b4367989b5fa69ac8fc68",
      "674912c7bac2464cb1900fbd5b24f4eb",
      "295a996b4e144ec2a4f90777e3621ad1",
      "06f8f876c8f4485fbc865f66a424b86f",
      "5cea7075fecf48e1a5d0b4bdb9a19cc6",
      "8db2a7b9a9e44791beccb66bc0dff25e",
      "b45e3a16eec34f76b67977b8fe47083c",
      "439e503717ab47a0a90307c192384fda",
      "4cc650125753442e9ce3102b1544bf84",
      "51d3fe0bce434e688c78bbe35eb39713",
      "9345dbe447614817933b24d79ca89fb8",
      "5f6672b6bf53446f90a43535527497ac",
      "8cebd8e40fde4173b9862d0d82a638d5",
      "c90d7c44e24643428fd6feaad30efeb9",
      "42455bc39b8643da860a7eb73b175afc",
      "f521b72f8d16454594c3b10e221f941c",
      "b42eb0a007dc4c6088891d0981fc63f4",
      "6c9483706c234198a943f20b85485520",
      "1289e2db0f64437ca0e63ef841977fe3",
      "4134a21dd27743558c0648b17983c690",
      "4cf9096bab0d49ffa3dce07fb7d608b2",
      "3a468c5149e24bca83381b73db682664",
      "3b6af9365c48415181481ad7cd6480e1"
     ]
    },
    "id": "K3Iox1kEre3Q",
    "outputId": "cc22bbb7-b2d0-414c-d009-3cff156639ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3698f070b9d34b65be4da893cbd4f30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6211ce062c457694470771b1da6169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2210a3724bd4c45ae899288e0ebabeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b9d2cc9bea4699b22aad07aeadc9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e65d14477d54284b6aa9da249378705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíº Loading FinGPT Financial Q&A dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdadcecb6186412db3b78e0b6bd9fdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/522 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea5b0d04b0644b7ae294e53f371a33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-ab79bf9300210e(‚Ä¶):   0%|          | 0.00/10.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984bad60c9d94d7bb21be259cc1720ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 5000 samples\n",
      "Dataset columns: ['input', 'output', 'instruction']\n",
      "\n",
      "üìã Sample entry:\n",
      "{'input': 'What is considered a business expense on a business trip?', 'output': 'The IRS Guidance pertaining to the subject.  In general the best I can say is your business expense may be deductible.  But it depends on the circumstances and what it is you want to deduct. Travel Taxpayers who travel away from home on business may deduct related   expenses, including the cost of reaching their destination, the cost   of lodging and meals and other ordinary and necessary expenses.   Taxpayers are considered ‚Äútraveling away from home‚Äù if their duties   require them to be away from home substantially longer than an   ordinary day‚Äôs work and they need to sleep or rest to meet the demands   of their work. The actual cost of meals and incidental expenses may be   deducted or the taxpayer may use a standard meal allowance and reduced   record keeping requirements. Regardless of the method used, meal   deductions are generally limited to 50 percent as stated earlier.    Only actual costs for lodging may be claimed as an expense and   receipts must be kept for documentation. Expenses must be reasonable   and appropriate; deductions for extravagant expenses are not   allowable. More information is available in Publication 463, Travel,   Entertainment, Gift, and Car Expenses. Entertainment Expenses for entertaining clients, customers or employees may be   deducted if they are both ordinary and necessary and meet one of the   following tests: Directly-related test: The main purpose of the entertainment activity is the conduct of business, business was actually conducted   during the activity and the taxpayer had more than a general   expectation of getting income or some other specific business benefit   at some future time.   Associated test: The entertainment was associated with the active conduct of the taxpayer‚Äôs trade or business and occurred directly   before or after a substantial business discussion. Publication 463 provides more extensive explanation of these tests as   well as other limitations and requirements for deducting entertainment   expenses. Gifts Taxpayers may deduct some or all of the cost of gifts given in the   course of their trade or business. In general, the deduction is   limited to $25 for gifts given directly or indirectly to any one   person during the tax year. More discussion of the rules and   limitations can be found in Publication 463. If your LLC reimburses you for expenses outside of this guidance it should be treated as Income for tax purposes. Edit for Meal Expenses: Amount of standard meal allowance.   The standard meal allowance is   the federal M&IE rate. For travel in 2010, the rate for most small   localities in the United States is $46 a day. Source IRS P463 Alternately you could reimburse at a per diem rate', 'instruction': 'Utilize your financial knowledge, give your answer or opinion to the input question or subject . Answer format is not limited.'}\n",
      "\n",
      "üí¨ Formatting financial Q&A prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674912c7bac2464cb1900fbd5b24f4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting financial Q&A with Phi-3 instruction template:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¢ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cebd8e40fde4173b9862d0d82a638d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing financial Q&A (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ FINANCIAL Q&A DATASET PREPARATION COMPLETE!\n",
      "======================================================================\n",
      "üíº Total samples: 5000\n",
      "üìè Max sequence length: 1024\n",
      "üîë Dataset keys: ['input_ids', 'attention_mask', 'labels']\n",
      "üíæ Approximate size: 19.53 MB\n",
      "\n",
      "üéØ Task: Financial Question Answering\n",
      "üí° Domain: Investment, Banking, Corporate Finance, Personal Finance\n",
      "ü§ñ Model: Phi-3-Mini-4K-Instruct (3.8B)\n",
      "üöÄ Ready to use 'final_dataset' in your SFTTrainer!\n",
      "======================================================================\n",
      "\n",
      "üìã Sample formatted prompt (first 600 chars):\n",
      "<|system|>\n",
      "You are FinSight, an expert financial advisor and analyst. Provide accurate, detailed answers to financial questions. Use clear explanations and cite relevant financial concepts when applicable.<|end|>\n",
      "<|user|>\n",
      "Utilize your financial knowledge, give your answer or opinion to the input question or subject . Answer format is not limited.\n",
      "\n",
      "Context: What is considered a business expense on a business trip?<|end|>\n",
      "<|assistant|>\n",
      "The IRS Guidance pertaining to the subject.  In general the best I can say is your business expense may be deductible.  But it depends on the circumstances and wh...\n",
      "\n",
      "üìä Sequence Length Statistics:\n",
      "   Average length (first 100): 320 tokens\n",
      "   Min length: 90 tokens\n",
      "   Max length: 1024 tokens\n",
      "\n",
      "üí° Tip: Longer sequences = more detailed financial answers!\n",
      "\n",
      "üìö Sample Questions from Dataset:\n",
      "\n",
      "   1. Utilize your financial knowledge, give your answer or opinion to the input question or subject . Answer format is not limited....\n",
      "\n",
      "   2. Offer your insights or judgment on the input financial query or topic using your financial expertise. Reply as normal question answering...\n",
      "\n",
      "   3. Based on your financial expertise, provide your response or viewpoint on the given financial question or topic. The response format is open....\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SETUP: Phi-3-Mini Model & Tokenizer\n",
    "# ============================================\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ============================================\n",
    "# LOAD FINANCIAL Q&A DATASET\n",
    "# ============================================\n",
    "print(\"üíº Loading FinGPT Financial Q&A dataset...\")\n",
    "\n",
    "# Load financial Q&A dataset from FinGPT\n",
    "raw_dataset = load_dataset(\n",
    "    \"FinGPT/fingpt-fiqa_qa\",\n",
    "    split=\"train[:5000]\"  # Using 5k samples for comprehensive training\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(raw_dataset)} samples\")\n",
    "print(f\"Dataset columns: {raw_dataset.column_names}\")\n",
    "print(f\"\\nüìã Sample entry:\")\n",
    "print(raw_dataset[0])\n",
    "\n",
    "# ============================================\n",
    "# FORMAT PROMPTS FOR PHI-3\n",
    "# ============================================\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"\n",
    "    Format financial Q&A into Phi-3's instruction format\n",
    "\n",
    "    Phi-3 chat format:\n",
    "    <|system|>\n",
    "    {system_message}<|end|>\n",
    "    <|user|>\n",
    "    {user_message}<|end|>\n",
    "    <|assistant|>\n",
    "    {assistant_response}<|end|>\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract question and answer from dataset\n",
    "    # The dataset has 'instruction' (question) and 'output' (answer)\n",
    "    # Some entries might also have 'input' for additional context\n",
    "    question = example.get(\"instruction\", example.get(\"question\", \"\"))\n",
    "    context = example.get(\"input\", \"\")\n",
    "    answer = example.get(\"output\", example.get(\"answer\", \"\"))\n",
    "\n",
    "    # System message for financial expertise\n",
    "    system_message = (\n",
    "        \"You are FinSight, an expert financial advisor and analyst. \"\n",
    "        \"Provide accurate, detailed answers to financial questions. \"\n",
    "        \"Use clear explanations and cite relevant financial concepts when applicable.\"\n",
    "    )\n",
    "\n",
    "    # Construct user message with context if available\n",
    "    if context and context.strip():\n",
    "        user_message = f\"{question}\\n\\nContext: {context}\"\n",
    "    else:\n",
    "        user_message = question\n",
    "\n",
    "    # Format in Phi-3 chat template\n",
    "    prompt = (\n",
    "        f\"<|system|>\\n{system_message}<|end|>\\n\"\n",
    "        f\"<|user|>\\n{user_message}<|end|>\\n\"\n",
    "        f\"<|assistant|>\\n{answer}<|end|>\"\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# ============================================\n",
    "# TOKENIZATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the formatted text with proper padding and labels\"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    # Using 1024 max length for detailed financial Q&A\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,  # Increased for comprehensive financial answers\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # Create labels for causal LM training\n",
    "    # Labels = input_ids, but -100 for padded tokens (ignored in loss)\n",
    "    labels = []\n",
    "    for input_ids, attention_mask in zip(tokenized[\"input_ids\"], tokenized[\"attention_mask\"]):\n",
    "        # Convert to list if needed\n",
    "        label = input_ids.copy() if isinstance(input_ids, list) else list(input_ids)\n",
    "\n",
    "        # Set padded positions to -100 (ignored in loss calculation)\n",
    "        for i, mask in enumerate(attention_mask):\n",
    "            if mask == 0:  # This is a padded token\n",
    "                label[i] = -100\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# ============================================\n",
    "# PROCESS DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüí¨ Formatting financial Q&A prompts...\")\n",
    "formatted_dataset = raw_dataset.map(\n",
    "    format_prompt,\n",
    "    desc=\"Formatting financial Q&A with Phi-3 instruction template\"\n",
    ")\n",
    "\n",
    "print(\"\\nüî¢ Tokenizing dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset.column_names,  # Remove original columns\n",
    "    desc=\"Tokenizing financial Q&A\",\n",
    "    num_proc=4  # Use multiple processes for faster tokenization\n",
    ")\n",
    "\n",
    "# Final dataset ready for training\n",
    "final_dataset = tokenized_dataset\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY & STATISTICS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ FINANCIAL Q&A DATASET PREPARATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üíº Total samples: {len(final_dataset)}\")\n",
    "print(f\"üìè Max sequence length: {len(final_dataset[0]['input_ids'])}\")\n",
    "print(f\"üîë Dataset keys: {final_dataset.column_names}\")\n",
    "print(f\"üíæ Approximate size: {len(final_dataset) * 1024 * 4 / 1024 / 1024:.2f} MB\")\n",
    "print(\"\\nüéØ Task: Financial Question Answering\")\n",
    "print(\"üí° Domain: Investment, Banking, Corporate Finance, Personal Finance\")\n",
    "print(\"ü§ñ Model: Phi-3-Mini-4K-Instruct (3.8B)\")\n",
    "print(\"üöÄ Ready to use 'final_dataset' in your SFTTrainer!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optional: Print a sample to verify formatting\n",
    "print(\"\\nüìã Sample formatted prompt (first 600 chars):\")\n",
    "sample_text = formatted_dataset[0][\"text\"]\n",
    "print(sample_text[:600] + \"...\\n\")\n",
    "\n",
    "# Show statistics about sequence lengths\n",
    "print(\"üìä Sequence Length Statistics:\")\n",
    "lengths = [len([x for x in seq if x != tokenizer.pad_token_id])\n",
    "           for seq in tokenized_dataset[\"input_ids\"][:100]]\n",
    "print(f\"   Average length (first 100): {sum(lengths)/len(lengths):.0f} tokens\")\n",
    "print(f\"   Min length: {min(lengths)} tokens\")\n",
    "print(f\"   Max length: {max(lengths)} tokens\")\n",
    "print(f\"\\nüí° Tip: Longer sequences = more detailed financial answers!\")\n",
    "\n",
    "# Sample some questions to show diversity\n",
    "print(\"\\nüìö Sample Questions from Dataset:\")\n",
    "for i in range(3):\n",
    "    q = raw_dataset[i].get(\"instruction\", raw_dataset[i].get(\"question\", \"\"))\n",
    "    print(f\"\\n   {i+1}. {q[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rWTIyS5oAySa",
    "outputId": "439b6525-2491-43a9-81e0-6aabe9d10636"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíº Starting Phi-3-Mini Financial Q&A Fine-tuning...\n",
      "üìä Training on 500 financial Q&A samples\n",
      "üîß LoRA Config: rank=8, alpha=16, dropout=0.1\n",
      "üíæ 4-bit quantization: nf4\n",
      "üéØ Target: FinSight 360 Financial Intelligence System\n",
      "‚è±Ô∏è Estimated time: ~1.5-2 hours on T4 GPU\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='121' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [121/375 09:34 < 20:25, 0.21 it/s, Epoch 0.96/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.340900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 30:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.760500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.786100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.720700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ Financial Q&A fine-tuning completed!\n",
      "üíæ Model saved to: /content/drive/MyDrive/phi3-financial/Phi3-Mini-FinancialQA-finetuned\n",
      "üíº Your Phi-3 Financial Assistant (FinSight Core) is ready!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# My SFT Trainer Configuration for Phi-3-Mini Financial Q&A Fine-tuning\n",
    "# Using LoRA: rank=8, alpha=16, dropout=0.1, 4-bit quantization (nf4)\n",
    "\n",
    "# My optimized parameters for LoRA training on financial Q&A\n",
    "min_effective_batch_size = 4  # Reduced for 3.8B model with longer sequences (1024 tokens)\n",
    "lr = 2e-5  # Slightly lower LR for Phi-3 stability on financial reasoning\n",
    "max_seq_length = 1024  # Increased for detailed financial Q&A responses\n",
    "collator_fn = None  # I'm not using a custom collator since I pre-pad in tokenization\n",
    "packing = False  # I disabled packing since I'm using fixed-length sequences\n",
    "steps = 20  # My logging and saving frequency\n",
    "num_train_epochs = 3  # Standard for financial fine-tuning with LoRA\n",
    "warmup_ratio = 0.1  # Warmup for stable training start\n",
    "\n",
    "# My SFT configuration for Phi-3-Mini Financial Intelligence\n",
    "sft_config = SFTConfig(\n",
    "    # I'm saving my financial model for FinSight 360 project\n",
    "    output_dir = '/content/drive/MyDrive/phi3-financial/Phi3-Mini-FinancialQA-finetuned',\n",
    "\n",
    "    # My data processing settings\n",
    "    packing = packing,\n",
    "    max_seq_length = max_seq_length,\n",
    "\n",
    "    # I disabled gradient checkpointing (not needed with 4-bit quantization)\n",
    "    gradient_checkpointing = False,\n",
    "\n",
    "    # My training batch and precision settings\n",
    "    per_device_train_batch_size = min_effective_batch_size,\n",
    "    auto_find_batch_size = True,  # Let trainer optimize for available VRAM\n",
    "    bf16 = True,  # Using bf16 for numerical stability in financial calculations\n",
    "\n",
    "    # My training schedule optimized for financial domain\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    learning_rate = lr,\n",
    "    lr_scheduler_type = \"cosine\",  # Smooth learning rate decay\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    weight_decay = 0.01,  # Regularization for financial terminology\n",
    "    max_grad_norm = 1.0,  # Gradient clipping for stability\n",
    "\n",
    "    # My logging and monitoring setup\n",
    "    report_to = 'wandb',  # Tracking FinSight model training\n",
    "    run_name = \"Phi3-Mini-FinancialQA-LoRA-r8-alpha16\",  # Descriptive run name\n",
    "\n",
    "    # My logging directory\n",
    "    logging_dir = '/content/drive/MyDrive/phi3-financial/Phi3-Mini-FinancialQA-finetuned/logs',\n",
    "\n",
    "    # My checkpoint and logging strategy\n",
    "    logging_strategy = 'steps',\n",
    "    save_strategy = 'steps',\n",
    "    logging_steps = steps,  # I log every 20 steps\n",
    "    save_steps = steps,     # I save checkpoint every 20 steps\n",
    "    save_total_limit = 2,   # Keep last 2 checkpoints to save space\n",
    ")\n",
    "\n",
    "# I'm using subset for faster initial training\n",
    "train_dataset = final_dataset.select(range(500))  # Using 2k samples for quick iteration\n",
    "\n",
    "# I create my trainer with the financial Q&A dataset\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                    # My Phi-3-Mini with LoRA (rank=8, alpha=16, dropout=0.1)\n",
    "    train_dataset = train_dataset,    # My 2k financial Q&A samples\n",
    "    processing_class = tokenizer,     # Phi-3 tokenizer\n",
    "    data_collator = collator_fn,      # Using default collator\n",
    "    args = sft_config,               # My financial training configuration\n",
    ")\n",
    "\n",
    "# I start the financial fine-tuning process\n",
    "print(\"üíº Starting Phi-3-Mini Financial Q&A Fine-tuning...\")\n",
    "print(f\"üìä Training on {len(train_dataset)} financial Q&A samples\")\n",
    "print(f\"üîß LoRA Config: rank=8, alpha=16, dropout=0.1\")\n",
    "print(f\"üíæ 4-bit quantization: nf4\")\n",
    "print(f\"üéØ Target: FinSight 360 Financial Intelligence System\")\n",
    "print(f\"‚è±Ô∏è Estimated time: ~1.5-2 hours on T4 GPU\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Financial Q&A fine-tuning completed!\")\n",
    "print(f\"üíæ Model saved to: {sft_config.output_dir}\")\n",
    "print(\"üíº Your Phi-3 Financial Assistant (FinSight Core) is ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630,
     "referenced_widgets": [
      "f68ea598d3cc4259a5b25b8a6b89640c",
      "75e593ba45824fe78f2c65e06702718f",
      "d1aa51ff23244d508299a87e1ecf617c",
      "209ae9b39b034d07b4ef8fee9c42b35c",
      "fa02ae2a9fe04008947fe980ca7a82e0",
      "80a2c5a7e29a4d8898a1795638b8eef7",
      "cf2c53eb78034e8984dc09a0cb5fd15d",
      "b920ef0eac3f4d228be0ec305755b138",
      "7f7162ced57f40ba869786e84f79847b",
      "58fb895e7581406a9b6747260769c71b",
      "b7171055dbf241c3bd199df70c95c732",
      "5f2ad1ba7f934f71957a0d13007e33e6",
      "9c61e40f9d154406bb0d9e38dac916f3",
      "69f1d2ed6b944c8289527d86b25a7ba9",
      "da54d9d781d34618a0ea1cfca2e5e0f7",
      "fc17e0d4809f4c40b2effe234f70e979",
      "97b8c8db3e9b4a4a988e168bbd3c79b3",
      "754f3e763a5b4b79ba58432d8d2131f9",
      "f227e2961f6941cc88b7d465ecf9afc7",
      "67e338a545614857ac1c0ecac1497d1e",
      "499ae554064b4925ae061544306afcad",
      "55c3498408c74e25b00d55d1e51acf9c",
      "d1acdae27dd3417897017034046d76bc",
      "8cf1a2f24a454ed1b2f915e89eea8e67",
      "fca7eaf100fb4ab0896a7701c5249f39",
      "4c831552d687439198a100c8d5f445fc",
      "f099e14d8c8f405bbe162e163eb4dcb7",
      "79a71cc10bd943fc9271a37bddb7bccc",
      "5af8b14286e441e0b74eaacb3f061228",
      "2c3f789a87484b938afb3f8e7d51ab5c",
      "7f892b489a94421198c80e6819bdeab6",
      "e89df12e1c0d4985b63f2944ea3fc31a",
      "955baeaf7a7649d986c9a15be4fd0d17",
      "14ce314007754666ba69eb6f916d5101",
      "e1d77af037fd4582aecc977349b53e54",
      "07490d065db34d4b9fadcca2fea33438",
      "bf099383cdc84e68bb343de940de9586",
      "4f3397d6e79e4763bc141648521a8abc",
      "5a0d11b973ce4d3f95810e10ec77d39c",
      "01425aece28b4173b445b56c33bc655c",
      "69610d22f3424aea933cba9a0236fbaa",
      "042aa832f3e64ccd9a75c1be8c92e355",
      "569944fbbe824266adc77f3d4d60455f",
      "781ac090f5454a31a6404f47bfba2ad8",
      "caad9d1f67f0495a917def06826f2b4e",
      "17bd0493f1fe43d3a600cefb1a08a474",
      "7b454f34ded04c989b37c9ece0e4d3f6",
      "f3f93e9d24d744e881e5fb6768bac237",
      "3abd625b09fd4418aab941ce0897a5c4",
      "fb444e79164e43df8baff90cece45301",
      "59058acfaa294a888357c6242002aab2",
      "a9df1f406afd4dbcb9192a265ee5cee7",
      "a0c86ea976864444a132938175a15daf",
      "0f63b0cf707e4fab98eb93646f74bbe7",
      "fc812fd2179042cf9168fc0c8e58b910",
      "a088ac4185694be09121c54d4496c3a9",
      "811477385b0049f0b5079346b654a108",
      "dc65a65f72b14cee808afad8cba34672",
      "c2f93d2fbc8c41f9bc56f5b67ae44b1b",
      "7c19e2daf161448ab71c3c2e94621d6a",
      "ac47594ee04346ddb814c93272f6785c",
      "54ab67d1548347a9b23716dbb8866c84",
      "0666a0357b4e492e9830e3db1244074f",
      "e2f0cbac94504c9ea9e0f1554aa965bf",
      "dfe245e4934e4cde901376332d6a1473",
      "5904854c6ab84fdca125d2785428fcc7",
      "20fe6b5e99b447b7b4d0164c7a54b6fb",
      "e48ca59572664cdfa71dfeea24ceaba9",
      "93a30a7d4f6a4147a635c116092517a9",
      "8146d702a37143e3a43e12e0925d4333",
      "089f20fcb4594496891d5b503c537ec6",
      "6fc4784a04284662b66ccc1910df31d0",
      "b4a439059d6e47069fbaae61b05eef02",
      "b330b85f3d88484b8251fb44e0ed1a27",
      "ad08770be9964c76b9c3a9772a89f967",
      "c01f98fbce4b4b139a7d83757583c157",
      "621950f2eb204d22bfc6f2be5be2f7b4",
      "f19da9bdd31744c7b80310b0e3560958",
      "a9422242cfb34dea825ff8d028347342",
      "a53e4dfe62de458c95643d67fd2410ea",
      "0a5aab62fc844e3b939db45c6ec21b0d",
      "7354d690755f4a8aaee6f39da68af895",
      "19f53e8b91154b509a48c1420687a578",
      "76c7b42619ec4f958f53d3090e8dca6d",
      "6e289a79a6144dcfa37dfe0a8b084f09",
      "4c954b7c14ca43e49cf6b681286efce5",
      "c1b48fc1543249dfad8172e5f2b26339",
      "1e925375af7442439c656b09ca887723"
     ]
    },
    "id": "W7SyBkyK-S-a",
    "outputId": "f9c46577-d848-4aac-98e6-dc9c4b2568ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving my trained Phi-3-Mini financial model...\n",
      "üîß Loading my PEFT model and merging adapter...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68ea598d3cc4259a5b25b8a6b89640c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Merging LoRA weights into base Phi-3 model...\n",
      "üíæ Saving my merged financial model...\n",
      "‚òÅÔ∏è Uploading my FinSight financial assistant to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2ad1ba7f934f71957a0d13007e33e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1acdae27dd3417897017034046d76bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ce314007754666ba69eb6f916d5101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...al-merged/tokenizer.model: 100%|##########|  500kB /  500kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caad9d1f67f0495a917def06826f2b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0004-of-00004.safetensors:  15%|#4        | 58.7MB /  394MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a088ac4185694be09121c54d4496c3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0002-of-00004.safetensors:   0%|          |  554kB / 4.98GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fe6b5e99b447b7b4d0164c7a54b6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0003-of-00004.safetensors:   0%|          | 1.11MB / 4.95GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19da9bdd31744c7b80310b0e3560958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0001-of-00004.safetensors:   0%|          | 16.8MB / 4.96GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ Model upload completed! üéâ\n",
      "üíº Your Phi-3 FinSight Financial Assistant is now live!\n",
      "======================================================================\n",
      "üîó Model URL: https://huggingface.co/sweatSmile/Phi3-Mini-FinSight-FinancialQA\n",
      "\n",
      "üìã Model Details:\n",
      "   - Base: microsoft/Phi-3-mini-4k-instruct (3.8B)\n",
      "   - Dataset: FinGPT/fingpt-fiqa_qa (2k samples)\n",
      "   - LoRA: rank=8, alpha=16, dropout=0.1\n",
      "   - Quantization: 4-bit (nf4)\n",
      "   - Domain: Financial Q&A for FinSight 360 System\n",
      "   - Use Case: Investment advice, banking queries, personal finance\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: I'm saving my trained financial model locally first\n",
    "print(\"üíæ Saving my trained Phi-3-Mini financial model...\")\n",
    "trainer.save_model('/content/phi3-financial-saved')\n",
    "\n",
    "# Step 2: I load and merge the LoRA adapter with the base model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print(\"üîß Loading my PEFT model and merging adapter...\")\n",
    "# I load the saved PEFT model (use the same path as Step 1)\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained('/content/phi3-financial-saved')\n",
    "\n",
    "# I merge and unload the adapter to get a single model\n",
    "print(\"‚öôÔ∏è Merging LoRA weights into base Phi-3 model...\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Step 3: I save the merged model with tokenizer\n",
    "print(\"üíæ Saving my merged financial model...\")\n",
    "merged_model.save_pretrained('/content/phi3-financial-merged')\n",
    "tokenizer.save_pretrained('/content/phi3-financial-merged')\n",
    "\n",
    "# Step 4: I upload my model to Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"‚òÅÔ∏è Uploading my FinSight financial assistant to Hugging Face Hub...\")\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path='/content/phi3-financial-merged',\n",
    "    repo_id=\"sweatSmile/Phi3-Mini-FinSight-FinancialQA\",  # My FinSight core model\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload Phi-3-Mini fine-tuned on 2k FinGPT financial Q&A with LoRA (r=8, alpha=16) - FinSight 360 Core\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Model upload completed! üéâ\")\n",
    "print(\"üíº Your Phi-3 FinSight Financial Assistant is now live!\")\n",
    "print(\"=\"*70)\n",
    "print(\"üîó Model URL: https://huggingface.co/sweatSmile/Phi3-Mini-FinSight-FinancialQA\")\n",
    "print(\"\\nüìã Model Details:\")\n",
    "print(\"   - Base: microsoft/Phi-3-mini-4k-instruct (3.8B)\")\n",
    "print(\"   - Dataset: FinGPT/fingpt-fiqa_qa (2k samples)\")\n",
    "print(\"   - LoRA: rank=8, alpha=16, dropout=0.1\")\n",
    "print(\"   - Quantization: 4-bit (nf4)\")\n",
    "print(\"   - Domain: Financial Q&A for FinSight 360 System\")\n",
    "print(\"   - Use Case: Investment advice, banking queries, personal finance\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
